---
title: "04 - Inter-omics data analyses"
output:
  bookdown::html_document2:
    includes:
      in_header: header.html
  bookdown::gitbook:
    includes:
      in_header: header.html
---
```{r setup04, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE, message = FALSE, warning = FALSE)
library(knitr)
```

```{r io1-functions}
library(tidyverse)
library(vegan)
library(kableExtra)

#############
# FUNCTIONS #
#############

# Prepare dataframes aka "data gymnastics". The output is a list with the three data set per experiment for the three different species gb, sf, wb
gymnastics <- function(df, my_colnames) {
  df[, c("df.1","df","mz","mzmin","mzmadf","rt","rtmin","rtmadf","npeaks","Gb","QC","Sf","Wb")]<- list(NULL)
  df[, c("isotopes","adduct","pcgroup","SD","MEAN","CV","fss","df.M.H..", "raw_peaks.pcgroup")]<- list(NULL)
  df <- data.frame(t(df))
  df["ID"] <- rownames(df)
  df["unified_ID"] <-masslynx$unified_ID[match(df$ID, masslynx[[my_colnames]])] 
  df <- na.omit(df)
  df <- df[order(df$unified_ID),] 
  rownames(df) <- df$unified_ID
  df["spec"] <- str_sub(df$unified_ID, 1,2)
  df <- df[!df$spec=="QC",]
  df[, c("ID","unified_ID")]<- list(NULL)
  df_gb <- df[df$spec=="Gb",]
  df_sf <- df[df$spec=="Sf",]
  df_wb <- df[df$spec=="Wb",]
  df_gb$spec <- NULL
  df_sf$spec <- NULL
  df_wb$spec <- NULL
  # ready for mantel
  metabolome <- list(gb=df_gb, sf=df_sf, wb=df_wb)
  return(metabolome)
}

# Prepares the microbiome data sets to match the metabolomes. The output is a list with all microbiomes and metabolomes
congruency <- function(metabolomes){
  metabolome_gb <- metabolomes$gb
  metabolome_sf <- metabolomes$sf
  metabolome_wb <- metabolomes$wb
  
  micro_gb <- micro[micro$spec=="Gb",]
  micro_sf <- micro[micro$spec=="Sf",]
  micro_wb <- micro[micro$spec=="Wb",]
  
  micro_gb[, c("spec","unified_ID")]<- list(NULL)
  micro_sf[, c("spec","unified_ID")]<- list(NULL)
  micro_wb[, c("spec","unified_ID")]<- list(NULL)
  
  micro_gb <- micro_gb[, colSums(micro_gb != 0) > 0]
  micro_sf <- micro_sf[, colSums(micro_sf != 0) > 0]
  micro_wb <- micro_wb[, colSums(micro_wb != 0) > 0]
  
  micro_gb <- sqrt(micro_gb)
  micro_sf <- sqrt(micro_sf)
  micro_wb <- sqrt(micro_wb)
  
  micro_gb <- wisconsin(micro_gb)
  micro_sf <- wisconsin(micro_sf)
  micro_wb <- wisconsin(micro_wb)
  
  micro_gb <- micro_gb[rownames(micro_gb) %in% rownames(metabolome_gb),]
  metabolome_gb <- metabolome_gb[rownames(metabolome_gb) %in% rownames(micro_gb),]
  all(rownames(micro_gb)==rownames(metabolome_gb))
  
  micro_sf <- micro_sf[rownames(micro_sf) %in% rownames(metabolome_sf),]
  metabolome_sf <- metabolome_sf[rownames(metabolome_sf) %in% rownames(micro_sf),]
  all(rownames(micro_sf)==rownames(metabolome_sf))
  
  micro_wb <- micro_wb[rownames(micro_wb) %in% rownames(metabolome_wb),]
  metabolome_wb <- metabolome_wb[rownames(metabolome_wb) %in% rownames(micro_wb),]
  all(rownames(micro_wb)==rownames(metabolome_wb))
  
  congruent_dfs <- list(micro_gb=micro_gb, micro_sf=micro_sf, micro_wb=micro_wb, metabolome_gb=metabolome_gb, metabolome_sf=metabolome_sf, metabolome_wb=metabolome_wb)
  return(congruent_dfs)
}

# MANTEL TEST Generates distance matrices "Bray-Curtis" for microbiome, scaled "euclidean" for metabolomes, runs a Mantel test with Spearman correlation, and saves the parameters to a data frame.
cloak <- function(congruent_dfs, experiment, filtering){
  gb_meta_dist <- vegdist(scale(congruent_dfs$metabolome_gb), "euclid")
  micro_gb_dist <- vegdist(congruent_dfs$micro_gb, method="bray")
  
  sf_meta_dist <- vegdist(scale(congruent_dfs$metabolome_sf), "euclid")
  micro_sf_dist <- vegdist(congruent_dfs$micro_sf, method="bray")
  
  wb_meta_dist <- vegdist(scale(congruent_dfs$metabolome_wb), "euclid")
  micro_wb_dist <- vegdist(congruent_dfs$micro_wb, method="bray")
  
  m_gb <- mantel(micro_gb_dist, gb_meta_dist, method="spearman")
  m_sf <- mantel(micro_sf_dist, sf_meta_dist, method="spearman")
  m_wb <- mantel(micro_wb_dist, wb_meta_dist, method="spearman")
  
  micro_dim <- as.data.frame(t(dim(congruent_dfs$micro_gb)))
  micro_dim <-rbind(micro_dim, as.data.frame(t(dim(congruent_dfs$micro_sf))))
  micro_dim <-rbind(micro_dim, as.data.frame(t(dim(congruent_dfs$micro_wb))))
  colnames(micro_dim) <- c("micro_samples","micro_OTUs")
  
  meta_dim <- as.data.frame(t(dim(congruent_dfs$metabolome_gb)))
  meta_dim <-rbind(meta_dim, as.data.frame(t(dim(congruent_dfs$metabolome_sf))))
  meta_dim <-rbind(meta_dim, as.data.frame(t(dim(congruent_dfs$metabolome_wb)))) 
  colnames(meta_dim) <- c("meta_samples","meta_features")
  
  stats <- data.frame(m_gb$statistic)
  stats <- rbind(stats, m_sf$statistic)
  stats <- rbind(stats, m_wb$statistic)
  
  signif <- data.frame(m_gb$signif)
  signif <- rbind(signif, m_sf$signif)
  signif <- rbind(signif, m_wb$signif)
  
  colnames(stats) <- c("statistic")
  stats["signif"] <- signif
  stats["Sponge species"] <- c("Geodia barretti", "Stryphnus fortis", "Weberella bursa")
  
  stats <- bind_cols(stats, micro_dim)
  stats <- bind_cols(stats, meta_dim)
  stats["data set"] <- filtering
  stats["Experiment"] <- experiment
  return(stats)
}

# PROTEST NMDS ordination and protest
ordination <- function(congruent_dfs, experiment, filtering){
  # If you do not have community data, you should probably set autotransform = FALSE. k: number of dimensions
  gb_meta_mds <- metaMDS(congruent_dfs$metabolome_gb, trymax = 100, distance =  "euclid", autotransforme=F) 
  micro_gb_mds <- metaMDS(congruent_dfs$micro_gb, trymax = 100, distance ="bray")
  
  sf_meta_mds <- metaMDS(congruent_dfs$metabolome_sf, trymax = 100, distance = "euclid", autotransforme=F) 
  micro_sf_mds <- metaMDS(congruent_dfs$micro_sf, trymax = 100, distance ="bray")
  
  wb_meta_mds <- metaMDS(congruent_dfs$metabolome_wb, trymax = 100, distance = "euclid", autotransforme=F) 
  micro_wb_mds <- metaMDS(congruent_dfs$micro_wb, trymax = 100, distance ="bray")
  
  gb_proc <- procrustes(micro_gb_mds, gb_meta_mds, scores = "sites")
  gb_prot <- protest(micro_gb_mds, gb_meta_mds, scores = "sites")
  
  sf_proc <- procrustes(micro_sf_mds, sf_meta_mds, scores = "sites")
  sf_prot <- protest(micro_sf_mds, sf_meta_mds, scores = "sites")
  
  wb_proc <- procrustes(micro_wb_mds, wb_meta_mds, scores = "sites")
  wb_prot <- protest(micro_wb_mds, wb_meta_mds, scores = "sites")
  
  micro_dim <- as.data.frame(t(dim(congruent_dfs$micro_gb)))
  micro_dim <-rbind(micro_dim, as.data.frame(t(dim(congruent_dfs$micro_sf))))
  micro_dim <-rbind(micro_dim, as.data.frame(t(dim(congruent_dfs$micro_wb))))
  colnames(micro_dim) <- c("micro_samples","micro_OTUs")
  
  meta_dim <- as.data.frame(t(dim(congruent_dfs$metabolome_gb)))
  meta_dim <-rbind(meta_dim, as.data.frame(t(dim(congruent_dfs$metabolome_sf))))
  meta_dim <-rbind(meta_dim, as.data.frame(t(dim(congruent_dfs$metabolome_wb))))
  colnames(meta_dim) <- c("meta_samples","meta_features")
  
  pss <- data.frame(gb_prot$ss)
  pss <- rbind(pss, sf_prot$ss)
  pss <- rbind(pss, wb_prot$ss)
  colnames(pss) <- c("Procrustes SS")
  
  cor <- data.frame(gb_prot$scale)
  cor <- rbind(cor, sf_prot$scale)
  cor <- rbind(cor, wb_prot$scale)
  colnames(cor) <- c("correlation in sym. rotation")
  
  signif <- data.frame(gb_prot$signif)
  signif <- rbind(signif, sf_prot$signif)
  signif <- rbind(signif, wb_prot$signif)
  colnames(signif) <- c("signif")
  
  stats <- bind_cols(pss, cor, signif, micro_dim, meta_dim)
  stats["data set"] <- filtering
  stats["Experiment"] <- experiment
  stats["Sponge species"] <- c("Geodia barretti", "Stryphnus fortis", "Weberella bursa")
  return(stats)
}

```

# Inter-omics

The inter-omics analyses are comprised of three parts. In the first part, we evaluate congruency of the prokaryotic and metabolomic data set as a whole using numerical methods (Mantel test) and ordination (Procrustes rotation and Protest). The second part consists of generating a microbial interaction network and annotating it with depth response of the OTUs and correlation with the barettin signal. In the third part, we rank OTUs based on properties hypothesised to be true forthe producer of barettin.

## Mantel test and procrustes rotations

### Libraries and functions

### Data sets

```{r io2-data_mp, eval=FALSE}
# LOAD DATA 
micro <- read.csv("data/OTU_all_R.csv", header = T, sep = ";")
colnames(micro)[colnames(micro) == "Sample_ID"] <- "unified_ID"
micro <- micro[order(micro$unified_ID),] 
rownames(micro) <- micro$unified_ID
micro["spec"] <- str_sub(micro$unified_ID, 1, 2)

meta_data <- read.csv("data/PANGAEA_Final.csv", header = T, sep = ";")  
masslynx <- meta_data
masslynx <- masslynx[c("unified_ID", "LC.MS.HILIC.positive", "LC.MS.HILIC.negative", "LC.MS.RP.positive",  "LC.MS.RP.negative")]
colnames(masslynx) <- c("unified_ID", "H_p","H_n","R_p","R_n")
masslynx <- na.omit(masslynx)
masslynx["HILIC_pos"] <- str_sub(masslynx$H_p, 1,-3)
masslynx["HILIC_neg"] <- str_sub(masslynx$H_n, 1,-3)
masslynx["RP_pos"] <- str_sub(masslynx$R_p, 1,-3)
masslynx["RP_neg"] <- str_sub(masslynx$R_n, 1,-3)

# load one set of experiments at a time, i.e. CLEANED, ION or PC_GROUPS

### CLEANED
hilic_pos <- read.csv("data/HILIC_pos_20190417_cleaned.csv", header=T, sep=",")
hilic_neg <- read.csv("data/HILIC_neg_20190421_cleaned.csv", header=T, sep=",") 
rp_pos <- read.csv("data/RP_pos_20190421_cleaned.csv", header=T, sep=",")
rp_neg <- read.csv("data/RP_neg_20190422_cleaned.csv", header=T, sep=",")

### ION
hilic_pos <- read.csv("data/HILIC_pos_20190417_cleaned_MH.csv", header=T, sep = ",")
hilic_neg <- read.csv("data/HILIC_neg_20190421_cleaned_MH.csv", header=T, sep=",")
rp_pos <- read.csv("data/RP_pos_20190421_cleaned_MH.csv", header=T, sep=",")
rp_neg <- read.csv("data/RP_neg_20190422_cleaned_MH.csv", header=T, sep=",")

### PC_GROUPS
hilic_pos <- read.csv("data/HILIC_pos_20190417_cleaned_pcgroup.csv", header=T, sep = ",")
hilic_neg <- read.csv("data/HILIC_neg_20190421_cleaned_pcgroup.csv", header=T, sep=",")
rp_pos <- read.csv("data/RP_pos_20190421_cleaned_pcgroup.csv", header=T, sep=",")
rp_neg <- read.csv("data/RP_neg_20190422_cleaned_pcgroup.csv", header=T, sep=",")

```

### Mantel test code and tabular output

```{r io3-mantel-test, eval=F}
###=============================== MANTEL TEST=================================

#run one of this at a time
metabolomes <- gymnastics(hilic_pos, "H_p")
metabolomes <- gymnastics(hilic_neg, "H_n")
metabolomes <- gymnastics(rp_pos, "R_p")
metabolomes <- gymnastics(rp_neg, "R_n")

# run this
congruent_dfs <- congruency(metabolomes)

# run one of these: MANTEL TEST
diagnostics_hp_cleaned  <- cloak(congruent_dfs=congruent_dfs, experiment="HILIC pos", filtering="cleaned")
diagnostics_hn_cleaned  <- cloak(congruent_dfs=congruent_dfs, experiment="HILIC neg", filtering="cleaned")
diagnostics_rp_cleaned  <- cloak(congruent_dfs=congruent_dfs, experiment="RP pos", filtering="cleaned")
diagnostics_rn_cleaned  <- cloak(congruent_dfs=congruent_dfs, experiment="RP neg", filtering="cleaned")

diagnostics_hp_ion  <- cloak(congruent_dfs=congruent_dfs, experiment="HILIC pos", filtering="ion")
diagnostics_hn_ion  <- cloak(congruent_dfs=congruent_dfs, experiment="HILIC neg", filtering="ion")
diagnostics_rp_ion  <- cloak(congruent_dfs=congruent_dfs, experiment="RP pos", filtering="ion")
diagnostics_rn_ion  <- cloak(congruent_dfs=congruent_dfs, experiment="RP neg", filtering="ion")

diagnostics_hp_pc_group  <- cloak(congruent_dfs=congruent_dfs, experiment="HILIC pos", filtering="pc_group")
diagnostics_hn_pc_group  <- cloak(congruent_dfs=congruent_dfs, experiment="HILIC neg", filtering="pc_group")
diagnostics_rp_pc_group  <- cloak(congruent_dfs=congruent_dfs, experiment="RP pos", filtering="pc_group")
diagnostics_rn_pc_group  <- cloak(congruent_dfs=congruent_dfs, experiment="RP neg", filtering="pc_group")


### Combine all test results into one file

diagnostics <- diagnostics_hp_cleaned
diagnostics <- rbind(diagnostics, diagnostics_hn_cleaned, diagnostics_rp_cleaned, diagnostics_rn_cleaned,   diagnostics_hp_pc_group, diagnostics_hn_pc_group, diagnostics_rp_pc_group, diagnostics_rn_pc_group,
                     diagnostics_hp_ion, diagnostics_hn_ion, diagnostics_rp_ion, diagnostics_rn_ion)
diagnostics
write.csv(diagnostics, "mantel_stats_FUN.csv")
```

```{r io4-mantel-results}
diagnostics <- read.csv("data/mantel_stats_FUN.csv")
diagnostics$X <- NULL
diagnostics <- diagnostics[,c("statistic", "signif", "micro_samples","micro_OTUs","meta_samples","meta_features","Sponge.species", "Experiment",  "data.set")]

options(kableExtra.html.bsTable = T)
kable(diagnostics, 
      col.names = c("Mantel statistic r", "significance", "N microbiome samples", "N OTUs", "N metabolome samples", "N features", "Sponge species", "Experiment", "data set"), 
      longtable = T, booktabs = T, 
      caption = "Mantel test diagnostics diagnositcs comparing the microbiome and metabolome of the same sponge specimens", 
      row.names=FALSE) %>%
  add_header_above(c("Diagnostics" = 6, "Data set attribution" = 3)) %>%
  kable_styling(bootstrap_options = c("striped","hover", "bordered", "condensed", "responsive"), font_size = 12, full_width = F,latex_options = c("striped", "scale_down"))

dig <- diagnostics[diagnostics$signif<=0.05,]
a <- aggregate(dig, by=list(dig$Sponge.species, dig$Experiment, dig$data.set), FUN="length")
summary(a$Group.1)
summary(a$Group.2)
summary(a$Group.3)

rm(diagnostics_hp_cleaned, diagnostics_hn_cleaned, diagnostics_rp_cleaned, diagnostics_rn_cleaned, diagnostics_hp_ion, diagnostics_hn_ion, diagnostics_rp_ion, diagnostics_rn_ion, diagnostics_hp_pc_group, diagnostics_hn_pc_group, diagnostics_rp_pc_group, diagnostics_rn_pc_group)
rm(hilic_pos, hilic_neg, rp_pos, rp_neg)
```

As we can see from the table, in `r dim(dig)[1]` cases, the Mantel test returns a significant correlation between the two matrices. Above you can see the the significant tests broken down by sponge species, HPCL-experiment and filtering approach.

### Procrustes rotation and protest code and tabular output

```{r io5_prt, eval=F}
###=============================== PROTEST ====================================

#run one of this at a time
metabolomes <- gymnastics(hilic_pos, "H_p")
metabolomes <- gymnastics(hilic_neg, "H_n")
metabolomes <- gymnastics(rp_pos, "R_p")
metabolomes <- gymnastics(rp_neg, "R_n")

# run this
congruent_dfs <- congruency(metabolomes)

# run one of these: PROTEST TEST
diagnostics_hp_cleaned  <- ordination(congruent_dfs=congruent_dfs, experiment="HILIC pos", filtering="cleaned")
diagnostics_hn_cleaned  <- ordination(congruent_dfs=congruent_dfs, experiment="HILIC neg", filtering="cleaned")
diagnostics_rp_cleaned  <- ordination(congruent_dfs=congruent_dfs, experiment="RP pos", filtering="cleaned")
diagnostics_rn_cleaned  <- ordination(congruent_dfs=congruent_dfs, experiment="RP neg", filtering="cleaned")

diagnostics_hp_ion  <- ordination(congruent_dfs=congruent_dfs, experiment="HILIC pos", filtering="ion")
diagnostics_hn_ion  <- ordination(congruent_dfs=congruent_dfs, experiment="HILIC neg", filtering="ion")
diagnostics_rp_ion  <- ordination(congruent_dfs=congruent_dfs, experiment="RP pos", filtering="ion")
diagnostics_rn_ion  <- ordination(congruent_dfs=congruent_dfs, experiment="RP neg", filtering="ion")

diagnostics_hp_pc_group  <- ordination(congruent_dfs=congruent_dfs, experiment="HILIC pos", filtering="pc_group")
diagnostics_hn_pc_group  <- ordination(congruent_dfs=congruent_dfs, experiment="HILIC neg", filtering="pc_group")
diagnostics_rp_pc_group  <- ordination(congruent_dfs=congruent_dfs, experiment="RP pos", filtering="pc_group")
diagnostics_rn_pc_group  <- ordination(congruent_dfs=congruent_dfs, experiment="RP neg", filtering="pc_group")


### Combine all test results into one file

diagnostics <- diagnostics_hp_cleaned
diagnostics <- rbind(diagnostics, diagnostics_hn_cleaned, diagnostics_rp_cleaned, diagnostics_rn_cleaned, 
                     diagnostics_hp_pc_group, diagnostics_hn_pc_group, diagnostics_rp_pc_group, diagnostics_rn_pc_group, 
                     diagnostics_hp_ion, diagnostics_hn_ion, diagnostics_rp_ion, diagnostics_rn_ion)
diagnostics

write.csv(diagnostics, "protest_stats_FUN.csv")

rm(diagnostics_hp_cleaned, diagnostics_hn_cleaned, diagnostics_rp_cleaned, diagnostics_rn_cleaned, diagnostics_hp_ion, diagnostics_hn_ion, diagnostics_rp_ion, diagnostics_rn_ion, diagnostics_hp_pc_group, diagnostics_hn_pc_group, diagnostics_rp_pc_group, diagnostics_rn_pc_group, diagnostics)
rm(hilic_pos, hilic_neg, rp_pos, rp_neg)
```

```{r io6_prt}
diagnostics <- read.csv("data/protest_stats_FUN.csv")
diagnostics$X <- NULL
diagnostics <- diagnostics[,c("Procrustes.SS","correlation.in.sym..rotation", "signif", "micro_samples","micro_OTUs","meta_samples","meta_features","Sponge.species", "Experiment",  "data.set")]

options(kableExtra.html.bsTable = T)
kable(diagnostics, 
      col.names = c("Procrustes sum of squares", "correlation in symmetric rotation", "significance", "N microbiome samples", "N OTUs", "N metabolome samples", "N features", "Sponge species", "Experiment", "data set"), 
      longtable = T, booktabs = T, 
      caption = "Protest diagnostics comparing the microbiome and metabolome of the same sponge specimens", 
      row.names=FALSE) %>%
  add_header_above(c("Diagnostics" = 7, "Data set attribution" = 3)) %>%
  kable_styling(bootstrap_options = c("striped","hover", "bordered", "condensed", "responsive"), font_size = 12, full_width = F, latex_options = c("striped", "scale_down"))

dig <- diagnostics[diagnostics$signif<=0.05,]
a <- aggregate(dig, by=list(dig$Sponge.species, dig$Experiment, dig$data.set), FUN="length")
summary(a$Group.1)
summary(a$Group.2)
summary(a$Group.3)


```
Out of the `r dim(diagnostics)[1]` tests performed, `r dim(diagnostics[diagnostics$signif<=0.05,])[1]` are significant (_p_ $\leq$ 0.05). Immediately above you can see the the significant tests broken down by sponge species, HPCL-experiment and filtering approach.

## Correlating barettin and prokaryotic relative abundance

```{r barettin-correlation, echo=TRUE, eval=TRUE, fig.asp=0.4}
cmp <- read.csv("data/metabolite_master_20190605.csv", header=T, sep=",")
cmp$X <- NULL
meta_data <- read.csv("data/Steffen_et_al_metadata_PANGAEA.csv", header = T, sep = ";")   
micro <- read.csv("data/OTU_all_R.csv", header = T, sep = ";")

# preparing meta data
meta_data_prep <- function(meta_data){
  meta_data <- meta_data[, c("unified_ID", "Depth", "Latitude", "Longitude", "MeanBottomTemp_Cdeg", "MeanBotSalinity_PSU", "YEAR")]
  colnames(meta_data) <- c("unified_ID", "Depth", "Latitude", "Longitude", "Temperature", "Salinity", "Year")
  meta_data <- meta_data[!(str_sub(meta_data$unified_ID, 1,2)=="QC"),]
  meta_data[] <- lapply(meta_data, function(x) if(is.factor(x)) factor(x) else x)
  # Gb12, Gb20 and Gb21 are missing temperature and salinity. Imputing data from closeby samples:
  meta_data$Salinity[meta_data$unified_ID=="Gb12"] <- 34.92
  meta_data$Salinity[meta_data$unified_ID=="Gb20"] <- 34.92
  meta_data$Salinity[meta_data$unified_ID=="Gb21"] <- 34.56
  meta_data$Temperature[meta_data$unified_ID=="Gb12"] <- 3.71
  meta_data$Temperature[meta_data$unified_ID=="Gb20"] <- 3.65
  meta_data$Temperature[meta_data$unified_ID=="Gb21"] <- 2.32
  meta_data["spec"] <- str_sub(meta_data$unified_ID,1,2)
  meta_data <- meta_data[order(meta_data$unified_ID),]
  return(meta_data)  
}

meta_data <- meta_data_prep(meta_data)

# separating OTU tables by sponge
OTU_prep_sqrt <- function(micro){
  rownames(micro) <- micro$Sample_ID
  micro$Sample_ID <- NULL
  #micro <- sqrt(micro) #sqrt could be toggled on/off here
  
  micro_gb <- micro[(str_sub(rownames(micro), 1,2)=="Gb"),]
  micro_sf <- micro[(str_sub(rownames(micro), 1,2)=="Sf"),]
  micro_wb <- micro[(str_sub(rownames(micro), 1,2)=="Wb"),]
  
  micro_gb <- micro_gb[,colSums(micro_gb!=0)>0]
  micro_sf <- micro_sf[,colSums(micro_sf!=0)>0]
  micro_wb <- micro_wb[,colSums(micro_wb!=0)>0]
  micros <- list(gb=micro_gb, sf=micro_sf, wb=micro_wb)
  return(micros)  
}

micro_ds <- OTU_prep_sqrt(micro) 

# calculating overall relative abundance of each OTU per sample
overall_rabdc <- function(micros){
  mic <- micros
  n <- 0
  k <- dim(mic)[1]
  mic["rowsum"] <- apply(mic, 1, sum)
  
  while (n<k) {
    n <- n+1
    mic[n,] <- mic[n,]/(mic$rowsum[n])
  }
  
  mic$rowsum <- NULL
  mic <- data.frame(t(mic))
#  mic["avg_rel_abdc"] <- apply(mic, 1, mean)
#  mic["occurrence"] <- ifelse(mic$avg>0.0025, "common", "rare")
  return(mic)
}

rabdc <- lapply(micro_ds, overall_rabdc)

# preparing congruent data sets
common_samples <- intersect(colnames(rabdc$gb), cmp$unified_ID)
rabdc <- rabdc$gb[,colnames(rabdc$gb) %in% common_samples]
rabdc <- data.frame(t(rabdc))

cmp <- cmp[cmp$unified_ID %in% common_samples,]
cmp[] <- lapply(cmp, function(x) if(is.factor(x)) factor(x) else x)
cmp <- cmp[order((cmp$unified_ID)),]

#all(rownames(rabdc)==cmp$unified_ID)

# CORRELATION for Gb and Sf

bar_cor <- function(rabdc_df, cmp){
  barettin <- data.frame(colnames(rabdc_df))
  colnames(barettin) <- "XOTU"
  barettin["barettin_estimate_P"] <- NA
  barettin["barettin_p_val_P"] <- NA
  barettin["barettin_estimate_S"] <- NA
  barettin["barettin_p_val_S"] <- NA
  
  n <- 0
  k <- dim(barettin)[1]
  while (n<k) {
    n <- n+1
    barettin$barettin_estimate_P[n] <- cor.test(as.numeric(rabdc_df[,n]), cmp$bar, method="pearson")$estimate
    barettin$barettin_p_val_P[n] <- cor.test(as.numeric(rabdc_df[,n]), cmp$bar, method="pearson")$p.value
    barettin$barettin_estimate_S[n] <- cor.test(as.numeric(rabdc_df[,n]), cmp$bar, method="spearman")$estimate
    barettin$barettin_p_val_S[n] <- cor.test(as.numeric(rabdc_df[,n]), cmp$bar, method="spearman")$p.value
  }
  
  barettin["barettin_fdr_P"] <- NA
  barettin["barettin_fdr_S"] <- NA
  barettin$barettin_fdr_P <- p.adjust(barettin$barettin_p_val_P, method="fdr")
  barettin$barettin_fdr_S <- p.adjust(barettin$barettin_p_val_S, method="fdr")
  return(barettin)
}

barettin_cor <- bar_cor(rabdc, cmp)

#write.csv(barettin_cor, "data/GB_OTU_barettin_correlation.csv", row.names = F)

par(mfrow=c(1,3))
hist(barettin_cor$barettin_estimate_P, main="Pearson's pmcc: \n OTU rel abdc ~ barettin signal", xlab="Product moment \n correlation coefficient")
hist(barettin_cor$barettin_p_val_P, breaks=20, main="Pearson's pmcc p-value", xlab="p-value")
hist(barettin_cor$barettin_fdr_P, breaks=20, main="Pearson's pmcc \n fdr corrected p-value", xlab="FDR corrected p-value")
par(mfrow=c(1,1))

par(mfrow=c(1,3))
hist(barettin_cor$barettin_estimate_S, main="Spearman's rho: \n OTU rel abdc ~ barettin signal", xlab="Rho")
hist(barettin_cor$barettin_p_val_S, breaks=20, main="Spearman's rho p-value", xlab="p-value")
hist(barettin_cor$barettin_fdr_S, breaks=20, main="Spearman's rho \n fdr corrected p-value", xlab="FDR corrected p-value")
par(mfrow=c(1,1))

# For Sf, for shortlist
cmp <- read.csv("data/metabolite_master_20190605.csv", header=T, sep=",")
cmp$X <- NULL
rabdc <- lapply(micro_ds, overall_rabdc)
common_samples <- intersect(colnames(rabdc$sf), cmp$unified_ID)
rabdc <- rabdc$sf[,colnames(rabdc$sf) %in% common_samples]
rabdc <- data.frame(t(rabdc))

cmp <- cmp[cmp$unified_ID %in% common_samples,]
cmp[] <- lapply(cmp, function(x) if(is.factor(x)) factor(x) else x)
cmp <- cmp[order((cmp$unified_ID)),]
all(rownames(rabdc)==cmp$unified_ID)
barettin_cor <- bar_cor(rabdc, cmp)
#write.csv(barettin_cor, "data/SF_OTU_barettin_correlation.csv", row.names = F)
```


## Microbial interaction network

### Generating network based on different algorithms

The overall goal of the subsequent seqctions of code is to produce a microbial interation network for _Geodia barretti_. Nodes will be OTUs/ASVs from _Geodia barretti_ samples and edges represent an interaction between those OTUs/ASVs. In this first part, we will employ different algorithms for network building. Network building algorithms are mannifold and their results not uncontroversial, thus the recommended strategy is to use different methods and merge the resulting networks to one consensus representation (Weiss _et al_., 2016), which we will do in the second part.

The original data set from 14 specimens of _Geodia barretti_ contained 420 OTUs/ASVs. To reduce sparsity, we removed OTUs/ASVs with two or less non-zero values resulting in a data set containing 289 OTUs/ASVs. This data set was used for network inference with the following methods:

1. MENA Pipeline
2. fastLSA
3. SparCC
4. Maximal information coefficient MIC

#### Molecular Ecological Network Analysis (MENA) Pipeline
The implementation of MENA (Deng _et al_., 2012; Zhou, Deng _et al_., 2010; Zhou, Deng _et al_.,2011) can be accessed at http://ieg4.rccc.ou.edu/mena. The data set was saved as tab separated values and all zeros were converted to blanks.
No further filtering for non-zero values was done (more than two non-zero values). For data preparation, default settings were applied, i.e.  missing data was only filled with 0.01 in blanks with paired valid values, logarithm was taken, Pearson correlation coefficient was selected. Likewise, Random matrix theory settings were kept at defaults, decreasing the cutoff from the top using Regress Poisson distribution only. The cutoff of 0.800 was chosen for the similarity matrix to construct the network, corresponding to a Chi-square test on Poisson distribution of 99.191 and a p-value of 0.001. This resulted in a network with 241 nodes and 3582 edges.

A second analysis was produced with the same settings except building the similarity matrix based on Spearman's Rho. The cutoff of 0.820 was chosen for the similarity matrix to construct the network, corresponding to a Chi-square test on Poisson distribution of 98.417 and a p-value of 0.001. This resulted in a network with 252 nodes and 2216 edges.

Network properties and parameters are summarized in MENA_network_parameters_Feb2019.xlsx.

#### Local Similarity Analysis: fastLSA
The command line program for calcularing local similarity (Durno _et al_., 2013) was downloaded from http://hallam.microbiology.ubc.ca/fastLSA/install/index.html and run specifying the input file, no time lag (-d 0) and significance level alpha (-a 0.05). All other paramters were kept at their default values. The input data set was a tab delimited text file stripped of OTU labels or sample IDs.

```{bash fastLSA1, eval=F}
$ ./fastLSA -i ../gb_289_feb2019.txt -d 0 -a 0.05 -o ../gb_289_feb2019.out
```

The output file, as specified on the website, containes five columns. 'index1' and 'index2' represent the significant paired indices ranging from 0 to n-1 (OTUs/ASVs). LSA denotes the LSA statistic of each pair, lag was set to 0 with the -d flag and the p-valueBound column provides the p-value's upper boundary for the significantly paired p-value.

To produce comparable data sets, we replaced the indices with their OTU IDs and removed superfluous columns.
```{r fastLSA2, echo=T, eval=T}
fastLSA <- read.csv("data/gb_289_feb2019.out", header = T, sep = "")
key <- read.csv("data/fastLSA_index_otu_CORRECTEDfeb2019.csv", header=T, sep=";")

fastLSA$index1_otu <- key$fastLSA_OTU[match(fastLSA$index1, key$fastLSA_index)]
fastLSA$index2_otu <- key$fastLSA_OTU[match(fastLSA$index2, key$fastLSA_index)]

fastLSA$index1 <- NULL
fastLSA$index2 <- NULL
fastLSA$lag <- NULL
fastLSA$X <- NULL
fastLSA <- fastLSA[,c(3, 4, 1, 2)]
fastLSA <- fastLSA[order(fastLSA$p.valueBound),]

#write.csv(fastLSA, "fastLSA_for_networks.csv")
rm(key)
```

LSA scores range from -1 for strong negatively correlations to 1, for strong positive correlations. There were no negative correlations in this data setand we refrained from scaleding the LSA score furhter.
The resulting network contrained `r length(unique(c(fastLSA$index1_otu, fastLSA$index2_otu)))` nodes and `r (dim(fastLSA)[1])-1` edges.

#### SparCC
SparCC (Friedman and Alm, 2012) is a network building algorithm for compositional data and can be found at https://bitbucket.org/yonatanf/sparcc.

Prior to running it I had to get help as there was a minor issue during compilation. SparCC needed specific versions of numpy, panda and python to run properly, which is easiest accomodated in a specific environment. The OTU table needs to be windows formatted text. The embedded code is an example, for the analysis, 500 iterations were combined.

```{bash sparcc1, eval=FALSE, tidy=TRUE}
$ cd to working directory with SparCC and the data set gb_289.csv
$ source activate sparcc
python SparCC.py ../gb_sparcc.txt -c ../gb_sparcc_cor_file.txt -v 
../gb_sparcc_coverage_file.txt -i 5
$ deactivate
```

Create a results directory and redirect all the output there. Pseudo p-value Calculation, generates -n shuffled data sets:

```{bash sparcc2, eval=FALSE, tidy=TRUE}
$ mkdir results #creates output directory
$ python MakeBootstraps.py ../gb_sparcc.txt -n 5 -t permutation_#.txt -p ../results/ 
```

And run SparCC.py on all the re-shuffled data sets: 

```{bash sparcc3, eval=FALSE, tidy=TRUE}
$ python SparCC.py ../results/permutation_0.txt -i 5 --cor_file=../results/perm_cor_0.txt
$ python SparCC.py ../results/permutation_1.txt -i 5 --cor_file=../results/perm_cor_1.txt
$ python SparCC.py ../results/permutation_2.txt -i 5 --cor_file=../results/perm_cor_2.txt
$ python SparCC.py ../results/permutation_3.txt -i 5 --cor_file=../results/perm_cor_3.txt
$ python SparCC.py ../results/permutation_4.txt -i 5 --cor_file=../results/perm_cor_4.txt
```

Generate p-values: 

```{bash sparcc4, eval=FALSE, tidy=TRUE}
$ python PseudoPvals.py ../results/gb_sparcc_cor_file.txt ../results/perm_cor_#.txt 5 
-o ../results/pvals.two_sided.txt -t two_sided
```

Formatting the resulting data set like so:

```{r sparcc5, eval=F}
library(reshape2)
cor_file <- read.csv("data/gb_sparcc_cor_file_289.csv",  header=T, sep=";")
p_vals <- read.csv("data/gb_289_pvals.two_sided.csv",  header=T, sep=";")

# make OTU ID the rowname
rownames(cor_file) <- cor_file[,1]
cor_file[,1] <- NULL
rownames(p_vals) <- p_vals[,1]
p_vals[,1] <- NULL
# check ds congruency
all(colnames(cor_file)==rownames(cor_file))
all(colnames(p_vals)==rownames(p_vals))

# melt into long format, all vs all comparison: 289^2=83521 rows
cor_file_m <- melt(as.matrix(cor_file))
p_vals_m <- melt(as.matrix(p_vals))

all(cor_file_m$Var1==p_vals_m$Var1) 
all(cor_file_m$Var2==p_vals_m$Var2) 

#complete data set with p_vals and "correlation coeff"
cor_file_m["p_vals"] <- p_vals_m$value

# This removes AB - BA duplicates but still contains self comprisons, AA, BB,CC etc.
cols <- c("Var1", "Var2")
newdf <- cor_file_m[,cols] #generate new data set with just those two
#a <- Sys.time()
for (i in 1:nrow(cor_file_m)){
  newdf[i, ] = sort(cor_file_m[i,cols])
}
#b <- Sys.time()
#b-a

cor_file_shortened<- cor_file_m[!duplicated(newdf),] #and can be removed with duplicate
cor_file_shortened <- cor_file_shortened[which(cor_file_shortened$Var1 != cor_file_shortened$Var2),] # removing self comparison
colnames(cor_file_shortened) <- c("Var1_SparCC", "Var2_SparCC", "SparCC", "pSparCC") #41616
write.csv(cor_file_shortened, "data/SparCC_for_networks.csv")
rm(cor_file_m, newdf, p_vals_m, i, cols)
```

#### Maximal information coefficient MIC 

MIC for pairwise interaction was calculated with the R package minearva. The MIC is part of a statistic called Maximal Information-Based Nonparametric Exploration (MINE).

```{r MIC1, tidy=TRUE}
library(minerva)

OTU <- read.csv("data/gb_289.csv", header=T, sep=";")
rownames(OTU) <- OTU[,1]
OTU[,1] <- NULL
OTU <- as.data.frame(t(OTU))

# Calculate MIC of original data set. 
MINE <- mine(OTU)
MIC <- MINE$MIC #dim(MIC): 289 289
```

Obtaining p-values for this statistic can be achieved by permutation of the original OTU table as below or empirically, by selecting the thousand strongest interactions.

```{r MIC2, eval=F}
# 10 needs to be replaced with 1000 for final version, three times!!!
# reshuffling the OTU table, saving the MIC to a list, a total of 1000 times
n <- 0
results <- list()

#For reproducibility, one could e.g.:
#set.seed(1984)

#c <- Sys.time()
while (n<1000){
  n <- n+1
  mock <- apply(OTU,MARGIN = 2,sample)
  mock_mine <- mine(mock)
  results[[n]] <- mock_mine$MIC
}
#d <- Sys.time()
#d-c

#for every element of the true matrix, go through all the same elements in the 1000 generated mock matrix MIC indices and count how many of those are greater.
MIC <- MINE$MIC

e_values <- matrix(nrow = nrow(MIC), ncol = ncol(MIC), data = 0)
for(i in 1:nrow(MIC)){
  for (j in 1:ncol(MIC)){
    n <- 0
    while (n<1000){
      n <- n+1
      if(results[[n]][i,j]>= MIC[i,j]){
        e_values[i, j] <- e_values[i, j]+1
      }
    }
  }
}

e_values <- e_values/1000
#write.csv(e_values, "data/MIC_e_values.csv")
```

For n=1000, the first part takes about 8 mins on one core, the second part about 2 min. The relevant output is saved in the initial calculations of the MIC and the corresponding e-values are in e_values. These are symmetric matrices that will be reduced to a long table with unique OTUs/ASVs pairs, their MIC and the e-value. For clarity, all other files are removed.

```{r MIC3, eval=F}
rm(MINE, mock, mock_mine, results)

# transforming symmetric matrix to unique-pair long format
cor_file <- data.frame(MIC)
p_vals <- data.frame(e_values)

# inspect the files, adapt them and test congruency
rownames(p_vals) <- rownames(cor_file)
colnames(p_vals) <- colnames(cor_file)
all(colnames(cor_file)==rownames(cor_file))

# melt into long format, all vs all comparison: 289*289=83521 rows
cor_file_m <- melt(as.matrix(cor_file))
all(colnames(p_vals)==rownames(p_vals))
p_vals_m <- melt(as.matrix(p_vals))

#complete data set with p_vals
cor_file_m["p_vals"] <- p_vals_m$value

# This removes AB - BA duplicates but still contains self comprisons, AA, BB, etc.
cols <- c("Var1", "Var2")
newdf <- cor_file_m[,cols] #generate new data set with just those two

for (i in 1:nrow(cor_file_m)){
  newdf[i, ] <-  sort(cor_file_m[i,cols])
}

cor_file_shortened<- cor_file_m[!duplicated(newdf),] #and can be removed with duplicate
cor_file_shortened <- cor_file_shortened[which(cor_file_shortened$Var1 != cor_file_shortened$Var2),]# removing self comparison
rm(cor_file_m, newdf, p_vals_m, i, cols)

#write.csv(cor_file_shortened, "data/MIC_for_networks.csv")
```

### Consolidation of the different networks

#### MIC

MIC allows to detect a variety of interactions. According to the manual of the R wrapper minerva, the resulting MIC score "is related to the relationship strenght and it can be interpreted as a correlation measure. It is symmetric and it ranges in [0,1], where it tends to 0 for statistically independent data and it approaches 1 in probability for noiseless functional relationships". Thus, it also contains strong negative relationship up to mutual exclusivity, which we want to filter out.

```{r MIC4}
mic <- read.csv("data/MIC_for_networks.csv", header=T, sep=",")
mic$X <- NULL
colnames(mic) <- c("node1_mic", "node2_mic", "MIC", "pMIC")
```

Initially, the MIC network generated by the R wrapper minerva contained MIC values for all possible edges (i.e. `r nrow(mic)`). Of those, `r sum(mic$pMIC<=0.05)` edges/interactions had a p-value $\leq$ 0.05. As we will only include those edges in the final network, we select those and calculate the linear regression coefficient and p-value for the regression, to test whether we are able to distinguish negative from positive interaction.

```{r MIC5}
# Original OTU table for regressions
OTU <- read.csv("data/gb_289.csv", header=T, sep=";")
rownames(OTU) <- OTU[,1]
OTU[,1] <- NULL
OTU["ID"] <- row.names(OTU)
```

```{r MIC6, eval=F}
# goal: in mic data frame, set to 'NA' MIC and pMIC of edges with a significant p-value for MIC that have a significant negative regression

mic["regression"] <- NA
mic["p_regression"] <- NA

for (i in 1:nrow(mic)){
  bac1 <- factor(mic[i,1])
  bac2 <- factor(mic[i,2])
  temp_ds <- data.frame(t(rbind(OTU[OTU$ID==bac1,], OTU[OTU$ID==bac2,])))
  temp_ds<- temp_ds[-c(15),]
  temp_ds[] <- lapply(temp_ds, function(x) if(is.factor(x)) factor(x) else x) # removes factors, not sure if necessary
  mic$regression[i] <-  summary(lm(c(temp_ds[,1])~ c(temp_ds[,2])))$coefficients[2,1] #slope
  mic$p_regression[i] <-  summary(lm(c(temp_ds[,1])~ c(temp_ds[,2])))$coefficients[2,4] #p-val
}

before <- sum(mic$pMIC<=0.05) #4370
mic$MIC <- ifelse((mic$pMIC<=0.05 & mic$regression<0 & mic$p_regression<=0.05), NA, mic$MIC)
mic$pMIC <- ifelse((mic$pMIC<=0.05 & mic$regression<0 & mic$p_regression<=0.05), NA, mic$pMIC)
after <- sum(mic$pMIC<=0.05, na.rm=T) #3522

write.csv(mic, "data/MIC.csv")
rm(temp_ds, bac1, bac2, i, mic)
```

The MIC data set initially contained 41616 edges, 4370 of which were significant prior to the removal of negative correlations and leaving 3522 edges with a p-value  $\leq$ 0.05.

#### SparCC

The next network data set is based on the SparCC algorithm for computing correlations in compositional data.

```{r SparCC}
sparcc <- read.csv("data/SparCC_for_networks.csv", header=T, sep=",")
sparcc$X <- NULL
colnames(sparcc) <- c("node1_sparcc", "node2_sparcc", "SparCC", "pSparCC")
#hist(sparcc$SparCC)
#hist(sparcc$pSparCC)
sparcc$pSparCC <- ifelse((sparcc$SparCC<0), NA, sparcc$pSparCC) #setting the p-values of negative interactions to NA
sparcc$SparCC <- ifelse((sparcc$SparCC<0), NA, sparcc$SparCC) #setting negative interactions to NA
```

The network based on the SparCC algorithm contained `r nrow(sparcc)` edges of which `r sum(is.na(sparcc$SparCC))` negative interactions that were removed. `r sum(sparcc$pSparCC<=0, na.rm=T)` significant positive edges remain.

#### MENA

The next two network data sets are generated by MENA based on random matrix theory.

```{r MENA1}
mena_pcc <- read.csv("data/MENA_0.800_PCC_edge_attribute.txt", header=F, sep=" ")
# "np" in V2 and -1 in V5 mean negative interaction, these should be removed.
dim(mena_pcc)[1]-dim(mena_pcc[mena_pcc$V5==-1,])[1] # Number of pos interactions
mena_pcc["pMENA_PCC"] <- ifelse((mena_pcc$V5<0), NA, 0.001)
mena_pcc$V2 <- NULL
mena_pcc$V4 <- NULL
mena_pcc$V5 <- NULL
colnames(mena_pcc) <- c("node1_mena", "node2_mena", "pMENA_PCC")

mena_scc <- read.csv("data/MENA_0.820_SCC_edge_attribute.txt", header=F, sep=" ")
dim(mena_scc)[1]-dim(mena_scc[mena_scc$V5==-1,])[1] # Number of pos interactions
mena_scc["pMENA_SCC"] <- ifelse((mena_scc$V5<0), NA, 0.001)
mena_scc$V2 <- NULL
mena_scc$V4 <- NULL
mena_scc$V5 <- NULL
colnames(mena_scc) <- c("node1_mena", "node2_mena", "pMENA_SCC")
```

MENA network with Pearson correlation contained `r nrow(mena_pcc)` edges of which `r sum(is.na(mena_pcc$pMENA_PCC))` negative interactions were removed. For Spearman correlations, the network contained `r nrow(mena_scc)` edges of which `r sum(is.na(mena_scc$pMENA_SCC))` negative interactions were removed.

#### LSA

The next network data set is based on local similarity. It does not contain any negative values for LSA, so we do not exclude any edges.

```{r LSA}
lsa <- read.csv("data/fastLSA_for_networks.csv", header=T, sep=",")
lsa$X <- NULL
colnames(lsa) <- c("node1_lsa", "node2_lsa", "LSA", "pLSA") 
dim(lsa)[1] # Number of edges
```

#### Integration of the networks

Now we combine all five networks into one data set.

```{r integration1, eval=FALSE}
mic <- read.csv("data/MIC.csv", header=T)
mic$X <- NULL
mic$regression <- NULL
mic$p_regression <- NULL
master_summary <- mic

library(dplyr)
master_summary <- full_join(master_summary,lsa, by=c("node1_mic"="node2_lsa","node2_mic"="node1_lsa")) 
#sum(!is.na(master_summary$pLSA))==nrow(lsa) #TRUE
master_summary <- full_join(master_summary,sparcc, by=c("node1_mic"="node1_sparcc","node2_mic"="node2_sparcc")) 
#sum(!is.na(master_summary$pSparCC))==sum(!is.na(sparcc$pSparCC)) #TRUE
master_summary <- full_join(master_summary,mena_pcc, by=c("node1_mic"="node2_mena","node2_mic"="node1_mena")) 
#sum(!is.na(master_summary$pMENA_PCC))==sum(!is.na(mena_pcc$pMENA_PCC)) #TRUE
master_summary <- full_join(master_summary,mena_scc, by=c("node1_mic"="node2_mena","node2_mic"="node1_mena")) 
#sum(!is.na(master_summary$pMENA_SCC))==sum(!is.na(mena_scc$pMENA_SCC)) #TRUE
master_summary <- master_summary[,c(1, 2, 4, 6, 8, 9, 10, 3, 5, 7)] #reorder columns
head(master_summary)
#write.csv(master_summary, "data/master_summary_networks_1.csv", row.names = FALSE)
```


```{r integration2, out.width='80%', eval=T}
# For p-value merging: metap::sumlog, or EmpiricalBrownsMethod::EBM
library(metap)

ms <- read.csv("data/master_summary_networks_1.csv", header=T)

par(mfrow=c(2,3))
hist(ms$pMIC)
hist(ms$pLSA) #xlim = range(0,1)
hist(ms$pSparCC)
plot(ms$pMENA_PCC)
plot(ms$pMENA_SCC)
par(mfrow=c(1,1))

#metap::sumlog doesn't think 0 is a valid p-value, replace all zeros with small non-zero values, e.g. half-minimum
ms$pSparCC[ms$pSparCC==0.0] <- 0.005
ms$pMIC[ms$pMIC==0.0] <- 0.0005

ms["NA_count"] <- NA
ms["signif_0.05"] <- NA
ms["signif_0.001"] <- NA
ms["sumlog"] <- NA

n <- 0
k <- dim(ms)[1]
while (n<k) {
  n <- n+1
  ms$NA_count[n] <-  sum(is.na(ms[n,3:7]))
  ms$signif_0.05[n] <-  sum(ms[n,3:7]<=0.05, na.rm=T)
  ms$signif_0.001[n] <-  sum(ms[n,3:7]<=0.001, na.rm=T)
  ifelse((ms$NA_count[n]<=3), (ms$sumlog[n] <- sumlog(ms[n,3:7][!is.na(ms[n,3:7])])$p), NA)
}
rm(k,n)

ms["p.adjust_Bonferroni"] <- p.adjust(ms$sumlog, method = "bonferroni")
ms["p.adjust_FDR"] <- p.adjust(ms$sumlog, method = "fdr") #aka Benjamini & Hochberg

par(mfrow=c(1,3))
hist(ms$signif_0.05, main="p-values <= 0.05", xlab="Counts per edge ", breaks = c(0,1,2,3,4,5), labels = TRUE) 
hist(ms$signif_0.001, main="p-values <= 0.001", xlab="Counts per edge ", breaks = c(0,1,2,3,4,5), labels = TRUE)
hist(ms$NA_count, main="NAs", xlab="Counts per edge ", breaks = c(0,1,2,3,4,5), labels = TRUE)
par(mfrow=c(1,1))

ds <- split(ms, ms$signif_0.05)
raw_nodes <- rbind(ds$`4`, ds$`5`) # Selection/inclusion criterion
rm(ds)

edges <- raw_nodes[,1:2]
nodes <- union(raw_nodes$node1_mic, raw_nodes$node2_mic)

write.csv(edges, "data/master_summary_networks_2.csv", row.names = F)

```

We selected all edges with at least 4 p-values $\leq$ 0.05 to visualise in the network. This consensus network has `r dim(edges)[1]` edges and `r length(nodes)` nodes. Below are two versions of the network. In the first version, all OTUs correlating with barettin are colour-coded by class. In the second version, all OTUs increasing/decreasing woth depth are highlighted by colour.

```{r igraph, fig.cap="Micrbial interaction networks highlighting OTUs correlating with barettin and OTUs correlating with depth."}
#THANK YOU: https://kateto.net/networks-r-igraph

edges <- read.csv("data/master_summary_networks_2.csv", header = T, sep = ",")
nodes <- data.frame(union(raw_nodes$node1_mic, raw_nodes$node2_mic))

# annotation data required: inc-dec/depth response, taxonomy, barettin corelation
# hex color code
depth <- read.csv("data/gb_OTUs_overall_rabdc_annotated.csv", header=T, sep=",")
taxonomy <- read.csv("data/microbiome_taxonomy.csv", header = T, sep = ";")
barettin <- read.csv("data/GB_OTU_barettin_correlation.csv", header = T, sep = ",")

# matching IDs
colnames(nodes) <- c("OTU_long")
nodes["OTU"] <- str_replace(nodes$OTU_long, "OTU196900", "")
depth["OTU"] <- str_replace(depth$XOTU, "X196900", "")
taxonomy["OTU"] <- str_replace(taxonomy$OTU_ID, "196900", "")
barettin["OTU"] <- str_replace(barettin$XOTU, "X196900", "")

# downsizing to relevant columns
#nodes$OTU_long <- NULL
depth <- depth[,c("ttest_pval", "ttest_fdr", "inc_dec_estimate", "inc_dec_p_val", "fdr", "classification", "OTU")]
colnames(depth) <- c("ttest_pval", "ttest_fdr", "inc_dec_estimate", "inc_dec_p_val", "inc_dec_fdr", "inc_dec_classification", "OTU")
barettin$XOTU <- NULL
taxonomy <- taxonomy[,c("Kingdom", "Phylum", "Class", "OTU")]
taxonomy[] <- lapply(taxonomy, str_trim)

nodes <- left_join(nodes, depth)
nodes <- left_join(nodes, barettin)
nodes <- left_join(nodes, taxonomy)

# Adding categorical information
nodes["barettin_c"] <- ifelse(nodes$barettin_estimate_P>0 & nodes$barettin_p_val_P<0.05, 1, 0) # for scaling node size
nodes["inc_dec_c"] <- 0
nodes$inc_dec_c[nodes$inc_dec_p_val<0.05] <- 1 # for scaling node size
nodes["inc_dec_c_group"] <- NA
nodes$inc_dec_c_group[nodes$inc_dec_c==1 & nodes$inc_dec_estimate>0]  <- c("deep") # for colouring shallow vs. deep
nodes$inc_dec_c_group[nodes$inc_dec_c==1 & nodes$inc_dec_estimate<0]  <- c("shallow") # for colouring shallow vs. deep
# Pearson pmcc > 0 &
#    p < 0.05   56
#    fdr < 0.05 22
# Spearman rho > 0 &
#    p < 0.05   42
#    fdr < 0.05  1

nodes[] <- lapply(nodes, function(x) if(is.factor(x)) factor(x) else x)

library(igraph)
net <- graph_from_data_frame(d=edges, vertices=nodes, directed=F) 
l <- layout_with_kk(net)

# 11 colourblind-friendly colours from https://medialab.github.io/iwanthue/

#Taxonomy coloring & barettin: Class
ecol <- rep("gray80", ecount(net))
vcol <- rep("grey40", vcount(net))
vcol[V(net)$Class=="Subgroup_26"] <- "#628ed6"
vcol[V(net)$Class=="Subgroup_15"] <- "#957d34"
vcol[V(net)$Class=="Subgroup_6"] <- "#45c097"
vcol[V(net)$Class=="Anaerolineae"] <- "#ba4758"
vcol[V(net)$Class=="JG30-KF-CM66"] <- "#b2467e"
vcol[V(net)$Class=="SAR202_clade"] <- "#5b3687"
vcol[V(net)$Class=="TK10"] <- "#ba5437"
vcol[V(net)$Class=="BD2-11_terrestrial_group"] <- "#69ab54"
vcol[V(net)$Class=="Alphaproteobacteria"] <- "#c777cb"
vcol[V(net)$Class=="JTB23"] <- "#c3a63e"
vcol[V(net)$Class=="Acidimicrobiia"] <- "#6a70d7"

colrs <-c("#628ed6","#957d34","#45c097","#ba4758","#b2467e","#5b3687","#ba5437","#69ab54","#c777cb","#c3a63e","#6a70d7")
V(net)$color <- colrs[V(net)$class]

# set vertex.label=V(net)$OTU for OTU numbers
plot(net, vertex.color=vcol, edge.color=ecol, vertex.size=V(net)$barettin_c*10, vertex.label=NA, layout=l) 
legend(x=1, y=0.5, c("Subgroup_26 (Acidobacteria)","Subgroup_15 (Acidobacteria)", "Subgroup_6 (Acidobacteria)", "Anaerolineae (Chloroflexi)", "JG30-KF-CM66 (Chloroflexi)", "SAR202_clade (Chloroflexi)", "TK10 (Chloroflexi)", "BD2-11_terrestrial_group (Gemmatimonadetes)", "Alphaproteobacteria (Proteobacteria)", "JTB23 (Proteobacteria)", "Acidimicrobiia (Actinobacteria)"), pt.bg=colrs, pch=21,col="#777777", pt.cex=1.5, cex=.7, bty="n", ncol=1)

# Depth by correlation
ecol <- rep("gray80", ecount(net))
vcol <- rep("grey40", vcount(net))
vcol[V(net)$inc_dec_c_group=="shallow"] <- "gold"
vcol[V(net)$inc_dec_c_group=="deep"] <- "blue"

V(net)$color <- colrs[V(net)$inc_dec_c_group]
colrs <-c("gold", "blue")

plot(net, vertex.color=vcol, edge.color=ecol, vertex.size=V(net)$inc_dec_c*10, vertex.label=NA, layout=l) 
legend(x=1, y=0.5, c("decreasing", "incerasing"), pt.bg=colrs, pch=21,col="#777777", pt.cex=1.5, cex=.7, bty="n", ncol=1)
```

## Shortlist

We believe the producer of barettin (and related compounds) to have the following properties:

 - common (average relative abundance > 0.25%)
 - specific to _G. barretti_
 - positively correlated with barettin

the OTUs below fulfill these criteria:
```{r shortlist}

#most annotation from NW
micro <- read.csv("data/OTU_all_R.csv", header = T, sep = ";")

OTU_prep_sqrt <- function(micro){
  rownames(micro) <- micro$Sample_ID
  micro$Sample_ID <- NULL
  #micro <- sqrt(micro)
  
  micro_gb <- micro[(str_sub(rownames(micro), 1,2)=="Gb"),]
  micro_sf <- micro[(str_sub(rownames(micro), 1,2)=="Sf"),]
  micro_wb <- micro[(str_sub(rownames(micro), 1,2)=="Wb"),]
  
  micro_gb <- micro_gb[,colSums(micro_gb!=0)>0] #removes columns that only contain 0
  micro_sf <- micro_sf[,colSums(micro_sf!=0)>0]
  micro_wb <- micro_wb[,colSums(micro_wb!=0)>0]
  micros <- list(gb=micro_gb, sf=micro_sf, wb=micro_wb)
  return(micros)  
}

micro_ds <- OTU_prep_sqrt(micro)

overall_rabdc <- function(micro){
  mic <- micro
  n <- 0
  k <- dim(mic)[1]
  mic["rowsum"] <- apply(mic, 1, sum)
  
  while (n<k) {
    n <- n+1
    mic[n,] <- mic[n,]/(mic$rowsum[n])
  }
  
  mic$rowsum <- NULL
  mic <- data.frame(t(mic))
  mic["avg_rel_abdc"] <- apply(mic, 1, mean)
  mic["occurrence"] <- ifelse(mic$avg>0.0025, "common", "rare")
  return(mic)
}

occurrence <- lapply(micro_ds, overall_rabdc)

depth <- read.csv("data/gb_OTUs_overall_rabdc_annotated.csv", header=T, sep=",")
taxonomy <- read.csv("data/microbiome_taxonomy.csv", header = T, sep = ";")
barettin <- read.csv("data/GB_OTU_barettin_correlation.csv", header = T, sep = ",")

# matching IDs
nodes["OTU"] <- str_replace(nodes$OTU_long, "OTU196900", "")
depth["OTU"] <- str_replace(depth$XOTU, "X196900", "")
taxonomy["OTU"] <- str_replace(taxonomy$OTU_ID, "196900", "")
barettin["OTU"] <- str_replace(barettin$XOTU, "X196900", "")
occurrence$gb["OTU"] <- str_replace(rownames(occurrence$gb), "X196900", "")

# downsizing to relevant columns
#nodes$OTU_long <- NULL
depth <- depth[,c("ttest_pval", "ttest_fdr", "inc_dec_estimate", "inc_dec_p_val", "fdr", "classification", "OTU")]
colnames(depth) <- c("ttest_pval", "ttest_fdr", "inc_dec_estimate", "inc_dec_p_val", "inc_dec_fdr", "inc_dec_classification", "OTU")
barettin$XOTU <- NULL
taxonomy[] <- lapply(taxonomy, str_trim)

# combining dfs
shortlist_gb <- left_join(occurrence$gb, depth)
shortlist_gb <- left_join(shortlist_gb, barettin)
shortlist_gb <- left_join(shortlist_gb, taxonomy)

# Exclude Sf OTUs
#intersect(colnames(micro_ds$gb), colnames(micro_ds$sf)) # shared OTUs
#length(intersect(colnames(micro_ds$gb), colnames(micro_ds$sf))) # 316

setdiff(colnames(micro_ds$gb), colnames(micro_ds$sf)) # Setdiff finds rows that appear in first table but not in second
length(setdiff(colnames(micro_ds$gb), colnames(micro_ds$sf))) # 104:ok!
gb_unique <- data.frame(setdiff(colnames(micro_ds$gb), colnames(micro_ds$sf)))
colnames(gb_unique) <- c("XOTU")
gb_unique["OTU"] <- str_replace(gb_unique$XOTU, "X196900", "")

gb_unique <- left_join(gb_unique, shortlist_gb)

gb_unique <- gb_unique %>%
  filter(occurrence=="common")%>%
  filter(barettin_estimate_P>0 & barettin_p_val_P<0.05)

# Recovery of those OTUs that also correlate with barettin in S. fortis
sf_shared <- data.frame(intersect(colnames(micro_ds$gb), colnames(micro_ds$sf)))
colnames(sf_shared) <- c("XOTU")
sf_barretin <- read.csv("data/SF_OTU_barettin_correlation.csv", header=T, sep=",")

sf_shared <- left_join(sf_shared, sf_barretin)

# Sf
# Pearson
#  p<0.05     24
#  pFDR>0.05   8
# Spearman
#  p<0.05     14
#  pFDR>0.05   0

sf_shared <- sf_shared %>%
  filter(barettin_estimate_P>0)  %>%
  filter(barettin_p_val_P<0.05)  

sf_shared["OTU"] <- str_replace(sf_shared$XOTU,"X196900", "")
shared_recovered <- shortlist_gb[shortlist_gb$OTU %in% sf_shared$OTU,]

# check again for the criteria in the new df
shared_recovered <- shared_recovered  %>%
  filter(occurrence=="common")%>%
  filter(barettin_estimate_P>0 & barettin_p_val_P<0.05)

# combine the two data sets: gb_unique and shared_recovered

gb_unique["group"] <- c("gb_unique")
shared_recovered["group"] <- c("sf_shared")

gb_unique <- gb_unique[,c("OTU", "group", "Kingdom", "Phylum", "Class")]
shared_recovered <- shared_recovered[,c("OTU", "group", "Kingdom", "Phylum", "Class")]

shortlist <- rbind(gb_unique, shared_recovered)

options(kableExtra.html.bsTable = T)
kable(shortlist, 
      col.names = c("OTU","group", "Kingdom", "Phylum", "Class"), 
      longtable = T, booktabs = T, 
      caption = "Shortlist of OTUs that were deemed candidate producers of barettin.", 
      row.names=FALSE) %>%
  add_header_above(c(" " = 2, "Taxonomy" = 3)) %>%
  kable_styling(bootstrap_options = c("striped","hover", "bordered", "condensed", "responsive"), full_width = F,latex_options = c("striped", "scale_down"))

```

```{r shortlisted-OTU-bar}
micro <- read.csv("data/OTU_all_R.csv", header = T, sep = ";")
meta_data <- read.csv("data/Steffen_et_al_metadata_PANGAEA.csv", header=T, sep=";")
cmp <- read.csv("data/metabolite_master_20190605.csv", header=T, sep=",")

meta_data <- meta_data_prep(meta_data)

OTU_prep <- function(micro){
  rownames(micro) <- micro$Sample_ID
  micro$Sample_ID <- NULL
  #micro <- sqrt(micro)
  
  micro_gb <- micro[(str_sub(rownames(micro), 1,2)=="Gb"),]
  micro_sf <- micro[(str_sub(rownames(micro), 1,2)=="Sf"),]
  micro_wb <- micro[(str_sub(rownames(micro), 1,2)=="Wb"),]
  
  micro_gb <- micro_gb[,colSums(micro_gb!=0)>0]
  micro_sf <- micro_sf[,colSums(micro_sf!=0)>0]
  micro_wb <- micro_wb[,colSums(micro_wb!=0)>0]
  micros <- list(gb=micro_gb, sf=micro_sf, wb=micro_wb)
  return(micros)  
}

micro_ds <- OTU_prep(micro)

overall_rabdc <- function(micros){
  mic <- micros
  n <- 0
  k <- dim(mic)[1]
  mic["rowsum"] <- apply(mic, 1, sum)
  
  while (n<k) {
    n <- n+1
    mic[n,] <- mic[n,]/(mic$rowsum[n])
  }
  
  mic$rowsum <- NULL
  mic <- data.frame(t(mic))
#  mic["avg_rel_abdc"] <- apply(mic, 1, mean)
#  mic["occurrence"] <- ifelse(mic$avg>0.0025, "common", "rare")
  return(mic)
}

rabdc <- lapply(micro_ds, overall_rabdc)

rabdc$gb[,c("avg_rel_abdc", "occurrence" )] <- list(NULL)

#Shortlist OTUs
sl <- rabdc$gb[c("X196900144", "X196900180", "X196900213", "X196900310", "X196900588", "X196900589"),]
sl <- data.frame(t(sl))
sl["unified_ID"] <- rownames(sl)
sl <- left_join(sl, meta_data[,c("unified_ID", "Depth")])
sl <- left_join(sl, cmp[,c("unified_ID", "bar")])
summary(lm(X196900144~bar, sl))
summary(lm(X196900180~bar, sl))
summary(lm(X196900213~bar, sl))
summary(lm(X196900310~bar, sl))
summary(lm(X196900588~bar, sl))
summary(lm(X196900589~bar, sl))
library(reshape2)
sl <- melt(sl, id.vars = c("unified_ID", "Depth", "bar"))
ggplot(sl, aes(x=Depth, y=value))+geom_point()+facet_grid(.~variable)

ggplot(sl, aes(x=bar, y=value))+geom_point()+geom_smooth(method='lm', formula= y~x)+facet_wrap(.~variable, scales="free")+theme_bw()+theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+xlab("Barettin signal")+ylab("OTU relative abundance")+scale_y_continuous(labels = scales::scientific)+scale_x_continuous(labels =scales::scientific)+ ggtitle("Shortlisted OTUs and barettin")

```

```{r sess04}
sessionInfo()
```

<!--chapter:end:04-Inter-omics.Rmd-->

