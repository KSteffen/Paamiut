[
["inter-omics.html", "4 Inter-omics 4.1 Mantel test and procrustes rotations 4.2 Microbial interaction network 4.3 Shortlist", " 4 Inter-omics The inter-omics analyses are comprised of three parts. In the first part, we evaluate congruency of the prokaryotic and metabolomic data set as a whole using numerical methods (Mantel test) and ordination (Procrustes rotation and Protest). The second part consists of generating a microbial interaction network and annotating it with depth response of the OTUs and correlation with the barettin signal. In the third part, we rank OTUs based on properties hypothesised to be true forthe producer of barettin. 4.1 Mantel test and procrustes rotations 4.1.1 Libraries and functions 4.1.2 Data sets # LOAD DATA micro &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) colnames(micro)[colnames(micro) == &quot;Sample_ID&quot;] &lt;- &quot;unified_ID&quot; micro &lt;- micro[order(micro$unified_ID), ] rownames(micro) &lt;- micro$unified_ID micro[&quot;spec&quot;] &lt;- str_sub(micro$unified_ID, 1, 2) meta_data &lt;- read.csv(&quot;data/PANGAEA_Final.csv&quot;, header = T, sep = &quot;;&quot;) masslynx &lt;- meta_data masslynx &lt;- masslynx[c(&quot;unified_ID&quot;, &quot;LC.MS.HILIC.positive&quot;, &quot;LC.MS.HILIC.negative&quot;, &quot;LC.MS.RP.positive&quot;, &quot;LC.MS.RP.negative&quot;)] colnames(masslynx) &lt;- c(&quot;unified_ID&quot;, &quot;H_p&quot;, &quot;H_n&quot;, &quot;R_p&quot;, &quot;R_n&quot;) masslynx &lt;- na.omit(masslynx) masslynx[&quot;HILIC_pos&quot;] &lt;- str_sub(masslynx$H_p, 1, -3) masslynx[&quot;HILIC_neg&quot;] &lt;- str_sub(masslynx$H_n, 1, -3) masslynx[&quot;RP_pos&quot;] &lt;- str_sub(masslynx$R_p, 1, -3) masslynx[&quot;RP_neg&quot;] &lt;- str_sub(masslynx$R_n, 1, -3) # load one set of experiments at a time, i.e. CLEANED, ION or PC_GROUPS ### CLEANED hilic_pos &lt;- read.csv(&quot;data/HILIC_pos_20190417_cleaned.csv&quot;, header = T, sep = &quot;,&quot;) hilic_neg &lt;- read.csv(&quot;data/HILIC_neg_20190421_cleaned.csv&quot;, header = T, sep = &quot;,&quot;) rp_pos &lt;- read.csv(&quot;data/RP_pos_20190421_cleaned.csv&quot;, header = T, sep = &quot;,&quot;) rp_neg &lt;- read.csv(&quot;data/RP_neg_20190422_cleaned.csv&quot;, header = T, sep = &quot;,&quot;) ### ION hilic_pos &lt;- read.csv(&quot;data/HILIC_pos_20190417_cleaned_MH.csv&quot;, header = T, sep = &quot;,&quot;) hilic_neg &lt;- read.csv(&quot;data/HILIC_neg_20190421_cleaned_MH.csv&quot;, header = T, sep = &quot;,&quot;) rp_pos &lt;- read.csv(&quot;data/RP_pos_20190421_cleaned_MH.csv&quot;, header = T, sep = &quot;,&quot;) rp_neg &lt;- read.csv(&quot;data/RP_neg_20190422_cleaned_MH.csv&quot;, header = T, sep = &quot;,&quot;) ### PC_GROUPS hilic_pos &lt;- read.csv(&quot;data/HILIC_pos_20190417_cleaned_pcgroup.csv&quot;, header = T, sep = &quot;,&quot;) hilic_neg &lt;- read.csv(&quot;data/HILIC_neg_20190421_cleaned_pcgroup.csv&quot;, header = T, sep = &quot;,&quot;) rp_pos &lt;- read.csv(&quot;data/RP_pos_20190421_cleaned_pcgroup.csv&quot;, header = T, sep = &quot;,&quot;) rp_neg &lt;- read.csv(&quot;data/RP_neg_20190422_cleaned_pcgroup.csv&quot;, header = T, sep = &quot;,&quot;) 4.1.3 Mantel test code and tabular output ### =============================== MANTEL TEST================================= # run one of this at a time metabolomes &lt;- gymnastics(hilic_pos, &quot;H_p&quot;) metabolomes &lt;- gymnastics(hilic_neg, &quot;H_n&quot;) metabolomes &lt;- gymnastics(rp_pos, &quot;R_p&quot;) metabolomes &lt;- gymnastics(rp_neg, &quot;R_n&quot;) # run this congruent_dfs &lt;- congruency(metabolomes) # run one of these: MANTEL TEST diagnostics_hp_cleaned &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;HILIC pos&quot;, filtering = &quot;cleaned&quot;) diagnostics_hn_cleaned &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;HILIC neg&quot;, filtering = &quot;cleaned&quot;) diagnostics_rp_cleaned &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;RP pos&quot;, filtering = &quot;cleaned&quot;) diagnostics_rn_cleaned &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;RP neg&quot;, filtering = &quot;cleaned&quot;) diagnostics_hp_ion &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;HILIC pos&quot;, filtering = &quot;ion&quot;) diagnostics_hn_ion &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;HILIC neg&quot;, filtering = &quot;ion&quot;) diagnostics_rp_ion &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;RP pos&quot;, filtering = &quot;ion&quot;) diagnostics_rn_ion &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;RP neg&quot;, filtering = &quot;ion&quot;) diagnostics_hp_pc_group &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;HILIC pos&quot;, filtering = &quot;pc_group&quot;) diagnostics_hn_pc_group &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;HILIC neg&quot;, filtering = &quot;pc_group&quot;) diagnostics_rp_pc_group &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;RP pos&quot;, filtering = &quot;pc_group&quot;) diagnostics_rn_pc_group &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;RP neg&quot;, filtering = &quot;pc_group&quot;) ### Combine all test results into one file diagnostics &lt;- diagnostics_hp_cleaned diagnostics &lt;- rbind(diagnostics, diagnostics_hn_cleaned, diagnostics_rp_cleaned, diagnostics_rn_cleaned, diagnostics_hp_pc_group, diagnostics_hn_pc_group, diagnostics_rp_pc_group, diagnostics_rn_pc_group, diagnostics_hp_ion, diagnostics_hn_ion, diagnostics_rp_ion, diagnostics_rn_ion) diagnostics write.csv(diagnostics, &quot;mantel_stats_FUN.csv&quot;) diagnostics &lt;- read.csv(&quot;data/mantel_stats_FUN.csv&quot;) diagnostics$X &lt;- NULL diagnostics &lt;- diagnostics[, c(&quot;statistic&quot;, &quot;signif&quot;, &quot;micro_samples&quot;, &quot;micro_OTUs&quot;, &quot;meta_samples&quot;, &quot;meta_features&quot;, &quot;Sponge.species&quot;, &quot;Experiment&quot;, &quot;data.set&quot;)] options(kableExtra.html.bsTable = T) kable(diagnostics, col.names = c(&quot;Mantel statistic r&quot;, &quot;significance&quot;, &quot;N microbiome samples&quot;, &quot;N OTUs&quot;, &quot;N metabolome samples&quot;, &quot;N features&quot;, &quot;Sponge species&quot;, &quot;Experiment&quot;, &quot;data set&quot;), longtable = T, booktabs = T, caption = &quot;Mantel test diagnostics diagnositcs comparing the microbiome and metabolome of the same sponge specimens&quot;, row.names = FALSE) %&gt;% add_header_above(c(Diagnostics = 6, `Data set attribution` = 3)) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), font_size = 12, full_width = F, latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;)) (#tab:io4_mantel_results)Mantel test diagnostics diagnositcs comparing the microbiome and metabolome of the same sponge specimens Diagnostics Data set attribution Mantel statistic r significance N microbiome samples N OTUs N metabolome samples N features Sponge species Experiment data set 0.6076416 0.004 10 420 10 3507 Geodia barretti HILIC pos cleaned 0.3857788 0.011 13 461 13 3507 Stryphnus fortis HILIC pos cleaned 0.4576405 0.008 15 135 15 3507 Weberella bursa HILIC pos cleaned 0.4516469 0.006 10 420 10 2808 Geodia barretti HILIC neg cleaned 0.1513297 0.186 13 461 13 2808 Stryphnus fortis HILIC neg cleaned 0.1854966 0.168 15 135 15 2808 Weberella bursa HILIC neg cleaned -0.2014493 0.833 10 420 10 4673 Geodia barretti RP pos cleaned 0.3127632 0.064 13 461 13 4673 Stryphnus fortis RP pos cleaned 0.3323657 0.035 15 135 15 4673 Weberella bursa RP pos cleaned -0.2465894 0.845 9 420 9 3166 Geodia barretti RP neg cleaned 0.1960950 0.173 13 461 13 3166 Stryphnus fortis RP neg cleaned 0.1721750 0.174 15 135 15 3166 Weberella bursa RP neg cleaned 0.5836627 0.009 10 420 10 2212 Geodia barretti HILIC pos pc_group 0.4082626 0.016 13 461 13 2212 Stryphnus fortis HILIC pos pc_group 0.4483931 0.012 15 135 15 2212 Weberella bursa HILIC pos pc_group 0.3951252 0.018 10 420 10 1351 Geodia barretti HILIC neg pc_group 0.3211093 0.040 13 461 13 1351 Stryphnus fortis HILIC neg pc_group 0.0315675 0.414 15 135 15 1351 Weberella bursa HILIC neg pc_group -0.1600791 0.774 10 420 10 2736 Geodia barretti RP pos pc_group 0.3181249 0.064 13 461 13 2736 Stryphnus fortis RP pos pc_group 0.3008501 0.067 15 135 15 2736 Weberella bursa RP pos pc_group -0.1879022 0.790 9 420 9 1678 Geodia barretti RP neg pc_group 0.2034548 0.156 13 461 13 1678 Stryphnus fortis RP neg pc_group 0.1507257 0.198 15 135 15 1678 Weberella bursa RP neg pc_group 0.3351779 0.050 10 420 10 105 Geodia barretti HILIC pos ion 0.2908357 0.086 13 461 13 105 Stryphnus fortis HILIC pos ion 0.4683599 0.010 15 135 15 105 Weberella bursa HILIC pos ion 0.4429513 0.005 10 420 10 123 Geodia barretti HILIC neg ion 0.0727247 0.320 13 461 13 123 Stryphnus fortis HILIC neg ion 0.2320236 0.115 15 135 15 123 Weberella bursa HILIC neg ion -0.2569170 0.908 10 420 10 171 Geodia barretti RP pos ion 0.3070980 0.083 13 461 13 171 Stryphnus fortis RP pos ion 0.2656853 0.072 15 135 15 171 Weberella bursa RP pos ion -0.1220077 0.703 9 420 9 105 Geodia barretti RP neg ion -0.0505950 0.597 13 461 13 105 Stryphnus fortis RP neg ion 0.1218225 0.217 15 135 15 105 Weberella bursa RP neg ion dig &lt;- diagnostics[diagnostics$signif &lt;= 0.05, ] a &lt;- aggregate(dig, by = list(dig$Sponge.species, dig$Experiment, dig$data.set), FUN = &quot;length&quot;) summary(a$Group.1) ## Geodia barretti Stryphnus fortis Weberella bursa ## 6 3 4 summary(a$Group.2) ## HILIC neg HILIC pos RP neg RP pos ## 4 8 0 1 summary(a$Group.3) ## cleaned ion pc_group ## 5 3 5 rm(diagnostics_hp_cleaned, diagnostics_hn_cleaned, diagnostics_rp_cleaned, diagnostics_rn_cleaned, diagnostics_hp_ion, diagnostics_hn_ion, diagnostics_rp_ion, diagnostics_rn_ion, diagnostics_hp_pc_group, diagnostics_hn_pc_group, diagnostics_rp_pc_group, diagnostics_rn_pc_group) rm(hilic_pos, hilic_neg, rp_pos, rp_neg) As we can see from the table, in 13 cases, the Mantel test returns a significant correlation between the two matrices. Above you can see the the significant tests broken down by sponge species, HPCL-experiment and filtering approach. 4.1.4 Procrustes rotation and protest code and tabular output ### =============================== PROTEST ==================================== # run one of this at a time metabolomes &lt;- gymnastics(hilic_pos, &quot;H_p&quot;) metabolomes &lt;- gymnastics(hilic_neg, &quot;H_n&quot;) metabolomes &lt;- gymnastics(rp_pos, &quot;R_p&quot;) metabolomes &lt;- gymnastics(rp_neg, &quot;R_n&quot;) # run this congruent_dfs &lt;- congruency(metabolomes) # run one of these: PROTEST TEST diagnostics_hp_cleaned &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;HILIC pos&quot;, filtering = &quot;cleaned&quot;) diagnostics_hn_cleaned &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;HILIC neg&quot;, filtering = &quot;cleaned&quot;) diagnostics_rp_cleaned &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;RP pos&quot;, filtering = &quot;cleaned&quot;) diagnostics_rn_cleaned &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;RP neg&quot;, filtering = &quot;cleaned&quot;) diagnostics_hp_ion &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;HILIC pos&quot;, filtering = &quot;ion&quot;) diagnostics_hn_ion &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;HILIC neg&quot;, filtering = &quot;ion&quot;) diagnostics_rp_ion &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;RP pos&quot;, filtering = &quot;ion&quot;) diagnostics_rn_ion &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;RP neg&quot;, filtering = &quot;ion&quot;) diagnostics_hp_pc_group &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;HILIC pos&quot;, filtering = &quot;pc_group&quot;) diagnostics_hn_pc_group &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;HILIC neg&quot;, filtering = &quot;pc_group&quot;) diagnostics_rp_pc_group &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;RP pos&quot;, filtering = &quot;pc_group&quot;) diagnostics_rn_pc_group &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;RP neg&quot;, filtering = &quot;pc_group&quot;) ### Combine all test results into one file diagnostics &lt;- diagnostics_hp_cleaned diagnostics &lt;- rbind(diagnostics, diagnostics_hn_cleaned, diagnostics_rp_cleaned, diagnostics_rn_cleaned, diagnostics_hp_pc_group, diagnostics_hn_pc_group, diagnostics_rp_pc_group, diagnostics_rn_pc_group, diagnostics_hp_ion, diagnostics_hn_ion, diagnostics_rp_ion, diagnostics_rn_ion) diagnostics write.csv(diagnostics, &quot;protest_stats_FUN.csv&quot;) rm(diagnostics_hp_cleaned, diagnostics_hn_cleaned, diagnostics_rp_cleaned, diagnostics_rn_cleaned, diagnostics_hp_ion, diagnostics_hn_ion, diagnostics_rp_ion, diagnostics_rn_ion, diagnostics_hp_pc_group, diagnostics_hn_pc_group, diagnostics_rp_pc_group, diagnostics_rn_pc_group, diagnostics) rm(hilic_pos, hilic_neg, rp_pos, rp_neg) diagnostics &lt;- read.csv(&quot;data/protest_stats_FUN.csv&quot;) diagnostics$X &lt;- NULL diagnostics &lt;- diagnostics[, c(&quot;Procrustes.SS&quot;, &quot;correlation.in.sym..rotation&quot;, &quot;signif&quot;, &quot;micro_samples&quot;, &quot;micro_OTUs&quot;, &quot;meta_samples&quot;, &quot;meta_features&quot;, &quot;Sponge.species&quot;, &quot;Experiment&quot;, &quot;data.set&quot;)] options(kableExtra.html.bsTable = T) kable(diagnostics, col.names = c(&quot;Procrustes sum of squares&quot;, &quot;correlation in symmetric rotation&quot;, &quot;significance&quot;, &quot;N microbiome samples&quot;, &quot;N OTUs&quot;, &quot;N metabolome samples&quot;, &quot;N features&quot;, &quot;Sponge species&quot;, &quot;Experiment&quot;, &quot;data set&quot;), longtable = T, booktabs = T, caption = &quot;Protest diagnostics comparing the microbiome and metabolome of the same sponge specimens&quot;, row.names = FALSE) %&gt;% add_header_above(c(Diagnostics = 7, `Data set attribution` = 3)) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), font_size = 12, full_width = F, latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;)) (#tab:io6_prt)Protest diagnostics comparing the microbiome and metabolome of the same sponge specimens Diagnostics Data set attribution Procrustes sum of squares correlation in symmetric rotation significance N microbiome samples N OTUs N metabolome samples N features Sponge species Experiment data set 0.4901473 0.7140397 0.009 10 420 10 3507 Geodia barretti HILIC pos cleaned 0.3360196 0.8148499 0.001 13 461 13 3507 Stryphnus fortis HILIC pos cleaned 0.6451196 0.5957184 0.009 15 135 15 3507 Weberella bursa HILIC pos cleaned 0.5056719 0.7030847 0.015 10 420 10 2808 Geodia barretti HILIC neg cleaned 0.7144829 0.5343380 0.043 13 461 13 2808 Stryphnus fortis HILIC neg cleaned 0.6454847 0.5954119 0.003 15 135 15 2808 Weberella bursa HILIC neg cleaned 0.7241690 0.5251961 0.124 10 420 10 4673 Geodia barretti RP pos cleaned 0.5535784 0.6681479 0.005 13 461 13 4673 Stryphnus fortis RP pos cleaned 0.9089831 0.3016901 0.513 15 135 15 4673 Weberella bursa RP pos cleaned 0.7744081 0.4749652 0.229 9 420 9 3166 Geodia barretti RP neg cleaned 0.7769037 0.4723307 0.107 13 461 13 3166 Stryphnus fortis RP neg cleaned 0.9426954 0.2393838 0.656 15 135 15 3166 Weberella bursa RP neg cleaned 0.4903060 0.7139285 0.008 10 420 10 2212 Geodia barretti HILIC pos pc_group 0.6122646 0.6226840 0.009 13 461 13 2212 Stryphnus fortis HILIC pos pc_group 0.5611609 0.6624494 0.002 15 135 15 2212 Weberella bursa HILIC pos pc_group 0.4442919 0.7454583 0.007 10 420 10 1351 Geodia barretti HILIC neg pc_group 0.6072203 0.6267214 0.013 13 461 13 1351 Stryphnus fortis HILIC neg pc_group 0.7647882 0.4849864 0.062 15 135 15 1351 Weberella bursa HILIC neg pc_group 0.7332277 0.5165001 0.146 10 420 10 2736 Geodia barretti RP pos pc_group 0.5557670 0.6665081 0.002 13 461 13 2736 Stryphnus fortis RP pos pc_group 0.9184545 0.2855616 0.579 15 135 15 2736 Weberella bursa RP pos pc_group 0.7952845 0.4524550 0.314 9 420 9 1678 Geodia barretti RP neg pc_group 0.7513407 0.4986575 0.089 13 461 13 1678 Stryphnus fortis RP neg pc_group 0.9669093 0.1819085 0.851 15 135 15 1678 Weberella bursa RP neg pc_group 0.7744285 0.4749437 0.254 9 420 9 3166 Geodia barretti HILIC pos ion 0.7768298 0.4724090 0.119 13 461 13 3166 Stryphnus fortis HILIC pos ion 0.9427350 0.2393010 0.671 15 135 15 3166 Weberella bursa HILIC pos ion 0.3977778 0.7760297 0.005 10 420 10 123 Geodia barretti HILIC neg ion 0.7620196 0.4878323 0.101 13 461 13 123 Stryphnus fortis HILIC neg ion 0.6263664 0.6112558 0.007 15 135 15 123 Weberella bursa HILIC neg ion 0.6879735 0.5585933 0.083 10 420 10 171 Geodia barretti RP pos ion 0.6735153 0.5713884 0.038 13 461 13 171 Stryphnus fortis RP pos ion 0.9830213 0.1303022 0.961 15 135 15 171 Weberella bursa RP pos ion 0.7788793 0.4702347 0.271 9 420 9 105 Geodia barretti RP neg ion 0.9754054 0.1568265 0.767 13 461 13 105 Stryphnus fortis RP neg ion 0.9595716 0.2010681 0.791 15 135 15 105 Weberella bursa RP neg ion dig &lt;- diagnostics[diagnostics$signif &lt;= 0.05, ] a &lt;- aggregate(dig, by = list(dig$Sponge.species, dig$Experiment, dig$data.set), FUN = &quot;length&quot;) summary(a$Group.1) ## Geodia barretti Stryphnus fortis Weberella bursa ## 5 7 4 summary(a$Group.2) ## HILIC neg HILIC pos RP neg RP pos ## 7 6 0 3 summary(a$Group.3) ## cleaned ion pc_group ## 7 3 6 Out of the 36 tests performed, 16 are significant (p \\(\\leq\\) 0.05). Immediately above you can see the the significant tests broken down by sponge species, HPCL-experiment and filtering approach. 4.2 Microbial interaction network 4.2.1 Generating network based on different algorithms The overall goal of the subsequent seqctions of code is to produce a microbial interation network for Geodia barretti. Nodes will be OTUs/ASVs from Geodia barretti samples and edges represent an interaction between those OTUs/ASVs. In this first part, we will employ different algorithms for network building. Network building algorithms are mannifold and their results not uncontroversial, thus the recommended strategy is to use different methods and merge the resulting networks to one consensus representation (Weiss et al., 2016), which we will do in the second part. The original data set from 14 specimens of Geodia barretti contained 420 OTUs/ASVs. To reduce sparsity, we removed OTUs/ASVs with two or less non-zero values resulting in a data set containing 289 OTUs/ASVs. This data set was used for network inference with the following methods: MENA Pipeline fastLSA SparCC Maximal information coefficient MIC 4.2.1.1 Molecular Ecological Network Analysis (MENA) Pipeline The implementation of MENA (Deng et al., 2012; Zhou, Deng et al., 2010; Zhou, Deng et al.,2011) can be accessed at http://ieg4.rccc.ou.edu/mena. The data set was saved as tab separated values and all zeros were converted to blanks. No further filtering for non-zero values was done (more than two non-zero values). For data preparation, default settings were applied, i.e. missing data was only filled with 0.01 in blanks with paired valid values, logarithm was taken, Pearson correlation coefficient was selected. Likewise, Random matrix theory settings were kept at defaults, decreasing the cutoff from the top using Regress Poisson distribution only. The cutoff of 0.800 was chosen for the similarity matrix to construct the network, corresponding to a Chi-square test on Poisson distribution of 99.191 and a p-value of 0.001. This resulted in a network with 241 nodes and 3582 edges. A second analysis was produced with the same settings except building the similarity matrix based on Spearman’s Rho. The cutoff of 0.820 was chosen for the similarity matrix to construct the network, corresponding to a Chi-square test on Poisson distribution of 98.417 and a p-value of 0.001. This resulted in a network with 252 nodes and 2216 edges. Network properties and parameters are summarized in MENA_network_parameters_Feb2019.xlsx. 4.2.1.2 Local Similarity Analysis: fastLSA The command line program for calcularing local similarity (Durno et al., 2013) was downloaded from http://hallam.microbiology.ubc.ca/fastLSA/install/index.html and run specifying the input file, no time lag (-d 0) and significance level alpha (-a 0.05). All other paramters were kept at their default values. The input data set was a tab delimited text file stripped of OTU labels or sample IDs. $ ./fastLSA -i ../gb_289_feb2019.txt -d 0 -a 0.05 -o ../gb_289_feb2019.out The output file, as specified on the website, containes five columns. ‘index1’ and ‘index2’ represent the significant paired indices ranging from 0 to n-1 (OTUs/ASVs). LSA denotes the LSA statistic of each pair, lag was set to 0 with the -d flag and the p-valueBound column provides the p-value’s upper boundary for the significantly paired p-value. To produce comparable data sets, we replaced the indices with their OTU IDs and removed superfluous columns. fastLSA &lt;- read.csv(&quot;data/gb_289_feb2019.out&quot;, header = T, sep = &quot;&quot;) key &lt;- read.csv(&quot;data/fastLSA_index_otu_CORRECTEDfeb2019.csv&quot;, header = T, sep = &quot;;&quot;) fastLSA$index1_otu &lt;- key$fastLSA_OTU[match(fastLSA$index1, key$fastLSA_index)] fastLSA$index2_otu &lt;- key$fastLSA_OTU[match(fastLSA$index2, key$fastLSA_index)] fastLSA$index1 &lt;- NULL fastLSA$index2 &lt;- NULL fastLSA$lag &lt;- NULL fastLSA$X &lt;- NULL fastLSA &lt;- fastLSA[, c(3, 4, 1, 2)] fastLSA &lt;- fastLSA[order(fastLSA$p.valueBound), ] # write.csv(fastLSA, &#39;fastLSA_for_networks.csv&#39;) rm(key) LSA scores range from -1 for strong negatively correlations to 1, for strong positive correlations. There were no negative correlations in this data setand we refrained from scaleding the LSA score furhter. The resulting network contrained 129 nodes and 207 edges. 4.2.1.3 SparCC SparCC (Friedman and Alm, 2012) is a network building algorithm for compositional data and can be found at https://bitbucket.org/yonatanf/sparcc. Prior to running it I had to get help as there was a minor issue during compilation. SparCC needed specific versions of numpy, panda and python to run properly, which is easiest accomodated in a specific environment. The OTU table needs to be windows formatted text. The embedded code is an example, for the analysis, 500 iterations were combined. $ cd to working directory with SparCC and the data set gb_289.csv $ source activate sparcc python SparCC.py ../gb_sparcc.txt -c ../gb_sparcc_cor_file.txt -v ../gb_sparcc_coverage_file.txt -i 5 $ deactivate Create a results directory and redirect all the output there. Pseudo p-value Calculation, generates -n shuffled data sets: $ mkdir results #creates output directory $ python MakeBootstraps.py ../gb_sparcc.txt -n 5 -t permutation_#.txt -p ../results/ And run SparCC.py on all the re-shuffled data sets: $ python SparCC.py ../results/permutation_0.txt -i 5 --cor_file=../results/perm_cor_0.txt $ python SparCC.py ../results/permutation_1.txt -i 5 --cor_file=../results/perm_cor_1.txt $ python SparCC.py ../results/permutation_2.txt -i 5 --cor_file=../results/perm_cor_2.txt $ python SparCC.py ../results/permutation_3.txt -i 5 --cor_file=../results/perm_cor_3.txt $ python SparCC.py ../results/permutation_4.txt -i 5 --cor_file=../results/perm_cor_4.txt Generate p-values: $ python PseudoPvals.py ../results/gb_sparcc_cor_file.txt ../results/perm_cor_#.txt 5 -o ../results/pvals.two_sided.txt -t two_sided Formatting the resulting data set like so: library(reshape2) cor_file &lt;- read.csv(&quot;data/gb_sparcc_cor_file_289.csv&quot;, header = T, sep = &quot;;&quot;) p_vals &lt;- read.csv(&quot;data/gb_289_pvals.two_sided.csv&quot;, header = T, sep = &quot;;&quot;) # make OTU ID the rowname rownames(cor_file) &lt;- cor_file[, 1] cor_file[, 1] &lt;- NULL rownames(p_vals) &lt;- p_vals[, 1] p_vals[, 1] &lt;- NULL # check ds congruency all(colnames(cor_file) == rownames(cor_file)) all(colnames(p_vals) == rownames(p_vals)) # melt into long format, all vs all comparison: 289^2=83521 rows cor_file_m &lt;- melt(as.matrix(cor_file)) p_vals_m &lt;- melt(as.matrix(p_vals)) all(cor_file_m$Var1 == p_vals_m$Var1) all(cor_file_m$Var2 == p_vals_m$Var2) # complete data set with p_vals and &#39;correlation coeff&#39; cor_file_m[&quot;p_vals&quot;] &lt;- p_vals_m$value # This removes AB - BA duplicates but still contains self comprisons, AA, BB,CC # etc. cols &lt;- c(&quot;Var1&quot;, &quot;Var2&quot;) newdf &lt;- cor_file_m[, cols] #generate new data set with just those two # a &lt;- Sys.time() for (i in 1:nrow(cor_file_m)) { newdf[i, ] = sort(cor_file_m[i, cols]) } # b &lt;- Sys.time() b-a cor_file_shortened &lt;- cor_file_m[!duplicated(newdf), ] #and can be removed with duplicate cor_file_shortened &lt;- cor_file_shortened[which(cor_file_shortened$Var1 != cor_file_shortened$Var2), ] # removing self comparison colnames(cor_file_shortened) &lt;- c(&quot;Var1_SparCC&quot;, &quot;Var2_SparCC&quot;, &quot;SparCC&quot;, &quot;pSparCC&quot;) #41616 write.csv(cor_file_shortened, &quot;data/SparCC_for_networks.csv&quot;) rm(cor_file_m, newdf, p_vals_m, i, cols) 4.2.1.4 Maximal information coefficient MIC MIC for pariwise interaction was calculated with the R package minearva. The MIC is part of a statistic called Maximal Information-Based Nonparametric Exploration (MINE). library(minerva) OTU &lt;- read.csv(&quot;data/gb_289.csv&quot;, header = T, sep = &quot;;&quot;) rownames(OTU) &lt;- OTU[, 1] OTU[, 1] &lt;- NULL OTU &lt;- as.data.frame(t(OTU)) # Calculate MIC of original data set. MINE &lt;- mine(OTU) MIC &lt;- MINE$MIC #dim(MIC): 289 289 Obtaining p-values for this statistic can be achieved by premutation of the original OTU table as below or empirically, by selecting the thousand strongest interactions. # 10 needs to be replaced with 1000 for final version, three times!!! # reshuffling the OTU table, saving the MIC to a list, a total of 1000 times n &lt;- 0 results &lt;- list() # For reproducibility, one could e.g.: set.seed(1984) # c &lt;- Sys.time() while (n &lt; 1000) { n &lt;- n + 1 mock &lt;- apply(OTU, MARGIN = 2, sample) mock_mine &lt;- mine(mock) results[[n]] &lt;- mock_mine$MIC } # d &lt;- Sys.time() d-c # for every element of the true matrix, go through all the same elements in the # 1000 generated mock matrix MIC indices and count how many of those are greater. MIC &lt;- MINE$MIC e_values &lt;- matrix(nrow = nrow(MIC), ncol = ncol(MIC), data = 0) for (i in 1:nrow(MIC)) { for (j in 1:ncol(MIC)) { n &lt;- 0 while (n &lt; 1000) { n &lt;- n + 1 if (results[[n]][i, j] &gt;= MIC[i, j]) { e_values[i, j] &lt;- e_values[i, j] + 1 } } } } e_values &lt;- e_values/1000 # write.csv(e_values, &#39;data/MIC_e_values.csv&#39;) For n=1000, the first part takes about 8 mins on one core, the second part about 2 min. The relevant output is saved in the initial calculations of the MIC and the corresponding e-values are in e_values. These are symmetric matrices that will be reduced to a long table with unique OTUs/ASVs pairs, their MIC and the e-value. For clarity, all other files are removed. rm(MINE, mock, mock_mine, results) # transforming symmetric matrix to unique-pair long format cor_file &lt;- data.frame(MIC) p_vals &lt;- data.frame(e_values) # inspect the files, adapt them and test congruency rownames(p_vals) &lt;- rownames(cor_file) colnames(p_vals) &lt;- colnames(cor_file) all(colnames(cor_file) == rownames(cor_file)) # melt into long format, all vs all comparison: 289*289=83521 rows cor_file_m &lt;- melt(as.matrix(cor_file)) all(colnames(p_vals) == rownames(p_vals)) p_vals_m &lt;- melt(as.matrix(p_vals)) # complete data set with p_vals cor_file_m[&quot;p_vals&quot;] &lt;- p_vals_m$value # This removes AB - BA duplicates but still contains self comprisons, AA, BB, # etc. cols &lt;- c(&quot;Var1&quot;, &quot;Var2&quot;) newdf &lt;- cor_file_m[, cols] #generate new data set with just those two for (i in 1:nrow(cor_file_m)) { newdf[i, ] &lt;- sort(cor_file_m[i, cols]) } cor_file_shortened &lt;- cor_file_m[!duplicated(newdf), ] #and can be removed with duplicate cor_file_shortened &lt;- cor_file_shortened[which(cor_file_shortened$Var1 != cor_file_shortened$Var2), ] # removing self comparison rm(cor_file_m, newdf, p_vals_m, i, cols) # write.csv(cor_file_shortened, &#39;data/MIC_for_networks.csv&#39;) 4.2.2 Consolidation of the different networks 4.2.2.1 MIC MIC allows to detect a variety of interactions. According to the manual of the R wrapper minerva, the resulting MIC score “is related to the relationship strenght and it can be interpreted as a correlation measure. It is symmetric and it ranges in [0,1], where it tends to 0 for statistically independent data and it approaches 1 in probability for noiseless functional relationships”. Thus, it also contains strong negative relationship up to mutual exclusivity, which we want to filter out. mic &lt;- read.csv(&quot;data/MIC_for_networks.csv&quot;, header = T, sep = &quot;,&quot;) mic$X &lt;- NULL colnames(mic) &lt;- c(&quot;node1_mic&quot;, &quot;node2_mic&quot;, &quot;MIC&quot;, &quot;pMIC&quot;) Initially, the MIC network generated by the R wrapper minerva contained MIC values for all possible edges (i.e. 41616). Of those, 4370 edges/interactions had a p-value \\(\\leq\\) 0.05. As we will only include those edges in the final network, we select those and calculate the linear regression coefficient and p-value for the regression, to test whether we are able to distinguish negative from positive interaction. # Original OTU table for regressions OTU &lt;- read.csv(&quot;data/gb_289.csv&quot;, header = T, sep = &quot;;&quot;) rownames(OTU) &lt;- OTU[, 1] OTU[, 1] &lt;- NULL OTU[&quot;ID&quot;] &lt;- row.names(OTU) # goal: in mic data frame, set to &#39;NA&#39; MIC and pMIC of edges with a significant # p-value for MIC that have a significant negative regression mic[&quot;regression&quot;] &lt;- NA mic[&quot;p_regression&quot;] &lt;- NA for (i in 1:nrow(mic)) { bac1 &lt;- factor(mic[i, 1]) bac2 &lt;- factor(mic[i, 2]) temp_ds &lt;- data.frame(t(rbind(OTU[OTU$ID == bac1, ], OTU[OTU$ID == bac2, ]))) temp_ds &lt;- temp_ds[-c(15), ] temp_ds[] &lt;- lapply(temp_ds, function(x) if (is.factor(x)) factor(x) else x) # removes factors, not sure if necessary mic$regression[i] &lt;- summary(lm(c(temp_ds[, 1]) ~ c(temp_ds[, 2])))$coefficients[2, 1] #slope mic$p_regression[i] &lt;- summary(lm(c(temp_ds[, 1]) ~ c(temp_ds[, 2])))$coefficients[2, 4] #p-val } before &lt;- sum(mic$pMIC &lt;= 0.05) #4370 mic$MIC &lt;- ifelse((mic$pMIC &lt;= 0.05 &amp; mic$regression &lt; 0 &amp; mic$p_regression &lt;= 0.05), NA, mic$MIC) mic$pMIC &lt;- ifelse((mic$pMIC &lt;= 0.05 &amp; mic$regression &lt; 0 &amp; mic$p_regression &lt;= 0.05), NA, mic$pMIC) after &lt;- sum(mic$pMIC &lt;= 0.05, na.rm = T) #3522 write.csv(mic, &quot;data/MIC.csv&quot;) rm(temp_ds, bac1, bac2, i, mic) The MIC data set initially contained 41616 edges, 4370 of which were significant prior to the removal of negative correlations and leaving 3522 edges with a p-value \\(\\leq\\) 0.05. 4.2.2.2 SparCC The next network data set is based on the SparCC algorithm for computing correlations in compositional data. sparcc &lt;- read.csv(&quot;data/SparCC_for_networks.csv&quot;, header = T, sep = &quot;,&quot;) sparcc$X &lt;- NULL colnames(sparcc) &lt;- c(&quot;node1_sparcc&quot;, &quot;node2_sparcc&quot;, &quot;SparCC&quot;, &quot;pSparCC&quot;) # hist(sparcc$SparCC) hist(sparcc$pSparCC) sparcc$pSparCC &lt;- ifelse((sparcc$SparCC &lt; 0), NA, sparcc$pSparCC) #setting the p-values of negative interactions to NA sparcc$SparCC &lt;- ifelse((sparcc$SparCC &lt; 0), NA, sparcc$SparCC) #setting negative interactions to NA The network based on the SparCC algorithm contained 41616 edges of which 20420 negative interactions that were removed. 6622 significant positive edges remain. 4.2.2.3 MENA The next two network data sets are generated by MENA based on random matrix theory. mena_pcc &lt;- read.csv(&quot;data/MENA_0.800_PCC_edge_attribute.txt&quot;, header = F, sep = &quot; &quot;) # &#39;np&#39; in V2 and -1 in V5 mean negative interaction, these should be removed. dim(mena_pcc)[1] - dim(mena_pcc[mena_pcc$V5 == -1, ])[1] # Number of pos interactions ## [1] 171 mena_pcc[&quot;pMENA_PCC&quot;] &lt;- ifelse((mena_pcc$V5 &lt; 0), NA, 0.001) mena_pcc$V2 &lt;- NULL mena_pcc$V4 &lt;- NULL mena_pcc$V5 &lt;- NULL colnames(mena_pcc) &lt;- c(&quot;node1_mena&quot;, &quot;node2_mena&quot;, &quot;pMENA_PCC&quot;) mena_scc &lt;- read.csv(&quot;data/MENA_0.820_SCC_edge_attribute.txt&quot;, header = F, sep = &quot; &quot;) dim(mena_scc)[1] - dim(mena_scc[mena_scc$V5 == -1, ])[1] # Number of pos interactions ## [1] 317 mena_scc[&quot;pMENA_SCC&quot;] &lt;- ifelse((mena_scc$V5 &lt; 0), NA, 0.001) mena_scc$V2 &lt;- NULL mena_scc$V4 &lt;- NULL mena_scc$V5 &lt;- NULL colnames(mena_scc) &lt;- c(&quot;node1_mena&quot;, &quot;node2_mena&quot;, &quot;pMENA_SCC&quot;) MENA network with Pearson correlation contained 3582 edges of which 3411 negative interactions were removed. For Spearman correlations, the network contained 2216 edges of which 1899 negative interactions were removed. 4.2.2.4 LSA The next network data set is based on local similarity. It does not contain any negative values for LSA, so we do not exclude any edges. lsa &lt;- read.csv(&quot;data/fastLSA_for_networks.csv&quot;, header = T, sep = &quot;,&quot;) lsa$X &lt;- NULL colnames(lsa) &lt;- c(&quot;node1_lsa&quot;, &quot;node2_lsa&quot;, &quot;LSA&quot;, &quot;pLSA&quot;) dim(lsa)[1] # Number of edges ## [1] 208 4.2.2.5 Integration of the networks Now we combine all five networks into one data set. mic &lt;- read.csv(&quot;data/MIC.csv&quot;, header = T) mic$X &lt;- NULL mic$regression &lt;- NULL mic$p_regression &lt;- NULL master_summary &lt;- mic library(dplyr) master_summary &lt;- full_join(master_summary, lsa, by = c(node1_mic = &quot;node2_lsa&quot;, node2_mic = &quot;node1_lsa&quot;)) # sum(!is.na(master_summary$pLSA))==nrow(lsa) #TRUE master_summary &lt;- full_join(master_summary, sparcc, by = c(node1_mic = &quot;node1_sparcc&quot;, node2_mic = &quot;node2_sparcc&quot;)) # sum(!is.na(master_summary$pSparCC))==sum(!is.na(sparcc$pSparCC)) #TRUE master_summary &lt;- full_join(master_summary, mena_pcc, by = c(node1_mic = &quot;node2_mena&quot;, node2_mic = &quot;node1_mena&quot;)) # sum(!is.na(master_summary$pMENA_PCC))==sum(!is.na(mena_pcc$pMENA_PCC)) #TRUE master_summary &lt;- full_join(master_summary, mena_scc, by = c(node1_mic = &quot;node2_mena&quot;, node2_mic = &quot;node1_mena&quot;)) # sum(!is.na(master_summary$pMENA_SCC))==sum(!is.na(mena_scc$pMENA_SCC)) #TRUE master_summary &lt;- master_summary[, c(1, 2, 4, 6, 8, 9, 10, 3, 5, 7)] #reorder columns head(master_summary) # write.csv(master_summary, &#39;data/master_summary_networks_1.csv&#39;, row.names = # FALSE) # For p-value merging: metap::sumlog, or EmpiricalBrownsMethod::EBM library(metap) ms &lt;- read.csv(&quot;data/master_summary_networks_1.csv&quot;, header = T) head(ms) ## node1_mic node2_mic pMIC pLSA pSparCC pMENA_PCC pMENA_SCC MIC LSA ## 1 OTU196900135 OTU1969004 0.918 NA 0.0 NA NA 0.1601125 NA ## 2 OTU196900136 OTU1969004 0.068 NA 0.0 NA NA 0.5087257 NA ## 3 OTU196900137 OTU1969004 0.008 NA 0.6 NA NA 0.6052891 NA ## 4 OTU196900138 OTU1969004 0.222 NA 0.0 NA NA 0.3705065 NA ## 5 OTU196900139 OTU1969004 0.247 NA 0.8 NA NA 0.3705065 NA ## 6 OTU196900140 OTU1969004 0.015 NA 0.6 NA NA 0.6893917 NA ## SparCC ## 1 0.5372737 ## 2 0.6541941 ## 3 0.5449118 ## 4 -0.5931877 ## 5 0.0806820 ## 6 0.1745214 par(mfrow = c(2, 3)) hist(ms$pMIC) hist(ms$pLSA) #xlim = range(0,1) hist(ms$pSparCC) plot(ms$pMENA_PCC) plot(ms$pMENA_SCC) par(mfrow = c(1, 1)) # metap::sumlog doesn&#39;t think 0 is a valid p-value, replace all zeros with small # non-zero values, e.g. half-minimum ms$pSparCC[ms$pSparCC == 0] &lt;- 0.005 ms$pMIC[ms$pMIC == 0] &lt;- 5e-04 ms[&quot;NA_count&quot;] &lt;- NA ms[&quot;signif_0.05&quot;] &lt;- NA ms[&quot;signif_0.001&quot;] &lt;- NA ms[&quot;sumlog&quot;] &lt;- NA n &lt;- 0 k &lt;- dim(ms)[1] while (n &lt; k) { n &lt;- n + 1 ms$NA_count[n] &lt;- sum(is.na(ms[n, 3:7])) ms$signif_0.05[n] &lt;- sum(ms[n, 3:7] &lt;= 0.05, na.rm = T) ms$signif_0.001[n] &lt;- sum(ms[n, 3:7] &lt;= 0.001, na.rm = T) ifelse((ms$NA_count[n] &lt;= 3), (ms$sumlog[n] &lt;- sumlog(ms[n, 3:7][!is.na(ms[n, 3:7])])$p), NA) } rm(k, n) ms[&quot;p.adjust_Bonferroni&quot;] &lt;- p.adjust(ms$sumlog, method = &quot;bonferroni&quot;) ms[&quot;p.adjust_FDR&quot;] &lt;- p.adjust(ms$sumlog, method = &quot;fdr&quot;) #aka Benjamini &amp; Hochberg write.csv(ms, &quot;data/master_summary_networks_2.csv&quot;) par(mfrow = c(1, 3)) hist(ms$signif_0.05, main = &quot;p-values &lt;= 0.05&quot;, xlab = &quot;Counts per edge &quot;, breaks = c(0, 1, 2, 3, 4, 5), labels = TRUE) hist(ms$signif_0.001, main = &quot;p-values &lt;= 0.001&quot;, xlab = &quot;Counts per edge &quot;, breaks = c(0, 1, 2, 3, 4, 5), labels = TRUE) hist(ms$NA_count, main = &quot;NAs&quot;, xlab = &quot;Counts per edge &quot;, breaks = c(0, 1, 2, 3, 4, 5), labels = TRUE) par(mfrow = c(1, 1)) ds &lt;- split(ms, ms$signif_0.05) raw_nodes &lt;- rbind(ds$`4`, ds$`5`) # Selection/inclusion criterion rm(ds) We selected all edges with at least 4 p-values \\(\\leq\\) 0.05 (n=155) to visualise in the network. library(igraph) edges &lt;- raw_nodes[, 1:2] nodes &lt;- union(raw_nodes$node1_mic, raw_nodes$node2_mic) ### K### meta_data &lt;- read.csv(&quot;~/Documents/Metabolomics/Depth_Gradient_study/R_depth_study/Network_analyses_R/gb_attributes_barettin_vip2.csv&quot;, header = T, sep = &quot;;&quot;) # head(meta_data) meta_data &lt;- meta_data[meta_data$OTU_otu %in% nodes, ] # head(meta_data) str(meta_data) meta_data$OTU_num &lt;- NULL meta_data$order &lt;- NULL nodes &lt;- meta_data nodes$labels &lt;- NA nodes$labels &lt;- ifelse(nodes$bar_scale_abs == 1, as.character(nodes$OTU_otu), NA) # sum(!is.na(nodes$labels)) #18 nodes$labels head(nodes, n=20) nodes$labels &lt;- sub(pattern = &quot;OTU196900&quot;, replacement = &quot;&quot;, x = nodes$labels, ignore.case = F) nodes[] &lt;- lapply(nodes, function(x) if (is.factor(x)) factor(x) else x) net &lt;- graph_from_data_frame(d = edges, vertices = nodes, directed = F) # plot(net) l &lt;- layout_with_kk(net) plot(net, # edge.arrow.size=.4,vertex.label=NA, layout=l) #plot again # Barettin highligted (orange node size) l &lt;- layout_with_kk(net) plot(net, edge.arrow.size = 0.2, edge.color = &quot;grey&quot;, vertex.color = &quot;orange&quot;, vertex.frame.color = &quot;#ffffff&quot;, vertex.label = NA, vertex.label.color = &quot;black&quot;, vertex.size = V(net)$bar_scale_rel * 10, layout = l) # node info and classification for all nodes correlated with barettin node_info &lt;- nodes[, c(&quot;OTU_otu&quot;, &quot;phylum&quot;, &quot;class&quot;)][nodes$bar_scale_abs == 1, ] node_info[&quot;counter&quot;] &lt;- 1 #for quick grasp with aggregate node_info nrow(node_info) #18 node_info[] &lt;- lapply(node_info, function(x) if (is.factor(x)) factor(x) else x) #dropping factors from full data set for all columns node_info[order(node_info$phylum, node_info$class), ] aggregate(. ~ phylum, data = node_info, sum) aggregate(. ~ class, data = node_info, sum) # Taxonomy coloring &amp; barettin: Phylum ecol &lt;- rep(&quot;gray80&quot;, ecount(net)) vcol &lt;- rep(&quot;grey40&quot;, vcount(net)) vcol[V(net)$phylum == &quot;Proteobacteria&quot;] &lt;- &quot;yellow&quot; vcol[V(net)$phylum == &quot;Acidobacteria&quot;] &lt;- &quot;purple&quot; vcol[V(net)$phylum == &quot;Chloroflexi&quot;] &lt;- &quot;blue&quot; vcol[V(net)$phylum == &quot;Gemmatimonadetes&quot;] &lt;- &quot;red&quot; # The unclassified OTU196900307 will be displayed in grey. plot(net, vertex.color = vcol, edge.color = ecol, vertex.size = V(net)$bar_scale_rel * 10, edge.width = E(net)$value^2, vertex.label = NA, layout = l) # PUBLICATION FIGURE Taxonomy coloring &amp; barettin: Class ecol &lt;- rep(&quot;gray80&quot;, ecount(net)) vcol &lt;- rep(&quot;grey40&quot;, vcount(net)) vcol[V(net)$class == &quot;Holophagae&quot;] &lt;- &quot;#9e0142&quot; vcol[V(net)$class == &quot;Subgroup_15&quot;] &lt;- &quot;#d53e4f&quot; vcol[V(net)$class == &quot;Subgroup_6&quot;] &lt;- &quot;#f46d43&quot; vcol[V(net)$class == &quot;Anaerolineae&quot;] &lt;- &quot;#fdae61&quot; vcol[V(net)$class == &quot;JG30-KF-CM66&quot;] &lt;- &quot;#fee08b&quot; vcol[V(net)$class == &quot;SAR202_clade&quot;] &lt;- &quot;#e6f598&quot; vcol[V(net)$class == &quot;TK10&quot;] &lt;- &quot;#abdda4&quot; vcol[V(net)$class == &quot;BD2-11_terrestrial_group&quot;] &lt;- &quot;#66c2a5&quot; vcol[V(net)$class == &quot;Alphaproteobacteria&quot;] &lt;- &quot;#3288bd&quot; vcol[V(net)$class == &quot;JTB23&quot;] &lt;- &quot;#5e4fa2&quot; colrs &lt;- c(&quot;#9e0142&quot;, &quot;#d53e4f&quot;, &quot;#f46d43&quot;, &quot;#fdae61&quot;, &quot;#fee08b&quot;, &quot;#e6f598&quot;, &quot;#abdda4&quot;, &quot;#66c2a5&quot;, &quot;#3288bd&quot;, &quot;#5e4fa2&quot;) V(net)$color &lt;- colrs[V(net)$class] # PUBLICATION FIGURE plot(net, vertex.color = vcol, edge.color = ecol, vertex.size = V(net)$bar_scale_rel * 10, edge.width = E(net)$value^2, vertex.label = NA, layout = l) # legend(x=-2.1, y=0.5, c(&#39;Holophagae (Acidobacteria)&#39;,&#39;Subgroup_15 # (Acidobacteria)&#39;, &#39;Subgroup_6 (Acidobacteria)&#39;, &#39;Anaerolineae (Chloroflexi)&#39;, # &#39;JG30-KF-CM66 (Chloroflexi)&#39;, &#39;SAR202_clade (Chloroflexi)&#39;, &#39;TK10 # (Chloroflexi)&#39;, &#39;BD2-11_terrestrial_group (Gemmatimonadetes)&#39;, # &#39;Alphaproteobacteria (Proteobacteria)&#39;, &#39;JTB23 (Proteobacteria)&#39;), pt.bg=colrs, # pch=21,col=&#39;#777777&#39;, pt.cex=2, cex=.8, bty=&#39;n&#39;, ncol=1) legend(x = -2.9, y = 0.5, c(&quot;Holophagae (Acidobacteria)&quot;, &quot;Subgroup_15 (Acidobacteria)&quot;, &quot;Subgroup_6 (Acidobacteria)&quot;, &quot;Anaerolineae (Chloroflexi)&quot;, &quot;JG30-KF-CM66 (Chloroflexi)&quot;, &quot;SAR202_clade (Chloroflexi)&quot;, &quot;TK10 (Chloroflexi)&quot;, &quot;BD2-11_terrestrial_group (Gemmatimonadetes)&quot;, &quot;Alphaproteobacteria (Proteobacteria)&quot;, &quot;JTB23 (Proteobacteria)&quot;), pt.bg = colrs, pch = 21, col = &quot;#777777&quot;, pt.cex = 1.5, cex = 0.7, bty = &quot;n&quot;, ncol = 1) # Labels, personal use plot(net, vertex.color = vcol, edge.color = ecol, vertex.size = V(net)$bar_scale_rel * 10, edge.width = E(net)$value^2, vertex.label = V(net)$labels, layout = l) # legend(x=-2.1, y=0.5, c(&#39;Holophagae (Acidobacteria)&#39;,&#39;Subgroup_15 # (Acidobacteria)&#39;, &#39;Subgroup_6 (Acidobacteria)&#39;, &#39;Anaerolineae (Chloroflexi)&#39;, # &#39;JG30-KF-CM66 (Chloroflexi)&#39;, &#39;SAR202_clade (Chloroflexi)&#39;, &#39;TK10 # (Chloroflexi)&#39;, &#39;BD2-11_terrestrial_group (Gemmatimonadetes)&#39;, # &#39;Alphaproteobacteria (Proteobacteria)&#39;, &#39;JTB23 (Proteobacteria)&#39;), pt.bg=colrs, # pch=21,col=&#39;#777777&#39;, pt.cex=2, cex=.8, bty=&#39;n&#39;, ncol=1) legend(x = -2.9, y = 0.5, c(&quot;Holophagae (Acidobacteria)&quot;, &quot;Subgroup_15 (Acidobacteria)&quot;, &quot;Subgroup_6 (Acidobacteria)&quot;, &quot;Anaerolineae (Chloroflexi)&quot;, &quot;JG30-KF-CM66 (Chloroflexi)&quot;, &quot;SAR202_clade (Chloroflexi)&quot;, &quot;TK10 (Chloroflexi)&quot;, &quot;BD2-11_terrestrial_group (Gemmatimonadetes)&quot;, &quot;Alphaproteobacteria (Proteobacteria)&quot;, &quot;JTB23 (Proteobacteria)&quot;), pt.bg = colrs, pch = 21, col = &quot;#777777&quot;, pt.cex = 1.5, cex = 0.7, bty = &quot;n&quot;, ncol = 1) # VIPs l &lt;- layout_with_kk(net) plot(net, edge.arrow.size = 0.2, edge.color = &quot;grey&quot;, vertex.color = &quot;orange&quot;, vertex.frame.color = &quot;#ffffff&quot;, vertex.label = NA, vertex.label.color = &quot;black&quot;, vertex.size = V(net)$vip_scale_rel * 5, layout = l) 4.3 Shortlist We believe the producer of barettin (and related compounds) to have the following properties: common (average relative abundance &gt; 0.25%) specific to G. barretti positively correlated with barettin head(rabdc_gb) head(rabdc_sf) setdiff(rownames(rabdc_gb), rownames(rabdc_sf)) #Setdiff finds rows that appear in first table but not in second gb_unique &lt;- data.frame(setdiff(rownames(rabdc_gb), rownames(rabdc_sf))) colnames(gb_unique) &lt;- c(&quot;XOTU&quot;) sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS 10.15.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] metap_1.3 minerva_1.5.8 kableExtra_1.1.0.9000 ## [4] vegan_2.5-6 lattice_0.20-41 permute_0.9-5 ## [7] forcats_0.5.0 stringr_1.4.0 dplyr_0.8.5 ## [10] purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 ## [13] tibble_3.0.0 ggplot2_3.3.0 tidyverse_1.3.0 ## [16] knitr_1.28 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-145 fs_1.4.0 lubridate_1.7.4 ## [4] webshot_0.5.2 httr_1.4.1 numDeriv_2016.8-1.1 ## [7] tools_3.5.1 backports_1.1.5 R6_2.4.1 ## [10] BiocGenerics_0.28.0 DBI_1.1.0 mgcv_1.8-31 ## [13] colorspace_1.4-1 sn_1.6-1 withr_2.1.2 ## [16] tidyselect_1.0.0 mnormt_1.5-6 compiler_3.5.1 ## [19] Biobase_2.42.0 cli_2.0.2 rvest_0.3.5 ## [22] formatR_1.7 xml2_1.3.0 TFisher_0.2.0 ## [25] sandwich_2.5-1 bookdown_0.18 scales_1.1.0 ## [28] mvtnorm_1.1-0 digest_0.6.25 rmarkdown_2.1 ## [31] pkgconfig_2.0.3 htmltools_0.4.0 bibtex_0.4.2.2 ## [34] plotrix_3.7-7 dbplyr_1.4.2 highr_0.8 ## [37] rlang_0.4.5 readxl_1.3.1 rstudioapi_0.11 ## [40] generics_0.0.2 zoo_1.8-7 jsonlite_1.6.1 ## [43] magrittr_1.5 Matrix_1.2-18 Rcpp_1.0.4 ## [46] munsell_0.5.0 fansi_0.4.1 lifecycle_0.2.0 ## [49] stringi_1.4.6 multcomp_1.4-12 yaml_2.2.1 ## [52] gbRd_0.4-11 MASS_7.3-51.5 grid_3.5.1 ## [55] parallel_3.5.1 crayon_1.3.4 haven_2.2.0 ## [58] splines_3.5.1 multtest_2.38.0 hms_0.5.3 ## [61] pillar_1.4.3 codetools_0.2-16 stats4_3.5.1 ## [64] reprex_0.3.0 mutoss_0.1-12 glue_1.3.2 ## [67] evaluate_0.14 modelr_0.1.6 vctrs_0.2.4 ## [70] Rdpack_0.11-1 cellranger_1.1.0 gtable_0.3.0 ## [73] assertthat_0.2.1 xfun_0.13.1 broom_0.5.5 ## [76] survival_3.1-11 viridisLite_0.3.0 cluster_2.1.0 ## [79] TH.data_1.0-10 ellipsis_0.3.0 "]
]
