[
["index.html", "Oceanographic setting influences the prokaryote community and metabolome in deep-sea sponges 1 Content 1.1 Experimental setup 1.2 Sample metadata analysis", " Oceanographic setting influences the prokaryote community and metabolome in deep-sea sponges Karin Steffen June 18, 2020 1 Content This is a set of code and data sets to document and reproduce the computational analyses in Steffen et al., 2020. In brief, it contains three parts: the metabolome, microbiota and inter-omics analyses. For the metabolome, we include detailed descriptions of the metabolomics data acquisition, data processing with xcms, and multivariate analyses with ropls. In addition, signals of known and novel compounds of interest were manually extracted and analysed. For the microbiota, we include community visualisation and ecological analyses in vegan. The inter-omics section contains mantel test and procrustes rotations, as well as the microbial interaction network annotated with the OTU’s correlation with barettin, and their response to depth. We hope to contribute to good science by providing reproducible documentation of the computational workflow. If you have questions, comments or suggestions please feel free to get in touch. 1.1 Experimental setup Three demosponge species Geodia barretti (n=20), Stryphnus fortis (n=15), and Weberella bursa (n=17) (Fig 1.6) were sampled in the Davis Strait between Canada and Greenland (61.147942-66.38245 Lat,-68.78077- -57.96573 Lon) from 244 m to 1467 m depth (Fig 1.1, 1.2). Temperature and salinity in situ were recorded. All sample metadata is deposited at PANGAEA. All data for these analyses can be downloaded here. library(xfun) xfun::pkg_load2(c(&quot;base64enc&quot;, &quot;htmltools&quot;, &quot;mime&quot;)) xfun::embed_dir(&quot;data/&quot;, text = &quot;Download full data&quot;) 1.1.1 Map library(ggplot2) library(ggmap) library(maps) library(mapdata) library(marmap) library(ggrepel) library(sf) library(rnaturalearth) library(rnaturalearthdata) sample_coords &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) sample_coords &lt;- sample_coords[, c(&quot;Species&quot;, &quot;unified_ID&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;)] sample_coords &lt;- na.omit(sample_coords) world &lt;- ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) sample_map2 &lt;- ggplot(data = world) + geom_sf() + coord_sf(xlim = c(-95, 0), ylim = c(45, 75), expand = T) + geom_point(data = sample_coords, aes(x = sample_coords$Longitude, y = sample_coords$Latitude)) + annotate(&quot;rect&quot;, xmin = -68.78077, xmax = -57.96573, ymin = 61.147942, ymax = 66.38245, alpha = 0.2) + ggtitle(&quot;Sample map&quot;) + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) sample_map2 Figure 1.1: Geographic placement of the sample site on the Northern hemisphere, between Canada and Greenland. Dots indicate individual samples. map_data &lt;- getNOAA.bathy(-70, -50, 58, 68, resolution = 4, keep = T, antimeridian = FALSE) sample_map_1 &lt;- autoplot(map_data, geom = c(&quot;r&quot;, &quot;c&quot;)) + scale_fill_gradient2(low = &quot;dodgerblue4&quot;, mid = &quot;gainsboro&quot;, high = &quot;darkgreen&quot;) + geom_point(aes(x = sample_coords$Longitude, y = sample_coords$Latitude, colour = factor(sample_coords$Species)), data = sample_coords) + ggtitle(&quot;Sample map&quot;) + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + labs(fill = &quot;Depth&quot;, col = &quot;Species&quot;) sample_map_1 Figure 1.2: Detailed map of the sampling site, samples represented by dots are coloured according to sponge species. # with labels: sample_map_1+geom_label_repel(aes(x=sample_coords$Longitude, y=sample_coords$Latitude, label = sample_coords$unified_ID), box.padding = 0.35, point.padding = 0.5, # data=sample_coords) # 3D map library(marmap) library(lattice) #for wireframe map_data_hires &lt;- getNOAA.bathy(-70, -50, 58, 68, resolution = 1, keep = T, antimeridian = FALSE) wireframe(unclass(map_data_hires), shade = T, aspect = c(1/2, 0.1), screen = list(z = 0, x = -50), par.settings = list(axis.line = list(col = &quot;transparent&quot;)), par.box = c(col = rgb(0, 0, 0, 0.1))) knitr::include_graphics(&quot;data/map_screenshot.png&quot;) Figure 1.3: 3D representation of the threshold and slope in the Davis strait. 1.1.2 Water masses knitr::include_graphics(&quot;data/Argo_Mean_on_Grid_Size_0dec25_2002_3-2018_6_Lambert_Lat_Range_59-65_Vector_IY20191123.png&quot;) Figure 1.4: Currents in the North Atlantic between Greenland and Canada. Analysis and figure contributed by Igor Yashayaev. knitr::include_graphics(&quot;data/water_masses_2002-2015 T-S Profiles Selected in Topo-range 300-1900 m &amp; Colored by Depth - Labeled.png&quot;) Figure 1.5: Temperature and salinity (T-S) plot showing the different water masses in the Davis strait by depth: Shelf Water (ShW), Slope Water (SW), Irminger Current (IC), Labrador Sea Water (LSW), and Icelandic slope water (ISW). Analysis and figure contributed by Igor Yashayaev. 1.1.3 Sponges knitr::include_graphics(&quot;data/sponges.png&quot;) Figure 1.6: A G. barretti and B S. fortis from West Shetland Channel. Image Crown Copyright ©2006, all rights reserved. Image provided by K. Howell, Plymouth University, UK. C W. bursa, picture taken by scuba diving at 40 m in Svalbard by Peter Leopold. G. barretti and S. fortis are high microbial abundance (HMA) sponges often found living in proximity. W. bursa is likely a low microbial abundance sponges (see diversity metrics, Nicole Boury-Esnault per.comm.). While all sponges originate from the same geographic region, W. bursa typically does not occur in the same habitats as G. barretti and S. fortis (Murillo et al. 2018). It is important to note that S. fortis is whiteish in colour, and the yellow shown in the picture stems from Hexadella dedritifera frequently found overgrowing it. 1.2 Sample metadata analysis To investigate underlying data patterns, we assessed correlation of our predictor variables, i.e. the meta data of our samples (depth, latitude, longitude, sampling year, salinity and temperature). 1.2.1 Correlations, visual inspection and variance inflation factors (VIF) across all samples md &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) md &lt;- md[, c(&quot;Species&quot;, &quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;YEAR&quot;, &quot;MeanBotSalinity_PSU&quot;, &quot;MeanBottomTemp_Cdeg&quot;, &quot;unified_ID&quot;)] a &lt;- cor.test(md$Depth, md$Latitude, method = &quot;spearman&quot;) a &lt;- paste(&quot;Rho =&quot;, round(a$estimate, digits = 3), &quot; p-value=&quot;, round(a$p.value, digits = 3)) b &lt;- cor.test(md$Depth, md$Longitude, method = &quot;spearman&quot;) b &lt;- paste(&quot;Rho =&quot;, round(b$estimate, digits = 3), &quot; p-value=&quot;, round(b$p.value, digits = 3)) c &lt;- cor.test(md$Depth, md$YEAR, method = &quot;spearman&quot;) c &lt;- paste(&quot;Rho =&quot;, round(c$estimate, digits = 3), &quot; p-value=&quot;, round(c$p.value, digits = 3)) d &lt;- cor.test(md$Depth, md$MeanBotSalinity_PSU, method = &quot;spearman&quot;) d &lt;- paste(&quot;Rho =&quot;, round(d$estimate, digits = 3), &quot; p-value=&quot;, round(d$p.value, digits = 3)) e &lt;- cor.test(md$Depth, md$MeanBottomTemp_Cdeg, method = &quot;spearman&quot;) e &lt;- paste(&quot;Rho =&quot;, round(e$estimate, digits = 3), &quot; p-value=&quot;, round(e$p.value, digits = 3)) f &lt;- cor.test(md$MeanBotSalinity_PSU, md$MeanBottomTemp_Cdeg, method = &quot;spearman&quot;) f &lt;- paste(&quot;T-S plot: Rho =&quot;, round(f$estimate, digits = 3), &quot; p-value=&quot;, round(f$p.value, digits = 3)) k &lt;- ggplot(md, aes(x = Latitude, y = Depth)) + geom_point() + scale_y_continuous(trans = &quot;reverse&quot;) + xlab(&quot;Latitude&quot;) + ylab(&quot;Depth&quot;) + ggtitle(a) + theme(plot.title = element_text(size = 10)) l &lt;- ggplot(md, aes(x = Longitude, y = Depth)) + geom_point() + scale_y_continuous(trans = &quot;reverse&quot;) + xlab(&quot;Longitude&quot;) + ylab(&quot;Depth&quot;) + ggtitle(b) + theme(plot.title = element_text(size = 10)) m &lt;- ggplot(md, aes(x = YEAR, y = Depth)) + geom_point() + scale_y_continuous(trans = &quot;reverse&quot;) + xlab(&quot;Year&quot;) + ylab(&quot;Depth&quot;) + ggtitle(c) + theme(plot.title = element_text(size = 10)) n &lt;- ggplot(md, aes(x = MeanBotSalinity_PSU, y = Depth)) + geom_point() + scale_y_continuous(trans = &quot;reverse&quot;) + xlab(&quot;Salinity&quot;) + ylab(&quot;Depth&quot;) + ggtitle(d) + theme(plot.title = element_text(size = 10)) o &lt;- ggplot(md, aes(x = MeanBottomTemp_Cdeg, y = Depth)) + geom_point() + scale_y_continuous(trans = &quot;reverse&quot;) + xlab(&quot;Temperature&quot;) + ylab(&quot;Depth&quot;) + ggtitle(e) + theme(plot.title = element_text(size = 10)) p &lt;- ggplot(md, aes(x = MeanBotSalinity_PSU, y = MeanBottomTemp_Cdeg, colour = md$Depth)) + geom_point(size = 2) + scale_colour_gradient(guide = guide_colourbar(), high = &quot;#f4c430&quot;, low = &quot;#3b3bc4&quot;, trans = &quot;reverse&quot;) + xlab(&quot;Salinity in psu&quot;) + ylab(&quot;Temperature in °C&quot;) + labs(colour = &quot;Depth&quot;) + ggtitle(f) + theme(plot.title = element_text(size = 10), legend.key.size = unit(0.3, &quot;cm&quot;)) + scale_x_continuous(limits = c(34.4, 35)) # The limits of the x-axis exclude two samples from water with comparably low salinity library(gridExtra) grid.arrange(k, l, m, n, o, p, nrow = 3, top = &quot;Spearman correlation of sample associated meta data&quot;) Figure 1.7: Correlation of depth with other environmental parameters/predictor variables across all samples. rm(a, b, c, d, e, f, k, l, m, n, o, p, df) In the bottom right plot showing temperature versus salinity, two samples from lower salinity were excluded for visual clarity. From the plots above and their correlation tests, we find that longitude, salinity and temperature are significantly correlated with depth, and that salinity and temperature also correlate with each other. This is further corroborated in the variance inflation factors. A VIF &gt; 10 indicates redundancy/collinearity. This means that we have to be cautious and avoid including redundant constraints in ecological models. library(usdm) md$Species &lt;- NULL md$unified_ID &lt;- NULL vif_all &lt;- vif(md) library(kableExtra) options(kableExtra.html.bsTable = T) kable(vif_all, longtable = T, booktabs = T, caption = &quot;VIF for meta data from all samples&quot;, row.names = F) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), full_width = F, latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;)) Table 1.1: VIF for meta data from all samples Variables VIF Depth 3.267983 Latitude 6.125058 Longitude 7.189093 YEAR 1.122117 MeanBotSalinity_PSU 16.131495 MeanBottomTemp_Cdeg 12.301386 rm(md) As we will be focussing on Geodia barretti, we repeat the same inspection procedure for the sample subset. 1.2.2 Correlations, visual inspection and variance inflation factors (VIF) across G. barretti samples md &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) md &lt;- md[, c(&quot;Species&quot;, &quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;YEAR&quot;, &quot;MeanBotSalinity_PSU&quot;, &quot;MeanBottomTemp_Cdeg&quot;, &quot;unified_ID&quot;)] md &lt;- md[md$Species == &quot;Geodia barretti&quot;, ] a &lt;- cor.test(md$Depth, md$Latitude, method = &quot;spearman&quot;) a &lt;- paste(&quot;Rho =&quot;, round(a$estimate, digits = 3), &quot; p-value=&quot;, round(a$p.value, digits = 3)) b &lt;- cor.test(md$Depth, md$Longitude, method = &quot;spearman&quot;) b &lt;- paste(&quot;Rho =&quot;, round(b$estimate, digits = 3), &quot; p-value=&quot;, round(b$p.value, digits = 3)) c &lt;- cor.test(md$Depth, md$YEAR, method = &quot;spearman&quot;) c &lt;- paste(&quot;Rho =&quot;, round(c$estimate, digits = 3), &quot; p-value=&quot;, round(c$p.value, digits = 3)) d &lt;- cor.test(md$Depth, md$MeanBotSalinity_PSU, method = &quot;spearman&quot;) d &lt;- paste(&quot;Rho =&quot;, round(d$estimate, digits = 3), &quot; p-value=&quot;, round(d$p.value, digits = 3)) e &lt;- cor.test(md$Depth, md$MeanBottomTemp_Cdeg, method = &quot;spearman&quot;) e &lt;- paste(&quot;Rho =&quot;, round(e$estimate, digits = 3), &quot; p-value=&quot;, round(e$p.value, digits = 3)) f &lt;- cor.test(md$MeanBotSalinity_PSU, md$MeanBottomTemp_Cdeg, method = &quot;spearman&quot;) f &lt;- paste(&quot;Rho =&quot;, round(f$estimate, digits = 3), &quot; p-value=&quot;, round(f$p.value, digits = 3)) k &lt;- ggplot(md, aes(x = Latitude, y = Depth)) + geom_point() + scale_y_continuous(trans = &quot;reverse&quot;) + xlab(&quot;Latitude&quot;) + ylab(&quot;Depth&quot;) + ggtitle(a) + theme(plot.title = element_text(size = 10)) l &lt;- ggplot(md, aes(x = Longitude, y = Depth)) + geom_point() + scale_y_continuous(trans = &quot;reverse&quot;) + xlab(&quot;Longitude&quot;) + ylab(&quot;Depth&quot;) + ggtitle(b) + theme(plot.title = element_text(size = 10)) m &lt;- ggplot(md, aes(x = YEAR, y = Depth)) + geom_point() + scale_y_continuous(trans = &quot;reverse&quot;) + xlab(&quot;Year&quot;) + ylab(&quot;Depth&quot;) + ggtitle(c) + theme(plot.title = element_text(size = 10)) n &lt;- ggplot(md, aes(x = MeanBotSalinity_PSU, y = Depth)) + geom_point() + scale_y_continuous(trans = &quot;reverse&quot;) + xlab(&quot;Salinity&quot;) + ylab(&quot;Depth&quot;) + ggtitle(d) + theme(plot.title = element_text(size = 10)) o &lt;- ggplot(md, aes(x = MeanBottomTemp_Cdeg, y = Depth)) + geom_point() + scale_y_continuous(trans = &quot;reverse&quot;) + xlab(&quot;Temperature&quot;) + ylab(&quot;Depth&quot;) + ggtitle(e) + theme(plot.title = element_text(size = 10)) p &lt;- ggplot(md, aes(x = MeanBotSalinity_PSU, y = MeanBottomTemp_Cdeg, colour = md$Depth)) + geom_point(size = 2) + scale_colour_gradient(guide = guide_colourbar(), high = &quot;#f4c430&quot;, low = &quot;#3b3bc4&quot;, trans = &quot;reverse&quot;) + xlab(&quot;Salinity in psu&quot;) + ylab(&quot;Temperature in °C&quot;) + labs(colour = &quot;Depth&quot;) + ggtitle(f) + theme(plot.title = element_text(size = 10), legend.key.size = unit(0.3, &quot;cm&quot;)) library(gridExtra) grid.arrange(k, l, m, n, o, p, nrow = 3, top = &quot;Spearman correlation of Geodia barretti associated meta data&quot;) Figure 1.8: Correlation of depth with other environmental parameters/predictor variables across samples of G. barretti. rm(a, b, c, d, e, f, k, l, m, n, o, p, df) library(usdm) md$Species &lt;- NULL md$unified_ID &lt;- NULL vif1 &lt;- vif(md) vif1$VIF &lt;- round(vif1$VIF, digits = 2) md$MeanBotSalinity_PSU &lt;- NULL md$MeanBottomTemp_Cdeg &lt;- NULL vif2 &lt;- vif(md) vif2$VIF &lt;- round(vif2$VIF, digits = 2) library(dplyr) vifs &lt;- full_join(vif1, vif2, by = c(Variables = &quot;Variables&quot;)) colnames(vifs) &lt;- c(&quot;Variables&quot;, &quot;All predictors&quot;, &quot;Subset&quot;) vifs[c(5, 6), 3] &lt;- &quot;&quot; library(kableExtra) options(kableExtra.html.bsTable = T) cap &lt;- paste(&quot;VIF for extensive and simplified models for&quot;, text_spec(&quot;G. barretti&quot;, italic = T)) kable(vifs, longtable = T, booktabs = T, caption = cap, row.names = F) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), full_width = F, latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;)) Table 1.2: VIF for extensive and simplified models for G. barretti Variables All predictors Subset Depth 22.15 5.91 Latitude 11.63 10.31 Longitude 16.59 13.38 YEAR 1.73 1.46 MeanBotSalinity_PSU 61.13 MeanBottomTemp_Cdeg 47.52 rm(md) With this subset, the collinearity becomes even more pronounced. Salinity, temperature and depth are strongly correlated and the VIFs indicated that they should not be included in a model together. This correlation is partly due to physical properties of water. Density of water increases with increasing salinity and decreasing temperature. To a lesser extent, longitude and lattitude are collinear in our sampling. However, removing the predictor variables salinity and temperature mititgated/lowered the effect of collinearity. References "],
["metabolomics.html", "2 Metabolomics 2.1 Experimental methods and detailed description 2.2 Data processing 2.3 Chromatorgrams 2.4 Multivariate analyses 2.5 VIPS 2.6 Metabolites and correlations", " 2 Metabolomics 2.1 Experimental methods and detailed description Additional sampling notes During the sampling on the Pâmiut cruises, the sponges remained on deck or in the laboratory for approximately 30–45 min before they were frozen to -20°C. The outside temperature usually oscillated around 4–5°C, and the sorting area was approximately 10°C. The sample consisted of both choanosome and coretx. Laboratory methods and data acquisition Metabolites were separated in connection to downstream mass spectrometry (HRMS) analysis using two different chromatographic columns (UPLC): a hydrophilic interaction liquid chromatography (HILIC) column that retains polar compounds, and a reversed-phase (RP) column that favours retention of non-polar compounds. All samples were processed in randomized order using glass instruments during the extraction to avoid chemical contamination. Mass spectrometry analysis Dried extracts in glass vials were dissolved in 200 µL solvent (HILIC: 50 µL H2O and 175 µL acetonitrile AcN; RP: 140 µL H2O and 10 µL AcN). Upon addition of the organic solvent for HILIC chromatography, all samples separated into two immiscible layers. The vials were centrifuged for 3 min at 2000 x g to yield an even separation. Only the top layer (approximately 150 µL) was transferred to a Chromacol 03-FISV MS-vial (Thermo Scientific, Waltham, Massachusetts, USA) for MS analyses. For RP chromatography, no layers were observed, and the entire volume of the dissolved sample was used. A five µL aliquot from each individual MS-vial for HILIC and RP, respectively, was combined to produce a quality control (QC) sample. High resolution MS analysis system and settings The extracts were analyzed back-to-back in positive and negative ionization mode on an Acquity I-Class Ultra Performance Liquid Chromatography UPLC coupled to a G2S Synapt Q-TOF with an electrospray ionization (ESI) ion source (all Waters Corp., Milford, MA, USA). Chromatographic separation in HILIC mode was performed on an Acquity UPLC BEH Amide column (1.7 µm, 2.1 mm inner diameter × 50 mm, Waters Corp.). Mobile phase A consisted of 95:5 acetonitrile/MQ water with 5 mM ammonium formate and 0.1 % formic acid (FA), and mobile phase B consisted of 40:60 acetonitrile/MQ water with 5 mM ammonium formate and 0.1 % FA. The gradient elution profile was as follows: mobile phase A was decreased non-linearly (slope factor 8, MassLynx) from 100 % A to 100 % B over 14 min, 100 % B was held for 2 min and then decreased back to 100 % A over 1 min. The column was re-equilibrated at 100 % A for 6 min for a total runtime of 23 min. Chromatographic separation in RP was performed on an Acquity UPLC BEH C18 column (1.7 µ m, 2.1 mm inner diameter × 50 mm, Waters). Mobile phase A consisted of MQ water with 0.1 % FA, and mobile phase B was AcN with 0.1 % FA. The gradient elution profile started at 95 % A, was decreased linearly over 14 min to 5 % A, and 5 % A was held for 2 min before the column was re-equilibrated at 95 % A for 4 min. The flow rate was set to 0.4 mL/min, the column temperature was set to 40°C, the samples were kept at 8°C and the injection volume was 5 µL in all experiments. Data acquisition Data acquisition was performed using MSE mode, and lock mass correction was applied using a solution of leucine enkephalin in both positive and negative mode. Ionization parameters were set as follows in positive/negative mode; the capillary voltage was 1kV/1.5 kV, the cone voltage was 30 V/25 V, the source offset was 50/60 and the source temperature was set to 120°C. Nitrogen was used as desolvation and cone gas with gas flows of 800 l/h and 50 l/h, respectively, and desolvation temperature was set to 500°C/450°C. For MSE acquisition a collision energy ramp from 20–45 eV was used with argon as collision gas. The instrument was calibrated in the m/z range 50–1500 using sodium formate prior to each analysis. All study samples were analysed in both RP and HILIC, in positive and negative ionization mode, resulting in four metabolite datasets per sponge specimen. The column and sample cone was cleaned in between each analysis mode. Prior to each analysis ten QC injections were made to condition the column, and to ensure stable retention times and signal intensities. The study samples were analysed in randomized order with QC injections interspaced every 6th injection. MS data processing Raw files were converted to netCDF files by Databridge (part of MassLynx, Waters Corporation, Milford, Massachusetts, USA). The netCDF files with the chromatographic spectra were sorted into folders according to species and processed with XCMS in R. Peak picking was performed using the centWave function with parameters ppm=8, peakwidth set to c(5,45) and the noise parameter set to 2000. Retention time alignment was performed with the obiwarp function and the response factor set to 10, grouping was performed with the “group” function and the “fillPeaks” function was used to impute a signal in cases where no matching pseudospectra were detected. The data set was curated to remove features eluting in the void (retention time less than 45 s). A raw data set as well as two normalized data sets (Log10-transformed and median fold change normalized) were produced and filtered to only retain features with a coefficient of variation &lt; 30% in the QC samples. After subsequent evaluation, raw data sets were used in subsequent statistics and modelling. An overview of the number of features and PC groups annotated by CAMERA, as well as their exclusion is given in a table at the end of this document. 2.2 Data processing 2.2.1 Peak picking with XCMS and annotation with CAMERA We processed samples from all three sponnge species in random order with interspersed injection of a combined QC sample to monitor stability of the UPLC-HRMS run. The acquired signals/spectra were converted to netCDF format using the Program DataBridge, and thereafter sorted into four folders, three for the sponge species (Gb, Sf, Wb) and one for the QC samples (QC). Peak picking and combination of pseudospectra is performed with the R package xcms, the subsequent annotation of adducts and isotopes with the R package CAMERA. 2.2.2 HILIC (Hydrophilic interaction chromatography) column with positive ESI (electron spray ionisation) # HILIC POS ALWAYS CHECK DATE, CHROMATOGRAPHIC CLOUMN (HILIC, RP) AND ESI-MODE (naming, CAMERA: pos, neg) # setwd() # work in directory containing the sorted CDF files. getwd() library(xcms) xset &lt;- xcmsSet(method = &quot;centWave&quot;, ppm = 8, peakwidth = c(5, 45), noise = 2000) save(xset, file = &quot;HILIC_pos_xset_20190417.Rda&quot;) # load(file=&#39;HILIC_pos_xset_20190417.Rda&#39;) #When resuming after a break xset &lt;- group(xset) xset2 &lt;- retcor(xset, method = &quot;obiwarp&quot;, response = 10, plottype = &quot;deviation&quot;) xset2 &lt;- group(xset2) xset3 &lt;- fillPeaks(xset2) save(xset3, file = &quot;HILIC_pos_xset3_20190417.Rda&quot;) reporttab &lt;- diffreport(xset3, &quot;Gb&quot;, &quot;Sf&quot;, &quot;Gb_Sf_HILIC_pos_20190417&quot;, 10) library(CAMERA) xsa &lt;- xsAnnotate(xset3) xsaF &lt;- groupFWHM(xsa, perfwhm = 0.3) xsaC &lt;- groupCorr(xsaF, cor_eic_th = 0.7) xsaFI &lt;- findIsotopes(xsaC) rules &lt;- read.csv(&quot;data/rules_jan_pos.csv&quot;, header = T, sep = &quot;,&quot;) xsaFA &lt;- findAdducts(xsaFI, polarity = &quot;positive&quot;, rules = rules) # write.csv(getPeaklist(xsaFA), file=&#39;HILIC_pos_20190417.csv&#39;) The other data experiments, HILIC with negative ESI and RP with positive and negative ESI are processed accordingly. 2.2.3 HILIC Chromatography with negative ESI # HILIC NEG 20190421 # setwd() # work in directory containing the sorted CDF files. getwd() library(xcms) xset &lt;- xcmsSet(method = &quot;centWave&quot;, ppm = 8, peakwidth = c(5, 45), noise = 2000) save(xset, file = &quot;HILIC_neg_xset_20190421.Rda&quot;) # load(file=&#39;HILIC_neg_xset_20190421.Rda&#39;) #When resuming after a break xset &lt;- group(xset) xset2 &lt;- retcor(xset, method = &quot;obiwarp&quot;, response = 10, plottype = &quot;deviation&quot;) xset2 &lt;- group(xset2) xset3 &lt;- fillPeaks(xset2) save(xset3, file = &quot;HILIC_neg_xset3_20190421.Rda&quot;) reporttab &lt;- diffreport(xset3, &quot;Gb&quot;, &quot;Sf&quot;, &quot;Gb_Sf_HILIC_neg_20190421&quot;, 10) library(CAMERA) xsa &lt;- xsAnnotate(xset3) xsaF &lt;- groupFWHM(xsa, perfwhm = 0.3) xsaC &lt;- groupCorr(xsaF, cor_eic_th = 0.7) xsaFI &lt;- findIsotopes(xsaC) rules &lt;- read.csv(&quot;data/rules_jan_neg.csv&quot;, header = T, sep = &quot;,&quot;) xsaFA &lt;- findAdducts(xsaFI, polarity = &quot;negative&quot;, rules = rules) # write.csv(getPeaklist(xsaFA), file=&#39;HILIC_neg_20190421.csv&#39;) 2.2.4 RP (Reversed phase) Chromatography with positive ESI # RP POS 20190421 # setwd() # work in directory containing the sorted CDF files. getwd() library(xcms) xset &lt;- xcmsSet(method = &quot;centWave&quot;, ppm = 8, peakwidth = c(5, 45), noise = 2000) save(xset, file = &quot;RP_pos_xset_20190421.Rda&quot;) # load(file=&#39;RP_pos_xset_20190421.Rda&#39;) #When resuming after a break xset &lt;- group(xset) xset2 &lt;- retcor(xset, method = &quot;obiwarp&quot;, response = 10, plottype = &quot;deviation&quot;) xset2 &lt;- group(xset2) xset3 &lt;- fillPeaks(xset2) save(xset3, file = &quot;RP_pos_xset3_20190421.Rda&quot;) reporttab &lt;- diffreport(xset3, &quot;Gb&quot;, &quot;Sf&quot;, &quot;Gb_Sf_RP_pos_20190421&quot;, 10) library(CAMERA) xsa &lt;- xsAnnotate(xset3) xsaF &lt;- groupFWHM(xsa, perfwhm = 0.3) xsaC &lt;- groupCorr(xsaF, cor_eic_th = 0.7) xsaFI &lt;- findIsotopes(xsaC) rules &lt;- read.csv(&quot;data/rules_jan_pos.csv&quot;, header = T, sep = &quot;,&quot;) xsaFA &lt;- findAdducts(xsaFI, polarity = &quot;positive&quot;, rules = rules) # write.csv(getPeaklist(xsaFA), file=&#39;RP_pos_20190421.csv&#39;) 2.2.5 RP Chromatography with negative ESI # RP NEG 20190422 # setwd() # work in directory containing the sorted CDF files. getwd() library(xcms) xset &lt;- xcmsSet(method = &quot;centWave&quot;, ppm = 8, peakwidth = c(5, 45), noise = 2000) save(xset, file = &quot;RP_neg_xset_20190422.Rda&quot;) # load(file=&#39;RP_neg_xset_20190422.Rda&#39;) #When resuming after a break xset &lt;- group(xset) xset2 &lt;- retcor(xset, method = &quot;obiwarp&quot;, response = 10, plottype = &quot;deviation&quot;) xset2 &lt;- group(xset2) xset3 &lt;- fillPeaks(xset2) save(xset3, file = &quot;RP_neg_xset3_20190422.Rda&quot;) reporttab &lt;- diffreport(xset3, &quot;Gb&quot;, &quot;Sf&quot;, &quot;Gb_Sf_RP_neg_20190422&quot;, 10) library(CAMERA) xsa &lt;- xsAnnotate(xset3) xsaF &lt;- groupFWHM(xsa, perfwhm = 0.3) xsaC &lt;- groupCorr(xsaF, cor_eic_th = 0.7) xsaFI &lt;- findIsotopes(xsaC) rules &lt;- read.csv(&quot;data/rules_jan_neg.csv&quot;, header = T, sep = &quot;,&quot;) xsaFA &lt;- findAdducts(xsaFI, polarity = &quot;negative&quot;, rules = rules) # write.csv(getPeaklist(xsaFA), file=&#39;RP_neg_20190422.csv&#39;) 2.2.6 Filtering of the raw data set Features with following properties were removed: Eluting in the void (RT &lt; 45 s) CV&gt;30 % in QC samples Adducts as annotated by CAMERA As the presence of the same parent feature in the the form of isotopes and adducts distorts downstream global metabolome analyses, we generated two versions of curated or selective feature tables. In both, void and unstably measured features are removed (“cleaned”). One retains only the feature with the largest signal per pc group (“cleaned_pcgroup”), the other is more stringent and only contains explicitly annotated parent ions (“cleaned_MH”), i.e. [M+H]+ for positive ESI and [M-H]- for negative ESI acquired data. library(dplyr) # ================================ HILIC pos =================================== raw_peaks &lt;- read.csv(&quot;data/HILIC_pos_20190417.csv&quot;) hp_dim1 &lt;- dim(raw_peaks)[1] hp_pcg1 &lt;- length(unique(raw_peaks$pcgroup)) # Removing features eluting in the void raw_peaks &lt;- raw_peaks[raw_peaks$rt &gt; 45, ] hp_dim2 &lt;- dim(raw_peaks)[1] hp_pcg2 &lt;- length(unique(raw_peaks$pcgroup)) # Removing features with a CV &lt; 30% f &lt;- which(colnames(raw_peaks) == &quot;IE_20170918_02001&quot;) # first QC HILIC pos l &lt;- which(colnames(raw_peaks) == &quot;IE_20170918_07601&quot;) # last QC HILIC pos raw_peaks[&quot;SD&quot;] &lt;- apply(raw_peaks[, f:l], 1, sd) raw_peaks[&quot;MEAN&quot;] &lt;- apply(raw_peaks[, f:l], 1, mean) raw_peaks[&quot;CV&quot;] &lt;- raw_peaks$SD/raw_peaks$MEAN raw_peaks &lt;- raw_peaks[raw_peaks$CV &lt; 0.3, ] hp_dim3 &lt;- dim(raw_peaks)[1] hp_pcg3 &lt;- length(unique(raw_peaks$pcgroup)) # write.csv(raw_peaks, &#39;HILIC_pos_20190417_cleaned.csv&#39;) # Keep only the feature with the greatest sum of signal per pc group l &lt;- dim(raw_peaks)[2] - 6 raw_peaks[&quot;fss&quot;] &lt;- apply(raw_peaks[, 13:l], 1, sum) #feature signal sum raw_peaks &lt;- raw_peaks[order(raw_peaks$pcgroup, -raw_peaks$fss), ] # for every pc group, the first line has the strongest signal. The function distinct() [dplyr package] can be used to keep only unique/distinct rows from a data frame. If there # are duplicate rows, _only the first row_ is preserved. selected_peaks &lt;- distinct(raw_peaks, raw_peaks$pcgroup, .keep_all = TRUE) hp_dim4 &lt;- dim(selected_peaks)[1] hp_pcg4 &lt;- length(unique(selected_peaks$pcgroup)) # write.csv(selected_peaks, &#39;HILIC_pos_20190417_cleaned_pcgroup.csv&#39;) rm(selected_peaks) # Keep only features with explicit annotations of [M+H]+ library(stringr) raw_peaks[&quot;[M+H]+&quot;] &lt;- NA pattern &lt;- &quot;[M+H]+&quot; raw_peaks[&quot;[M+H]+&quot;] &lt;- str_detect(raw_peaks$adduct, paste0(&quot;^\\\\Q&quot;, pattern, &quot;\\\\E&quot;)) selected_peaks &lt;- raw_peaks[raw_peaks$`[M+H]+` == &quot;TRUE&quot;, ] hp_dim5 &lt;- dim(selected_peaks)[1] hp_pcg5 &lt;- length(unique(selected_peaks$pcgroup)) # write.csv(selected_peaks, &#39;HILIC_pos_20190417_cleaned_MH.csv&#39;) hilic_pos &lt;- raw_peaks rm(f, l, raw_peaks, selected_peaks) # ================================ HILIC neg =================================== raw_peaks &lt;- read.csv(&quot;data/HILIC_neg_20190421.csv&quot;) hn_dim1 &lt;- dim(raw_peaks)[1] hn_pcg1 &lt;- length(unique(raw_peaks$pcgroup)) # Removing features eluting in the void raw_peaks &lt;- raw_peaks[raw_peaks$rt &gt; 45, ] hn_dim2 &lt;- dim(raw_peaks)[1] hn_pcg2 &lt;- length(unique(raw_peaks$pcgroup)) # Removing features with a CV &lt; 30% f &lt;- which(colnames(raw_peaks) == &quot;IE_20170919_01301&quot;) # first QC HILIC neg l &lt;- which(colnames(raw_peaks) == &quot;IE_20170919_06901&quot;) # last QC HILIC neg raw_peaks[&quot;SD&quot;] &lt;- apply(raw_peaks[, f:l], 1, sd) raw_peaks[&quot;MEAN&quot;] &lt;- apply(raw_peaks[, f:l], 1, mean) raw_peaks[&quot;CV&quot;] &lt;- raw_peaks$SD/raw_peaks$MEAN raw_peaks &lt;- raw_peaks[raw_peaks$CV &lt; 0.3, ] hn_dim3 &lt;- dim(raw_peaks)[1] hn_pcg3 &lt;- length(unique(raw_peaks$pcgroup)) # write.csv(raw_peaks, &#39;HILIC_neg_20190421_cleaned.csv&#39;) # Keep only the feature with the greatest sum of signal per pc group l &lt;- dim(raw_peaks)[2] - 6 raw_peaks[&quot;fss&quot;] &lt;- apply(raw_peaks[, 13:l], 1, sum) #feature signal sum raw_peaks &lt;- raw_peaks[order(raw_peaks$pcgroup, -raw_peaks$fss), ] selected_peaks &lt;- distinct(raw_peaks, raw_peaks$pcgroup, .keep_all = TRUE) hn_dim4 &lt;- dim(selected_peaks)[1] hn_pcg4 &lt;- length(unique(selected_peaks$pcgroup)) # write.csv(selected_peaks, &#39;HILIC_neg_20190421_cleaned_pcgroup.csv&#39;) rm(selected_peaks) # Keep only features with explicit annotations of [M-H]- library(stringr) raw_peaks[&quot;[M-H]-&quot;] &lt;- NA pattern &lt;- &quot;[M-H]-&quot; raw_peaks[&quot;[M-H]-&quot;] &lt;- str_detect(raw_peaks$adduct, paste0(&quot;^\\\\Q&quot;, pattern, &quot;\\\\E&quot;)) selected_peaks &lt;- raw_peaks[raw_peaks$`[M-H]-` == &quot;TRUE&quot;, ] hn_dim5 &lt;- dim(selected_peaks)[1] hn_pcg5 &lt;- length(unique(selected_peaks$pcgroup)) # write.csv(selected_peaks, &#39;HILIC_neg_20190421_cleaned_MH.csv&#39;) hilic_neg &lt;- raw_peaks rm(f, l, raw_peaks) # ================================== RP pos ==================================== raw_peaks &lt;- read.csv(&quot;data/RP_pos_20190421.csv&quot;) rp_dim1 &lt;- dim(raw_peaks)[1] rp_pcg1 &lt;- length(unique(raw_peaks$pcgroup)) # Removing features eluting in the void raw_peaks &lt;- raw_peaks[raw_peaks$rt &gt; 45, ] rp_dim2 &lt;- dim(raw_peaks)[1] rp_pcg2 &lt;- length(unique(raw_peaks$pcgroup)) # Removing features with a CV &lt; 30% f &lt;- which(colnames(raw_peaks) == &quot;IE_20171002_01401&quot;) # first QC RP neg l &lt;- which(colnames(raw_peaks) == &quot;IE_20171002_07201&quot;) # last QC RP neg raw_peaks[&quot;SD&quot;] &lt;- apply(raw_peaks[, f:l], 1, sd) raw_peaks[&quot;MEAN&quot;] &lt;- apply(raw_peaks[, f:l], 1, mean) raw_peaks[&quot;CV&quot;] &lt;- raw_peaks$SD/raw_peaks$MEAN raw_peaks &lt;- raw_peaks[raw_peaks$CV &lt; 0.3, ] rp_dim3 &lt;- dim(raw_peaks)[1] rp_pcg3 &lt;- length(unique(raw_peaks$pcgroup)) # write.csv(raw_peaks, &#39;RP_pos_20190421_cleaned.csv&#39;) # Keep only the feature with the greatest sum of signal per pc group l &lt;- dim(raw_peaks)[2] - 6 raw_peaks[&quot;fss&quot;] &lt;- apply(raw_peaks[, 13:l], 1, sum) #feature signal sum raw_peaks &lt;- raw_peaks[order(raw_peaks$pcgroup, -raw_peaks$fss), ] selected_peaks &lt;- distinct(raw_peaks, raw_peaks$pcgroup, .keep_all = TRUE) rp_dim4 &lt;- dim(selected_peaks)[1] rp_pcg4 &lt;- length(unique(selected_peaks$pcgroup)) # write.csv(selected_peaks, &#39;RP_pos_20190421_cleaned_pcgroup.csv&#39;) rm(selected_peaks) # Keep only features with explicit annotations of [M+H]+ library(stringr) raw_peaks[&quot;[M+H]+&quot;] &lt;- NA pattern &lt;- &quot;[M+H]+&quot; raw_peaks[&quot;[M+H]+&quot;] &lt;- str_detect(raw_peaks$adduct, paste0(&quot;^\\\\Q&quot;, pattern, &quot;\\\\E&quot;)) selected_peaks &lt;- raw_peaks[raw_peaks$`[M+H]+` == &quot;TRUE&quot;, ] rp_dim5 &lt;- dim(selected_peaks)[1] rp_pcg5 &lt;- length(unique(selected_peaks$pcgroup)) # write.csv(selected_peaks, &#39;RP_pos_20190421_cleaned_MH.csv&#39;) rp_pos &lt;- raw_peaks rm(f, l, raw_peaks, selected_peaks) # ================================== RP neg ==================================== raw_peaks &lt;- read.csv(&quot;data/RP_neg_20190422.csv&quot;) rn_dim1 &lt;- dim(raw_peaks)[1] rn_pcg1 &lt;- length(unique(raw_peaks$pcgroup)) # Removing features eluting in the void raw_peaks &lt;- raw_peaks[raw_peaks$rt &gt; 45, ] rn_dim2 &lt;- dim(raw_peaks)[1] rn_pcg2 &lt;- length(unique(raw_peaks$pcgroup)) # Removing features with a CV &lt; 30% f &lt;- which(colnames(raw_peaks) == &quot;IE_20171003_01301&quot;) # first QC RP neg l &lt;- which(colnames(raw_peaks) == &quot;IE_20171003_05701&quot;) # 3rd last QC RP neg; QC empty after this injection raw_peaks[&quot;SD&quot;] &lt;- apply(raw_peaks[, f:l], 1, sd) raw_peaks[&quot;MEAN&quot;] &lt;- apply(raw_peaks[, f:l], 1, mean) raw_peaks[&quot;CV&quot;] &lt;- raw_peaks$SD/raw_peaks$MEAN raw_peaks &lt;- raw_peaks[raw_peaks$CV &lt; 0.3, ] rn_dim3 &lt;- dim(raw_peaks)[1] rn_pcg3 &lt;- length(unique(raw_peaks$pcgroup)) # write.csv(raw_peaks, &#39;RP_neg_20190422_cleaned.csv&#39;) # Keep only the feature with the greatest sum of signal per pc group l &lt;- dim(raw_peaks)[2] - 6 raw_peaks[&quot;fss&quot;] &lt;- apply(raw_peaks[, 13:l], 1, sum) #feature signal sum raw_peaks &lt;- raw_peaks[order(raw_peaks$pcgroup, -raw_peaks$fss), ] selected_peaks &lt;- distinct(raw_peaks, raw_peaks$pcgroup, .keep_all = TRUE) rn_dim4 &lt;- dim(selected_peaks)[1] rn_pcg4 &lt;- length(unique(selected_peaks$pcgroup)) # write.csv(selected_peaks, &#39;RP_neg_20190422_cleaned_pcgroup.csv&#39;) rm(selected_peaks) # Keep only features with explicit annotations of [M-H]- library(stringr) raw_peaks[&quot;[M-H]-&quot;] &lt;- NA pattern &lt;- &quot;[M-H]-&quot; raw_peaks[&quot;[M-H]-&quot;] &lt;- str_detect(raw_peaks$adduct, paste0(&quot;^\\\\Q&quot;, pattern, &quot;\\\\E&quot;)) selected_peaks &lt;- raw_peaks[raw_peaks$`[M-H]-` == &quot;TRUE&quot;, ] rn_dim5 &lt;- dim(selected_peaks)[1] rn_pcg5 &lt;- length(unique(selected_peaks$pcgroup)) # write.csv(selected_peaks, &#39;RP_neg_20190422_cleaned_MH.csv&#39;) hilic_neg &lt;- raw_peaks rm(f, l, raw_peaks, selected_peaks) 2.2.7 Data quality control To monitor the stability of the signal during the UPLC-HRMS run, we plot the total signal per sample by injection order. hp &lt;- read.csv(&quot;data/HILIC_pos_20190417.csv&quot;, header = T, sep = &quot;,&quot;) hn &lt;- read.csv(&quot;data/HILIC_neg_20190421.csv&quot;, header = T, sep = &quot;,&quot;) rp &lt;- read.csv(&quot;data/RP_pos_20190421.csv&quot;, header = T, sep = &quot;,&quot;) rn &lt;- read.csv(&quot;data/RP_neg_20190422.csv&quot;, header = T, sep = &quot;,&quot;) # subsetting, keep only sample data hp &lt;- hp[, 13:(dim(hp)[2] - 3)] #14 hn &lt;- hn[, 13:(dim(hn)[2] - 3)] rp &lt;- rp[, 13:(dim(rp)[2] - 3)] rn &lt;- rn[, 13:(dim(rn)[2] - 3)] # sum signal of columns, i.e. per sample hp[nrow(hp) + 1, ] &lt;- apply(hp, 2, sum) hn[nrow(hn) + 1, ] &lt;- apply(hn, 2, sum) rp[nrow(rp) + 1, ] &lt;- apply(rp, 2, sum) rn[nrow(rn) + 1, ] &lt;- apply(rn, 2, sum) # data frame gymnastics hp_df &lt;- data.frame(t((hp[dim(hp)[1], ]))) hn_df &lt;- data.frame(t((hn[dim(hn)[1], ]))) rp_df &lt;- data.frame(t((rp[dim(rp)[1], ]))) rn_df &lt;- data.frame(t((rn[dim(rn)[1], ]))) colnames(hp_df) &lt;- c(&quot;colsum_hp&quot;) hp_df[&quot;id&quot;] &lt;- rownames(hp_df) colnames(hn_df) &lt;- c(&quot;colsum_hn&quot;) hn_df[&quot;id&quot;] &lt;- rownames(hn_df) colnames(rp_df) &lt;- c(&quot;colsum_rp&quot;) rp_df[&quot;id&quot;] &lt;- rownames(rp_df) colnames(rn_df) &lt;- c(&quot;colsum_rn&quot;) rn_df[&quot;id&quot;] &lt;- rownames(rn_df) library(stringr) hp_df[&quot;hp_io&quot;] &lt;- as.integer(str_sub(hp_df$id, -4, -3)) hn_df[&quot;hn_io&quot;] &lt;- as.integer(str_sub(hn_df$id, -4, -3)) rp_df[&quot;rp_io&quot;] &lt;- as.integer(str_sub(rp_df$id, -4, -3)) rn_df[&quot;rn_io&quot;] &lt;- as.integer(str_sub(rn_df$id, -4, -3)) hp_df &lt;- hp_df[order(hp_df$hp_io), ] hp_df[&quot;n&quot;] &lt;- seq(nrow(hp_df)) hp_df[&quot;experiment&quot;] &lt;- &quot;HILIC pos.&quot; hn_df &lt;- hn_df[order(hn_df$hn_io), ] hn_df[&quot;n&quot;] &lt;- seq(nrow(hn_df)) hn_df[&quot;experiment&quot;] &lt;- &quot;HILIC neg.&quot; rp_df &lt;- rp_df[order(rp_df$rp_io), ] rp_df[&quot;n&quot;] &lt;- seq(nrow(rp_df)) rp_df[&quot;experiment&quot;] &lt;- &quot;RP pos.&quot; rn_df &lt;- rn_df[order(rn_df$rn_io), ] rn_df[&quot;n&quot;] &lt;- seq(nrow(rn_df)) rn_df[&quot;experiment&quot;] &lt;- &quot;RP neg.&quot; colnames(hp_df) &lt;- c(&quot;colsum&quot;, &quot;id&quot;, &quot;io&quot;, &quot;n&quot;, &quot;experiment&quot;) colnames(hn_df) &lt;- c(&quot;colsum&quot;, &quot;id&quot;, &quot;io&quot;, &quot;n&quot;, &quot;experiment&quot;) colnames(rp_df) &lt;- c(&quot;colsum&quot;, &quot;id&quot;, &quot;io&quot;, &quot;n&quot;, &quot;experiment&quot;) colnames(rn_df) &lt;- c(&quot;colsum&quot;, &quot;id&quot;, &quot;io&quot;, &quot;n&quot;, &quot;experiment&quot;) stability &lt;- rbind(hp_df, hn_df, rp_df, rn_df) library(ggplot2) ggplot(stability, aes(x = n, y = colsum, color = experiment)) + geom_line(size = 1.5) + ggtitle(&quot;Signal intensity during UPLC-HRMS experiments&quot;) + xlab(&quot;sample injection order&quot;) + ylab(&quot;total signal intensity per sample&quot;) + labs(colour = &quot;Experiment&quot;) + theme(legend.position = &quot;bottom&quot;) + geom_smooth(method = &quot;lm&quot;, size = 0.5) + facet_grid(experiment ~ ., scales = &quot;free&quot;) Figure 2.1: Cummulative signal intensity across all samples and chromatographic experiments shows only a minor effect of injection order. rm(hp, hp_df, hn, hn_df, rp, rp_df, rn, rn_df, stability) We see that while signal intensity in all experimental data seems fairly stable, the HILIC positive data set has the highest intensity and virtually no changes at all throughout data acquisition. Therefore, we mostly rely on analyses of the HILIC pos data set. 2.2.8 Summary of data processing Below we outline the number of features removed in every step as shown in the table and corresponding figure. “CV&gt;30%” is the data set subsequently labelled “cleaned”, “repr. feat.” are the PC-group data set and “ion”\" are sometimes also labelled “MH”. initial_f &lt;- c(hp_dim1, hn_dim1, rp_dim1, rn_dim1) rt_f &lt;- c(hp_dim2, hn_dim2, rp_dim2, rn_dim2) cv_f &lt;- c(hp_dim3, hn_dim3, rp_dim3, rn_dim3) representative_f &lt;- c(hp_dim4, hn_dim4, rp_dim4, rn_dim4) ion_f &lt;- c(hp_dim5, hn_dim5, rp_dim5, rn_dim5) initial_p &lt;- c(hp_pcg1, hn_pcg1, rp_pcg1, rn_pcg1) rt_p &lt;- c(hp_pcg2, hn_pcg2, rp_pcg2, rn_pcg2) cv_p &lt;- c(hp_pcg3, hn_pcg3, rp_pcg3, rn_pcg3) representative_p &lt;- c(hp_pcg4, hn_pcg4, rp_pcg4, rn_pcg4) ion_p &lt;- c(hp_pcg5, hn_pcg5, rp_pcg5, rn_pcg5) df &lt;- cbind(initial_f, rt_f, cv_f, representative_f, ion_f, initial_p, rt_p, cv_p, representative_p, ion_p) rownames(df) &lt;- c(&quot;HILIC pos&quot;, &quot;HILIC neg&quot;, &quot;RP pos&quot;, &quot;RP neg&quot;) library(kableExtra) options(kableExtra.html.bsTable = T) kable(df, col.names = c(&quot;initial&quot;, &quot;RT &gt;45 s&quot;, &quot;CV &lt;30%&quot;, &quot;repr. feat.&quot;, &quot;ion&quot;, &quot;initial&quot;, &quot;RT &gt;45 s&quot;, &quot;CV &lt;30%&quot;, &quot;repr. feat.&quot;, &quot;ion&quot;), longtable = T, booktabs = T, caption = &quot;Number of features and pc groups remaining after each filtering step.&quot;) %&gt;% add_header_above(c(&quot;&quot;, features = 5, `pc groups` = 5)) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), full_width = F, latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;)) Table 2.1: Number of features and pc groups remaining after each filtering step. features pc groups initial RT &gt;45 s CV &lt;30% repr. feat. ion initial RT &gt;45 s CV &lt;30% repr. feat. ion HILIC pos 5113 4389 3507 2212 105 3298 2781 2212 2212 79 HILIC neg 4049 3081 2808 1351 123 2044 1495 1351 1351 62 RP pos 5821 5588 4673 2736 171 3420 3277 2736 2736 128 RP neg 3867 3621 3166 1678 105 2069 1918 1678 1678 80 library(reshape2) df &lt;- melt(df) df[&quot;filter&quot;] &lt;- c(rep((&quot;initial&quot;), 4), rep((&quot;RT &gt;45 s&quot;), 4), rep((&quot;CV &lt;30%&quot;), 4), rep((&quot;rep&quot;), 4), rep((&quot;ion&quot;), 4)) df$filter &lt;- factor(df$filter, levels = c(&quot;initial&quot;, &quot;RT &gt;45 s&quot;, &quot;CV &lt;30%&quot;, &quot;rep&quot;, &quot;ion&quot;)) df[&quot;category&quot;] &lt;- c(rep((&quot;feature&quot;), 20), rep((&quot;pc group&quot;), 20)) df$Var2 &lt;- NULL colnames(df) &lt;- c(&quot;acquisition&quot;, &quot;value&quot;, &quot;filter&quot;, &quot;category&quot;) str(df) ## &#39;data.frame&#39;: 40 obs. of 5 variables: ## $ acquisition: Factor w/ 4 levels &quot;HILIC neg&quot;,&quot;HILIC pos&quot;,..: 2 1 4 3 2 1 4 3 2 1 ... ## $ value : Factor w/ 10 levels &quot;cv_f&quot;,&quot;cv_p&quot;,..: 3 3 3 3 9 9 9 9 1 1 ... ## $ filter : int 5113 4049 5821 3867 4389 3081 5588 3621 3507 2808 ... ## $ category : Factor w/ 5 levels &quot;initial&quot;,&quot;RT &gt;45 s&quot;,..: 1 1 1 1 2 2 2 2 3 3 ... ## $ NA : chr &quot;feature&quot; &quot;feature&quot; &quot;feature&quot; &quot;feature&quot; ... # library(ggplot2) ggplot(df, aes(fill=filter, x=acquisition, y=value))+ geom_bar(position=&#39;dodge&#39;, stat = &#39;identity&#39;)+ facet_grid(.~category)+ xlab(&#39;Data acquisition # mode&#39;)+ylab(&#39;Count&#39;)+labs(fill=&#39;Filtering step&#39;)+ theme_bw()+ scale_fill_grey()+ scale_y_continuous(breaks=seq(0,6000,1000))+ theme(axis.text.x = element_text(angle = 45, vjust # = 0.5, hjust = 0.5), legend.position=&#39;bottom&#39;) After preliminary analyses of metabolomics data, a few outliers were re-identified by P. Cárdenas, and one specimen was removed from the data set as it originated from another species. 2.3 Chromatorgrams In G. barretti barettin and 8,9-dihydrobarettin are major compounds with distinct peaks in the chromatogram. We wanted to see whether S. fortis and W. bursa also had major compounds and whether differences in sample depth would be directly reflected in the chromatograms. gb_chrom &lt;- read.csv(&quot;data/Gb_all_chromatograms.csv&quot;, header = T, sep = &quot;;&quot;) # differences in RT negligible/insignificant between the different chromatograms/samples gb_chrom[, c(&quot;Gb10_RT&quot;, &quot;Gb12_RT&quot;, &quot;Gb13_RT&quot;, &quot;Gb14_RT&quot;, &quot;Gb15_RT&quot;, &quot;Gb16_RT&quot;, &quot;Gb17_RT&quot;, &quot;Gb18_RT&quot;, &quot;Gb19_RT&quot;, &quot;Gb2_RT&quot;, &quot;Gb20_RT&quot;, &quot;Gb21_RT&quot;, &quot;Gb3_RT&quot;, &quot;Gb7_RT&quot;, &quot;Gb8_RT&quot;, &quot;Gb9_RT&quot;)] &lt;- list(NULL) colnames(gb_chrom)[colnames(gb_chrom) == &quot;Gb1_RT&quot;] &lt;- &quot;RT&quot; gb_chrom &lt;- gb_chrom[gb_chrom$RT &lt;= 12, ] # removing tail w/o any signal, run ended at 12 min # ugly but works p_gb_chrom &lt;- ggplot(gb_chrom, aes(x = RT, y = Gb1)) + geom_line() + theme_classic() + geom_rect(data = NULL, aes(xmin = 6.3, xmax = 6.5, ymin = -Inf, ymax = Inf), fill = &quot;lightgreen&quot;, alpha = 0.5) + geom_rect(data = NULL, aes(xmin = 6.95, xmax = 7.15, ymin = -Inf, ymax = Inf), fill = &quot;lightblue&quot;, alpha = 0.5) + geom_line(aes(x = RT, y = Gb2)) + geom_line(aes(x = RT, y = Gb3)) + geom_line(aes(x = RT, y = Gb7)) + geom_line(aes(x = RT, y = Gb8)) + geom_line(aes(x = RT, y = Gb9)) + geom_line(aes(x = RT, y = Gb10)) + geom_line(aes(x = RT, y = Gb15)) + geom_line(aes(x = RT, y = Gb16)) + geom_line(aes(x = RT, y = Gb17)) + geom_line(aes(x = RT, y = Gb19)) + geom_line(aes(x = RT, y = Gb21)) + geom_line(aes(x = RT, y = Gb12, color = &quot;red&quot;, alpha = 0.5)) + geom_line(aes(x = RT, y = Gb13, color = &quot;red&quot;, alpha = 0.5)) + geom_line(aes(x = RT, y = Gb14, color = &quot;red&quot;, alpha = 0.5)) + geom_line(aes(x = RT, y = Gb18, color = &quot;red&quot;, alpha = 0.5)) + geom_line(aes(x = RT, y = Gb20, color = &quot;red&quot;, alpha = 0.5)) + xlab(&quot;RT&quot;) + ylab(&quot;Signal intensity&quot;) + ggtitle(&quot;Overlay chromatograms from G. barretti HILIC positive&quot;) + theme(legend.position = &quot;none&quot;) sf_chrom &lt;- read.csv(&quot;data/Sf_all_chromatograms.csv&quot;, header = T, sep = &quot;;&quot;) sf_chrom[, c(&quot;Sf10_RT&quot;, &quot;Sf11_RT&quot;, &quot;Sf12_RT&quot;, &quot;Sf13_RT&quot;, &quot;Sf14_RT&quot;, &quot;Sf15_RT&quot;, &quot;Sf2_RT&quot;, &quot;Sf3_RT&quot;, &quot;Sf4_RT&quot;, &quot;Sf5_RT&quot;, &quot;Sf6_RT&quot;, &quot;Sf7_RT&quot;)] &lt;- list(NULL) colnames(sf_chrom)[colnames(sf_chrom) == &quot;Sf1_RT&quot;] &lt;- &quot;RT&quot; sf_chrom &lt;- sf_chrom[sf_chrom$RT &lt;= 12, ] p_sf_chrom &lt;- ggplot(sf_chrom, aes(x = RT, y = Sf1)) + geom_line() + geom_line(aes(x = RT, y = Sf2)) + geom_line(aes(x = RT, y = Sf3)) + geom_line(aes(x = RT, y = Sf4)) + geom_line(aes(x = RT, y = Sf5)) + geom_line(aes(x = RT, y = Sf6)) + geom_line(aes(x = RT, y = Sf7)) + geom_line(aes(x = RT, y = Sf10, color = &quot;red&quot;, alpha = 0.5)) + geom_line(aes(x = RT, y = Sf11, color = &quot;red&quot;, alpha = 0.5)) + geom_line(aes(x = RT, y = Sf12, color = &quot;red&quot;, alpha = 0.5)) + geom_line(aes(x = RT, y = Sf13, color = &quot;red&quot;, alpha = 0.5)) + geom_line(aes(x = RT, y = Sf14, color = &quot;red&quot;, alpha = 0.5)) + geom_line(aes(x = RT, y = Sf15, color = &quot;red&quot;, alpha = 0.5)) + xlab(&quot;RT&quot;) + ylab(&quot;Signal intensity&quot;) + ggtitle(&quot;Overlay chromatograms from S. fortis HILIC positive&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) wb_chrom &lt;- read.csv(&quot;data/Wb_all_chromatograms.csv&quot;, header = T, sep = &quot;;&quot;) wb_chrom[, c(&quot;Wb10_RT&quot;, &quot;Wb11_RT&quot;, &quot;Wb12_RT&quot;, &quot;Wb13_RT&quot;, &quot;Wb14_RT&quot;, &quot;Wb15_RT&quot;, &quot;Wb16_RT&quot;, &quot;Wb17_RT&quot;, &quot;Wb2_RT&quot;, &quot;Wb3_RT&quot;, &quot;Wb4_RT&quot;, &quot;Wb5_RT&quot;, &quot;Wb6_RT&quot;, &quot;Wb7_RT&quot;, &quot;Wb8_RT&quot;)] &lt;- list(NULL) colnames(wb_chrom)[colnames(wb_chrom) == &quot;Wb1_RT&quot;] &lt;- &quot;RT&quot; wb_chrom &lt;- wb_chrom[wb_chrom$RT &lt;= 12, ] p_wb_chrom &lt;- ggplot(wb_chrom, aes(x = RT, y = Wb1)) + geom_line() + geom_line(aes(x = RT, y = Wb2)) + geom_line(aes(x = RT, y = Wb3)) + geom_line(aes(x = RT, y = Wb4)) + geom_line(aes(x = RT, y = Wb5)) + geom_line(aes(x = RT, y = Wb6)) + geom_line(aes(x = RT, y = Wb7)) + geom_line(aes(x = RT, y = Wb8)) + geom_line(aes(x = RT, y = Wb10)) + geom_line(aes(x = RT, y = Wb11)) + geom_line(aes(x = RT, y = Wb12)) + geom_line(aes(x = RT, y = Wb13, color = &quot;red&quot;, alpha = 0.5)) + geom_line(aes(x = RT, y = Wb14, color = &quot;red&quot;, alpha = 0.5)) + geom_line(aes(x = RT, y = Wb15, color = &quot;red&quot;, alpha = 0.5)) + geom_line(aes(x = RT, y = Wb16, color = &quot;red&quot;, alpha = 0.5)) + geom_line(aes(x = RT, y = Wb17, color = &quot;red&quot;, alpha = 0.5)) + xlab(&quot;RT&quot;) + ylab(&quot;Signal intensity&quot;) + ggtitle(&quot;Overlay chromatograms from W. bursa HILIC positive&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) library(gridExtra) grid.arrange(p_gb_chrom, p_sf_chrom, p_wb_chrom, nrow = 3) rm(p_gb_chrom, p_sf_chrom, p_wb_chrom, gb_chrom, sf_chrom, wb_chrom) 2.4 Multivariate analyses We start by visualising all metabolomes with a PCA and a PERMANOVA to get a first impression of our data. library(ropls) library(dplyr) library(ggplot2) library(tidyverse) library(vegan) # load data hilic_pos &lt;- read.csv(&quot;data/HILIC_pos_20190417_cleaned.csv&quot;, header = T, sep = &quot;,&quot;) hilic_neg &lt;- read.csv(&quot;data/HILIC_neg_20190421_cleaned.csv&quot;, header = T, sep = &quot;,&quot;) rp_pos &lt;- read.csv(&quot;data/RP_pos_20190421_cleaned.csv&quot;, header = T, sep = &quot;,&quot;) rp_neg &lt;- read.csv(&quot;data/RP_neg_20190422_cleaned.csv&quot;, header = T, sep = &quot;,&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) formatting &lt;- function(metabolome, meta_data, r, my_colnames) { formatted &lt;- metabolome formatted &lt;- formatted[, 14:(dim(formatted)[2] - r)] formatted &lt;- data.frame(t(formatted)) formatted[&quot;ID&quot;] &lt;- rownames(formatted) formatted[&quot;unified_ID&quot;] &lt;- meta_data$unified_ID[match(formatted$ID, meta_data[[my_colnames]])] formatted[&quot;filter&quot;] &lt;- str_sub(formatted$unified_ID, 1, 2) formatted &lt;- formatted[!formatted$filter == &quot;QC&quot;, ] formatted &lt;- na.omit(formatted) # formatted$filter &lt;- NULL formatted$ID &lt;- NULL formatted &lt;- formatted[order(formatted$unified_ID), ] rownames(formatted) &lt;- formatted$unified_ID return(formatted) } hilic_pos &lt;- formatting(hilic_pos, meta_data, 6, &quot;LC.MS.HILIC.positive&quot;) hilic_neg &lt;- formatting(hilic_neg, meta_data, 6, &quot;LC.MS.HILIC.negative&quot;) rp_pos &lt;- formatting(rp_pos, meta_data, 6, &quot;LC.MS.RP.positive&quot;) rp_neg &lt;- formatting(rp_neg, meta_data, 6, &quot;LC.MS.RP.negative&quot;) md &lt;- meta_data[, c(&quot;Species&quot;, &quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;YEAR&quot;, &quot;MeanBottomTemp_Cdeg&quot;, &quot;MeanBotSalinity_PSU&quot;, &quot;unified_ID&quot;)] # Overview PCA illustrating the metabolomes differ by sponge species pca_wrapper &lt;- function(metabolome, md, my_title) { metabolome$unified_ID &lt;- NULL metabolome$filter &lt;- NULL metabolome &lt;- prcomp(metabolome, scale = T) k &lt;- summary(metabolome)[[&quot;importance&quot;]] metabolome_df &lt;- data.frame(metabolome$x) #scores, i.e. principal components of the sponge sample metabolome_df[&quot;unified_ID&quot;] &lt;- as.factor(rownames(metabolome_df)) x1 &lt;- paste(&quot;PC1&quot;, round(k[2, 1], digits = 3) * 100, &quot;%&quot;) y1 &lt;- paste(&quot;PC2&quot;, round(k[2, 2], digits = 3) * 100, &quot;%&quot;) metabolome_df &lt;- left_join(metabolome_df[, c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC3&quot;, &quot;unified_ID&quot;)], md[, c(&quot;Species&quot;, &quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;MeanBottomTemp_Cdeg&quot;, &quot;MeanBotSalinity_PSU&quot;, &quot;unified_ID&quot;)]) p &lt;- ggplot(metabolome_df, aes(x = PC1, y = PC2)) + geom_point(size = 3, aes(shape = factor(Species))) + ggtitle(my_title) + xlab(x1) + ylab(y1) + theme(legend.position = &quot;none&quot;) #+labs(shape=&#39;Species&#39;) return(p) } hp_pca &lt;- pca_wrapper(hilic_pos, md, &quot;Hilic pos&quot;) hn_pca &lt;- pca_wrapper(hilic_neg, md, &quot;Hilic neg&quot;) rp_pca &lt;- pca_wrapper(rp_pos, md, &quot;RP pos&quot;) rn_pca &lt;- pca_wrapper(rp_neg, md, &quot;RP pos&quot;) library(gridExtra) grid.arrange(hp_pca, hn_pca, rp_pca, rn_pca, nrow = 2) Figure 2.2: PCA of the cleaned data sets. Dots are Gb samples, squares are Wb samples, traingles are Sf samples. For a general overview of the metabolomic data, Fig. @ is an OPLS model predicting variation as a function of depth versus orthogonal variation in all three sponge metabolomes combined. It is based on the HILIC positive cleaned data. library(ropls) library(dplyr) library(ggplot2) library(tidyverse) library(vegan) # load data hilic_pos &lt;- read.csv(&quot;data/HILIC_pos_20190417_cleaned.csv&quot;, header = T, sep = &quot;,&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) formatting &lt;- function(metabolome, meta_data, r, my_colnames) { formatted &lt;- metabolome formatted &lt;- formatted[, 14:(dim(formatted)[2] - r)] formatted &lt;- data.frame(t(formatted)) formatted[&quot;ID&quot;] &lt;- rownames(formatted) formatted[&quot;unified_ID&quot;] &lt;- meta_data$unified_ID[match(formatted$ID, meta_data[[my_colnames]])] formatted[&quot;filter&quot;] &lt;- str_sub(formatted$unified_ID, 1, 2) formatted &lt;- formatted[!formatted$filter == &quot;QC&quot;, ] formatted &lt;- na.omit(formatted) # formatted$filter &lt;- NULL formatted$ID &lt;- NULL formatted &lt;- formatted[order(formatted$unified_ID), ] rownames(formatted) &lt;- formatted$unified_ID return(formatted) } hilic_pos &lt;- formatting(hilic_pos, meta_data, 6, &quot;LC.MS.HILIC.positive&quot;) md &lt;- meta_data[, c(&quot;Species&quot;, &quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;YEAR&quot;, &quot;MeanBottomTemp_Cdeg&quot;, &quot;MeanBotSalinity_PSU&quot;, &quot;unified_ID&quot;)] md &lt;- md[md$unified_ID %in% rownames(hilic_pos), ] md &lt;- md[order(md$unified_ID), ] all(rownames(hilic_pos) == md$unified_ID) hilic_pos[, c(&quot;unified_ID&quot;, &quot;filter&quot;)] &lt;- list(NULL) hilic.ropls.pca &lt;- opls(hilic_pos, plotL = FALSE) hilic.opls &lt;- opls(hilic_pos, md[, &quot;Depth&quot;], permI = 1000, orthoI = NA, scaleC = &quot;pareto&quot;, plotL = FALSE) h.ortho.score &lt;- getScoreMN(hilic.opls, orthoL = T) h.pred.score &lt;- getScoreMN(hilic.opls, orthoL = F) h.df.3d &lt;- data.frame(h.pred.score, h.ortho.score, md$Depth, md$unified_ID, md$Species) # write.csv(h.df.3d, &#39;data/metabolome_opls_3d.csv&#39;) hilic.opls@summaryDF R2X(cum) R2Y(cum) Q2(cum) RMSEE pre ort pR2Y pQ2 Total 0.534 0.987 0.662 43.7 1 4 0.001 0.001 library(plotly) h.df.3d &lt;- read.csv(&quot;data/metabolome_opls_3d.csv&quot;, sep = &quot;,&quot;, header = T) # format background and axes axx &lt;- list(backgroundcolor = &quot;rgb(211,211,211)&quot;, gridcolor = &quot;rgb(255,255,255)&quot;, title = &quot;predicted&quot;, showbackground = TRUE) axy &lt;- list(backgroundcolor = &quot;rgb(211,211,211)&quot;, gridcolor = &quot;rgb(255,255,255)&quot;, title = &quot;orthogonal 1&quot;, showbackground = TRUE) axz &lt;- list(backgroundcolor = &quot;rgb(211,211,211)&quot;, gridcolor = &quot;rgb(255,255,255)&quot;, title = &quot;orthogonal 2&quot;, showbackground = TRUE) met3D &lt;- plot_ly(h.df.3d, x = ~h.df.3d$p1, y = ~h.df.3d$o1, z = ~h.df.3d$o2, symbol = ~md.Species, symbols = c(&quot;diamond&quot;, &quot;x&quot;, &quot;circle&quot;), color = ~h.df.3d$md.Depth) %&gt;% add_markers() %&gt;% layout(scene = list(xaxis = axx, yaxis = axy, zaxis = axz)) met3D Figure 2.3: Multivariate analysis (OPLS) of all sponge metabolomes by depth. At this point, this is purely for overview. In depth methods and evaluation are provided in the section below. # for saving f&lt;- basename(tempfile(&#39;OPLS_metabolome_plotly&#39;, &#39;.&#39;, &#39;.html&#39;)) on.exit(unlink(f), add = TRUE) html &lt;- htmlwidgets::saveWidget(met3D, f) library(vegan) permanova &lt;- function(metabolome, md) { metabolome$filter &lt;- NULL md &lt;- md[md$unified_ID %in% metabolome$unified_ID, ] metabolome &lt;- metabolome[order(metabolome$unified_ID), ] md &lt;- md[order(md$unified_ID), ] metabolome$unified_ID &lt;- NULL m &lt;- adonis(metabolome ~ Species, md) return(m) } permanova(hilic_pos, md) ## ## Call: ## adonis(formula = metabolome ~ Species, data = md) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## Species 2 5.6422 2.82112 34.302 0.62026 0.001 *** ## Residuals 42 3.4543 0.08224 0.37974 ## Total 44 9.0965 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 permanova(hilic_neg, md) ## ## Call: ## adonis(formula = metabolome ~ Species, data = md) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## Species 2 2.0768 1.03838 26.046 0.55363 0.001 *** ## Residuals 42 1.6744 0.03987 0.44637 ## Total 44 3.7512 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 permanova(rp_pos, md) ## ## Call: ## adonis(formula = metabolome ~ Species, data = md) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## Species 2 6.0969 3.04847 35.972 0.6314 0.001 *** ## Residuals 42 3.5593 0.08475 0.3686 ## Total 44 9.6563 1.0000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 permanova(rp_neg, md) ## ## Call: ## adonis(formula = metabolome ~ Species, data = md) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## Species 2 6.4179 3.2089 36.013 0.63725 0.001 *** ## Residuals 41 3.6533 0.0891 0.36275 ## Total 43 10.0712 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From the PCA and the PERMANOVA (function adonis, Pr(&gt;F) is the p-value), we gather that the metabolomes regardless of how they were acquired are significatly different in the three sponge species. Therefore, we separate the metabolomes for investigating the effect of depth on them individually. In order to investigate whether depth has an effect on the species-specific metabolomes, we use OPLS orthogonal projections of latent structures as multivariate analysis approach. This method allows to separate variations in the data based on e.g. depth and orthogonal, i.e. unrelated variation. # splitting df and adapting meta data spl_ada &lt;- function(metabolome, md) { metabolome_gb &lt;- metabolome[metabolome$filter == &quot;Gb&quot;, ] metabolome_sf &lt;- metabolome[metabolome$filter == &quot;Sf&quot;, ] metabolome_wb &lt;- metabolome[metabolome$filter == &quot;Wb&quot;, ] md_gb &lt;- md[md$unified_ID %in% metabolome_gb$unified_ID, ] md_sf &lt;- md[md$unified_ID %in% metabolome_sf$unified_ID, ] md_wb &lt;- md[md$unified_ID %in% metabolome_wb$unified_ID, ] metabolome_gb &lt;- metabolome_gb[order(metabolome_gb$unified_ID), ] metabolome_sf &lt;- metabolome_sf[order(metabolome_sf$unified_ID), ] metabolome_wb &lt;- metabolome_wb[order(metabolome_wb$unified_ID), ] metabolome_gb$filter &lt;- NULL metabolome_gb$unified_ID &lt;- NULL metabolome_sf$filter &lt;- NULL metabolome_sf$unified_ID &lt;- NULL metabolome_wb$filter &lt;- NULL metabolome_wb$unified_ID &lt;- NULL md_gb &lt;- md_gb[order(md_gb$unified_ID), ] md_sf &lt;- md_sf[order(md_sf$unified_ID), ] md_wb &lt;- md_wb[order(md_wb$unified_ID), ] mva_data &lt;- list(metabolome_gb = metabolome_gb, metabolome_sf = metabolome_sf, metabolome_wb = metabolome_wb, md_gb = md_gb, md_sf = md_sf, md_wb = md_wb) return(mva_data) } hp_opls_df &lt;- spl_ada(hilic_pos, md) hn_opls_df &lt;- spl_ada(hilic_neg, md) rp_opls_df &lt;- spl_ada(rp_pos, md) rn_opls_df &lt;- spl_ada(rp_neg, md) # P should be NA; or 1 ropls_wrapper &lt;- function(opls_df, p1, p2, p3) { # ropls&#39; PCA gb.pca &lt;- opls(opls_df$metabolome_gb, plotL = F, printL = F) sf.pca &lt;- opls(opls_df$metabolome_sf, plotL = F, printL = F) wb.pca &lt;- opls(opls_df$metabolome_wb, plotL = F, printL = F) md_gb &lt;- opls_df$md_gb md_sf &lt;- opls_df$md_sf md_wb &lt;- opls_df$md_wb # ropls&#39; OPLS gb.opls &lt;- opls(opls_df$metabolome_gb, md_gb[, &quot;Depth&quot;], permI = 1000, orthoI = p1, scaleC = &quot;pareto&quot;, plotL = F) sf.opls &lt;- opls(opls_df$metabolome_sf, md_sf[, &quot;Depth&quot;], permI = 1000, orthoI = p2, scaleC = &quot;pareto&quot;, plotL = F) wb.opls &lt;- opls(opls_df$metabolome_wb, md_wb[, &quot;Depth&quot;], permI = 1000, orthoI = p3, scaleC = &quot;pareto&quot;, plotL = F) # summary gb.opls &lt;- gb.opls@summaryDF sf.opls &lt;- sf.opls@summaryDF wb.opls &lt;- wb.opls@summaryDF opls_diagnostics &lt;- rbind(gb.opls, sf.opls, wb.opls) opls_diagnostics[&quot;Species&quot;] &lt;- c(&quot;Geodia barretti&quot;, &quot;Stryphnus fortis&quot;, &quot;Weberella bursa&quot;) return(opls_diagnostics) } a &lt;- Sys.time() hp_cleaned_opls &lt;- ropls_wrapper(hp_opls_df, NA, 1, NA) hn_cleaned_opls &lt;- ropls_wrapper(hn_opls_df, NA, 1, NA) rp_cleaned_opls &lt;- ropls_wrapper(rp_opls_df, NA, 1, NA) rn_cleaned_opls &lt;- ropls_wrapper(rn_opls_df, 1, 1, NA) b &lt;- Sys.time() b - a cleaned &lt;- rbind(hp_cleaned_opls, hn_cleaned_opls, rp_cleaned_opls, rn_cleaned_opls) cleaned[&quot;Experiment&quot;] &lt;- c(rep(&quot;HILIC pos&quot;, 3), rep(&quot;HILIC neg&quot;, 3), rep(&quot;RP pos&quot;, 3), rep(&quot;RP neg&quot;, 3)) write.csv(cleaned, &quot;data/cleaned_opls_diagnostics.csv&quot;, row.names = F) knitr::include_graphics(&quot;data/opls_Gb_Hp.jpeg&quot;) Figure 2.4: Example output for OPLS models generated with ROPLS. This is HILIC positive cleaned data set for G. barretti. In the same manner, we can analyse the two remaining sets produced by different filtering options. # load data ION hilic_pos &lt;- read.csv(&quot;data/HILIC_pos_20190417_cleaned_MH.csv&quot;, header = T, sep = &quot;,&quot;) hilic_neg &lt;- read.csv(&quot;data/HILIC_neg_20190421_cleaned_MH.csv&quot;, header = T, sep = &quot;,&quot;) rp_pos &lt;- read.csv(&quot;data/RP_pos_20190421_cleaned_MH.csv&quot;, header = T, sep = &quot;,&quot;) rp_neg &lt;- read.csv(&quot;data/RP_neg_20190422_cleaned_MH.csv&quot;, header = T, sep = &quot;,&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) hilic_pos &lt;- formatting(hilic_pos, meta_data, 8, &quot;LC.MS.HILIC.positive&quot;) hilic_neg &lt;- formatting(hilic_neg, meta_data, 8, &quot;LC.MS.HILIC.negative&quot;) rp_pos &lt;- formatting(rp_pos, meta_data, 8, &quot;LC.MS.RP.positive&quot;) rp_neg &lt;- formatting(rp_neg, meta_data, 8, &quot;LC.MS.RP.negative&quot;) md &lt;- meta_data[, c(&quot;Species&quot;, &quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;YEAR&quot;, &quot;MeanBottomTemp_Cdeg&quot;, &quot;MeanBotSalinity_PSU&quot;, &quot;unified_ID&quot;)] hp_pca &lt;- pca_wrapper(hilic_pos, md, &quot;Hilic pos&quot;) hn_pca &lt;- pca_wrapper(hilic_neg, md, &quot;Hilic neg&quot;) rp_pca &lt;- pca_wrapper(rp_pos, md, &quot;RP pos&quot;) rn_pca &lt;- pca_wrapper(rp_neg, md, &quot;RP pos&quot;) grid.arrange(hp_pca, hn_pca, rp_pca, rn_pca, nrow = 2) Figure 2.5: PCA of the metabolome data set filtered to retain only annotated ions. Dots are Gb samples, squares are Wb samples, traingles are Sf samples. permanova(hilic_pos, md) ## ## Call: ## adonis(formula = metabolome ~ Species, data = md) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## Species 2 7.9121 3.9560 48.131 0.69623 0.001 *** ## Residuals 42 3.4521 0.0822 0.30377 ## Total 44 11.3642 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 permanova(hilic_neg, md) ## ## Call: ## adonis(formula = metabolome ~ Species, data = md) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## Species 2 1.23799 0.61899 31.471 0.59978 0.001 *** ## Residuals 42 0.82609 0.01967 0.40022 ## Total 44 2.06408 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 permanova(rp_pos, md) ## ## Call: ## adonis(formula = metabolome ~ Species, data = md) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## Species 2 8.9172 4.4586 52.824 0.71554 0.001 *** ## Residuals 42 3.5450 0.0844 0.28446 ## Total 44 12.4621 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 permanova(rp_neg, md) ## ## Call: ## adonis(formula = metabolome ~ Species, data = md) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## Species 2 7.4863 3.7432 33.508 0.62043 0.001 *** ## Residuals 41 4.5801 0.1117 0.37957 ## Total 43 12.0665 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 hp_opls_df &lt;- spl_ada(hilic_pos, md) hn_opls_df &lt;- spl_ada(hilic_neg, md) rp_opls_df &lt;- spl_ada(rp_pos, md) rn_opls_df &lt;- spl_ada(rp_neg, md) a &lt;- Sys.time() hp_cleaned_opls &lt;- ropls_wrapper(hp_opls_df, NA, 1, NA) hn_cleaned_opls &lt;- ropls_wrapper(hn_opls_df, NA, NA, NA) rp_cleaned_opls &lt;- ropls_wrapper(rp_opls_df, NA, 1, NA) rn_cleaned_opls &lt;- ropls_wrapper(rn_opls_df, 1, 1, NA) b &lt;- Sys.time() b - a ion &lt;- rbind(hp_cleaned_opls, hn_cleaned_opls, rp_cleaned_opls, rn_cleaned_opls) ion[&quot;Experiment&quot;] &lt;- c(rep(&quot;HILIC pos&quot;, 3), rep(&quot;HILIC neg&quot;, 3), rep(&quot;RP pos&quot;, 3), rep(&quot;RP neg&quot;, 3)) write.csv(ion, &quot;data/cleaned_MH_opls_diagnostics.csv&quot;, row.names = F) # load data PC_GROUP hilic_pos &lt;- read.csv(&quot;data/HILIC_pos_20190417_cleaned_pcgroup.csv&quot;, header = T, sep = &quot;,&quot;) hilic_neg &lt;- read.csv(&quot;data/HILIC_neg_20190421_cleaned_pcgroup.csv&quot;, header = T, sep = &quot;,&quot;) rp_pos &lt;- read.csv(&quot;data/RP_pos_20190421_cleaned_pcgroup.csv&quot;, header = T, sep = &quot;,&quot;) rp_neg &lt;- read.csv(&quot;data/RP_neg_20190422_cleaned_pcgroup.csv&quot;, header = T, sep = &quot;,&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) hilic_pos &lt;- formatting(hilic_pos, meta_data, 8, &quot;LC.MS.HILIC.positive&quot;) hilic_neg &lt;- formatting(hilic_neg, meta_data, 8, &quot;LC.MS.HILIC.negative&quot;) rp_pos &lt;- formatting(rp_pos, meta_data, 8, &quot;LC.MS.RP.positive&quot;) rp_neg &lt;- formatting(rp_neg, meta_data, 8, &quot;LC.MS.RP.negative&quot;) md &lt;- meta_data[, c(&quot;Species&quot;, &quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;YEAR&quot;, &quot;MeanBottomTemp_Cdeg&quot;, &quot;MeanBotSalinity_PSU&quot;, &quot;unified_ID&quot;)] hp_pca &lt;- pca_wrapper(hilic_pos, md, &quot;Hilic pos&quot;) hn_pca &lt;- pca_wrapper(hilic_neg, md, &quot;Hilic neg&quot;) rp_pca &lt;- pca_wrapper(rp_pos, md, &quot;RP pos&quot;) rn_pca &lt;- pca_wrapper(rp_neg, md, &quot;RP pos&quot;) grid.arrange(hp_pca, hn_pca, rp_pca, rn_pca, nrow = 2) Figure 2.6: PCA of the metabolome data set filtered to retain the feature with the strongest signal per PC group. Dots are Gb samples, squares are Wb samples, traingles are Sf samples. permanova(hilic_pos, md) ## ## Call: ## adonis(formula = metabolome ~ Species, data = md) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## Species 2 6.0660 3.03302 33.376 0.6138 0.001 *** ## Residuals 42 3.8167 0.09087 0.3862 ## Total 44 9.8828 1.0000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 permanova(hilic_neg, md) ## ## Call: ## adonis(formula = metabolome ~ Species, data = md) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## Species 2 5.1323 2.56616 32.819 0.6098 0.001 *** ## Residuals 42 3.2840 0.07819 0.3902 ## Total 44 8.4163 1.0000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 permanova(rp_pos, md) ## ## Call: ## adonis(formula = metabolome ~ Species, data = md) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## Species 2 5.7191 2.85957 35.582 0.62886 0.001 *** ## Residuals 42 3.3754 0.08037 0.37114 ## Total 44 9.0945 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 permanova(rp_neg, md) ## ## Call: ## adonis(formula = metabolome ~ Species, data = md) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## Species 2 6.8700 3.4350 33.333 0.61919 0.001 *** ## Residuals 41 4.2251 0.1031 0.38081 ## Total 43 11.0952 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 hp_opls_df &lt;- spl_ada(hilic_pos, md) hn_opls_df &lt;- spl_ada(hilic_neg, md) rp_opls_df &lt;- spl_ada(rp_pos, md) rn_opls_df &lt;- spl_ada(rp_neg, md) a &lt;- Sys.time() hp_cleaned_opls &lt;- ropls_wrapper(hp_opls_df, NA, NA, NA) hn_cleaned_opls &lt;- ropls_wrapper(hn_opls_df, NA, NA, NA) rp_cleaned_opls &lt;- ropls_wrapper(rp_opls_df, NA, NA, NA) rn_cleaned_opls &lt;- ropls_wrapper(rn_opls_df, NA, NA, NA) b &lt;- Sys.time() b - a pcg &lt;- rbind(hp_cleaned_opls, hn_cleaned_opls, rp_cleaned_opls, rn_cleaned_opls) pcg[&quot;Experiment&quot;] &lt;- c(rep(&quot;HILIC pos&quot;, 3), rep(&quot;HILIC neg&quot;, 3), rep(&quot;RP pos&quot;, 3), rep(&quot;RP neg&quot;, 3)) write.csv(pcg, &quot;data/cleaned_PC_opls_diagnostics.csv&quot;, row.names = F) In some cases, an OPLS model was not appropriate. Typically, the model chooses the number of orthogonal components by itself (orthI=NA) and when the first component is not significant, the model building will stop. However, one can force any number of orthogonal components and run the OPLS model anyway. You can spot those cases when the NA was set to 1 in the ropls_wrapper function The multivariate analyses (OPLS) were quite time consuming, requiring 1.4 hours for the “cleaned” data sets, 46.3 min for the “PC-group” data stes and 4 min for the “ion” data sets on my system. Therefore, the combined model diagnostics are summarised in Tab 2.2. library(kableExtra) cleaned &lt;- read.csv(&quot;data/cleaned_opls_diagnostics.csv&quot;, header = T) pcgrp &lt;- read.csv(&quot;data/cleaned_PC_opls_diagnostics.csv&quot;, header = T) ion &lt;- read.csv(&quot;data/cleaned_MH_opls_diagnostics.csv&quot;, header = T) opls_diagnostics &lt;- rbind(cleaned, pcgrp, ion) opls_diagnostics[&quot;filtering&quot;] &lt;- c(rep(&quot;cleaned&quot;, 12), rep(&quot;pc group&quot;, 12), rep(&quot;ion&quot;, 12)) options(kableExtra.html.bsTable = T) kable(opls_diagnostics, col.names = c(&quot;R2X cum&quot;, &quot;R2Y cum&quot;, &quot;Q2 cum&quot;, &quot;RMSEE&quot;, &quot;pre&quot;, &quot;ort&quot;, &quot;pR2Y&quot;, &quot;pQ2&quot;, &quot;Species&quot;, &quot;Experiment&quot;, &quot;data set&quot;), longtable = T, booktabs = T, caption = &quot;Model diagnostics of the metabolome variations modelled by depth&quot;, row.names = FALSE) %&gt;% add_header_above(c(Diagnostics = 8, `Data set attribution` = 3)) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), font_size = 12, full_width = F, latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;)) Table 2.2: Model diagnostics of the metabolome variations modelled by depth Diagnostics Data set attribution R2X cum R2Y cum Q2 cum RMSEE pre ort pR2Y pQ2 Species Experiment data set 0.425 0.998 0.8670 19.7 1 2 0.001 0.001 Geodia barretti HILIC pos cleaned 0.257 0.988 0.6530 42.0 1 1 0.067 0.002 Stryphnus fortis HILIC pos cleaned 0.496 0.999 0.5490 13.7 1 3 0.002 0.028 Weberella bursa HILIC pos cleaned 0.464 0.988 0.7670 47.4 1 2 0.010 0.001 Geodia barretti HILIC neg cleaned 0.436 0.924 0.6290 106.0 1 1 0.074 0.002 Stryphnus fortis HILIC neg cleaned 0.366 0.985 0.5560 42.4 1 2 0.233 0.013 Weberella bursa HILIC neg cleaned 0.585 0.998 0.7960 22.6 1 3 0.003 0.001 Geodia barretti RP pos cleaned 0.328 0.965 0.5680 71.5 1 1 0.039 0.010 Stryphnus fortis RP pos cleaned 0.343 0.980 0.3350 48.8 1 2 0.113 0.108 Weberella bursa RP pos cleaned 0.304 0.937 0.6140 99.8 1 1 0.040 0.003 Geodia barretti RP neg cleaned 0.390 0.924 0.5060 106.0 1 1 0.089 0.015 Stryphnus fortis RP neg cleaned 0.412 0.985 0.4500 41.9 1 2 0.029 0.027 Weberella bursa RP neg cleaned 0.429 0.997 0.8660 23.7 1 2 0.003 0.001 Geodia barretti HILIC pos pc group 0.362 0.998 0.7020 19.9 1 2 0.049 0.004 Stryphnus fortis HILIC pos pc group 0.542 0.995 0.5820 25.9 1 3 0.114 0.015 Weberella bursa HILIC pos pc group 0.499 0.990 0.7500 43.2 1 2 0.004 0.002 Geodia barretti HILIC neg pc group 0.433 0.990 0.7230 40.9 1 2 0.068 0.002 Stryphnus fortis HILIC neg pc group 0.390 0.986 0.6000 39.8 1 2 0.132 0.005 Weberella bursa HILIC neg pc group 0.602 0.996 0.8090 28.9 1 3 0.014 0.001 Geodia barretti RP pos pc group 0.464 0.990 0.5850 40.9 1 2 0.122 0.017 Stryphnus fortis RP pos pc group 0.393 0.979 0.3590 49.8 1 2 0.045 0.115 Weberella bursa RP pos pc group 0.373 0.979 0.6850 59.9 1 2 0.137 0.002 Geodia barretti RP neg pc group 0.637 0.991 0.6140 39.9 1 3 0.455 0.035 Stryphnus fortis RP neg pc group 0.440 0.980 0.4170 48.6 1 2 0.089 0.049 Weberella bursa RP neg pc group 0.761 0.995 0.6610 34.2 1 5 0.072 0.018 Geodia barretti HILIC pos ion 0.294 0.915 0.3540 112.0 1 1 0.226 0.109 Stryphnus fortis HILIC pos ion 0.694 0.988 0.6570 40.2 1 4 0.032 0.019 Weberella bursa HILIC pos ion 0.401 0.942 0.6580 105.0 1 2 0.013 0.002 Geodia barretti HILIC neg ion 0.815 0.992 0.7570 39.5 1 3 0.007 0.008 Stryphnus fortis HILIC neg ion 0.556 0.949 0.6530 77.3 1 2 0.005 0.004 Weberella bursa HILIC neg ion 0.778 0.910 0.7940 126.0 1 1 0.001 0.001 Geodia barretti RP pos ion 0.368 0.874 0.2560 137.0 1 1 0.026 0.159 Stryphnus fortis RP pos ion 0.479 0.960 0.4060 68.1 1 2 0.005 0.032 Weberella bursa RP pos ion 0.430 0.624 0.0345 244.0 1 1 0.268 0.198 Geodia barretti RP neg ion 0.821 0.392 0.0936 300.0 1 1 0.585 0.190 Stryphnus fortis RP neg ion 0.804 0.973 0.5850 61.8 1 4 0.065 0.027 Weberella bursa RP neg ion # write.csv(opls_diagnostics, &#39;data/opls_diagnostics.csv&#39;, row.names = F) opls_diagnostics$Species &lt;- as.character(opls_diagnostics$Species) opls_diagnostics$Species[opls_diagnostics$Species == &quot;Geodia barretti&quot;] &lt;- c(&quot;G. barretti&quot;) opls_diagnostics$Species[opls_diagnostics$Species == &quot;Stryphnus fortis&quot;] &lt;- c(&quot;S. fortis&quot;) opls_diagnostics$Species[opls_diagnostics$Species == &quot;Weberella bursa&quot;] &lt;- c(&quot;W. bursa&quot;) k &lt;- ggplot(opls_diagnostics, aes(x = R2X.cum., y = Q2.cum.)) + geom_point(aes(col = Species, shape = Experiment)) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + xlab(&quot;R2&quot;) + ylab(&quot;Q2&quot;) + guides(col = guide_legend(title = &quot;&quot;), shape = guide_legend(title = &quot;&quot;)) library(ggExtra) ggMarginal(k, groupColour = TRUE, groupFill = TRUE) Figure 2.7: Model performance visualised. The larger R2 and Q2, the better the model fits with our data. 2.5 VIPS From the multivariate model (OPLS) you can derive a vaiable importance in projection (VIF) score for every feature in this model. This is a approximation (but not a statistics test!) of features that support the model. A feature with a VIP score \\(\\geq\\) 1 is considered important and the higher the score, the greater the contribution to the model. 2.6 Metabolites and correlations A manually curated data set of signal intensities was produced for analysis of identified known and novel compounds. library(ggplot2) library(ggrepel) library(reshape) library(tidyverse) library(forcats) cmp &lt;- read.csv(&quot;data/metabolite_master_20190605.csv&quot;, header = T, sep = &quot;,&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) md &lt;- meta_data[c(&quot;Depth&quot;, &quot;unified_ID&quot;)] # FUN prep: remove, QC samples format depth and IDs prep &lt;- function(metabolites, md) { metabolites$X &lt;- NULL metabolites[&quot;spec&quot;] &lt;- str_sub(metabolites$unified_ID, 1, 2) metabolites &lt;- metabolites[!metabolites$spec == &quot;QC&quot;, ] metabolites &lt;- left_join(metabolites, md) metabolites &lt;- metabolites[-c(9), ] #removes Gb16 (Geodia atlantica) metabolites$Depth &lt;- as.numeric(metabolites$Depth) metabolites &lt;- metabolites[with(metabolites, order(metabolites$spec, metabolites$Depth)), ] metabolites[] &lt;- lapply(metabolites, function(x) if (is.factor(x)) factor(x) else x) metabolites$unified_ID &lt;- factor(metabolites$unified_ID, levels = metabolites$unified_ID) return(metabolites) } metabolites &lt;- prep(cmp, md) library(reshape2) mets &lt;- metabolites[, c(&quot;asb&quot;, &quot;his&quot;, &quot;cre&quot;, &quot;pch&quot;, &quot;crn&quot;, &quot;acl&quot;, &quot;ser&quot;, &quot;mbc&quot;, &quot;chol_s&quot;, &quot;bet&quot;, &quot;choline&quot;, &quot;ura&quot;, &quot;unified_ID&quot;, &quot;spec&quot;, &quot;Depth&quot;)] colnames(mets) &lt;- c(&quot;arseno- betaine&quot;, &quot;histamine&quot;, &quot;creatine&quot;, &quot;phospho- choline&quot;, &quot;carnitine&quot;, &quot;acetyl- choline&quot;, &quot;serotonin&quot;, &quot;2-methyl- butyroyl- carnitine&quot;, &quot;choline sulfate&quot;, &quot;betaine&quot;, &quot;choline&quot;, &quot;uranidine&quot;, &quot;unified_ID&quot;, &quot;spec&quot;, &quot;Depth&quot;) mets &lt;- melt(mets, id.vars = c(&quot;spec&quot;, &quot;Depth&quot;, &quot;unified_ID&quot;)) mets[&quot;Species&quot;] &lt;- NA mets$Species[mets$spec == &quot;Gb&quot;] &lt;- c(&quot;Geodia barretti&quot;) mets$Species[mets$spec == &quot;Sf&quot;] &lt;- c(&quot;Stryphnus fortis&quot;) mets$Species[mets$spec == &quot;Wb&quot;] &lt;- c(&quot;Weberella bursa&quot;) # labeller needs formula interface all &lt;- ggplot(mets, aes(x = Depth, y = value)) + geom_point() + facet_grid(variable ~ Species, labeller = labeller(variable = label_wrap_gen(10, multi_line = TRUE)), scales = &quot;free&quot;) all &lt;- all + ggtitle(&quot;VIPs&quot;) + ylab(&quot;signal&quot;) + xlab(&quot;Depth&quot;) + theme_bw() + theme(strip.text.x = element_text(face = &quot;italic&quot;), panel.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;), panel.border = element_rect(colour = &quot;black&quot;, fill = NA), axis.text.x = element_text(angle = 90, hjust = 1)) + scale_y_continuous(labels = scales::scientific) + scale_x_continuous(breaks = scales::pretty_breaks(n = 12)) all cmp &lt;- read.csv(&quot;data/metabolite_master_20190605.csv&quot;, header = T, sep = &quot;,&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) md &lt;- meta_data[c(&quot;Depth&quot;, &quot;unified_ID&quot;)] metabolites &lt;- prep(cmp, md) mets &lt;- metabolites[, c(&quot;asb&quot;, &quot;his&quot;, &quot;cre&quot;, &quot;pch&quot;, &quot;crn&quot;, &quot;acl&quot;, &quot;ser&quot;, &quot;mbc&quot;, &quot;chol_s&quot;, &quot;bet&quot;, &quot;choline&quot;, &quot;ura&quot;, &quot;spec&quot;, &quot;Depth&quot;)] mets_gb &lt;- mets[mets$spec == &quot;Gb&quot;, ] mets_sf &lt;- mets[mets$spec == &quot;Sf&quot;, ] mets_wb &lt;- mets[mets$spec == &quot;Wb&quot;, ] mets_gb$spec &lt;- NULL mets_sf$spec &lt;- NULL mets_wb$spec &lt;- NULL mets_gb$ser &lt;- NULL # too many NAs metabolite_stats &lt;- function(mets) { # Prepare df for collecting results results &lt;- data.frame(colnames(mets)[-length(colnames(mets))]) colnames(results) &lt;- c(&quot;metabolite&quot;) results[, c(&quot;cor_rho&quot;, &quot;cor_p.value&quot;, &quot;cor_fdr&quot;, &quot;t.test_stat&quot;, &quot;t.test_param&quot;, &quot;t.test_p.val&quot;, &quot;t.test_fdr&quot;)] &lt;- NA n &lt;- 0 k &lt;- dim(mets)[2] - 1 while (n &lt; k) { n &lt;- n + 1 corr &lt;- cor.test(mets[, n], mets$Depth, method = &quot;spearman&quot;) results$cor_rho[n] &lt;- corr$estimate results$cor_p.value[n] &lt;- corr$p.value tt &lt;- t.test(mets[, n][mets$Depth &lt; 1000], mets[, n][mets$Depth &gt; 1000]) results$t.test_stat[n] &lt;- tt$statistic results$t.test_param[n] &lt;- tt$parameter results$t.test_p.val[n] &lt;- tt$p.value } cor_fdr &lt;- c(p.adjust(results$cor_p.value, method = &quot;fdr&quot;)) # colnames(cor_fdr) &lt;- c(&#39;cor_fdr&#39;) results$cor_fdr &lt;- cor_fdr t.test_fdr &lt;- c(p.adjust(results$t.test_p.val, method = &quot;fdr&quot;)) # colnames(t.test_fdr) &lt;- c(&#39;t.test_fdr&#39;) results$t.test_fdr &lt;- t.test_fdr return(results) } gb_met_stats &lt;- metabolite_stats(mets_gb) sf_met_stats &lt;- metabolite_stats(mets_sf) wb_met_stats &lt;- metabolite_stats(mets_wb) gb_met_stats[&quot;Species&quot;] &lt;- c(&quot;Gb&quot;) sf_met_stats[&quot;Species&quot;] &lt;- c(&quot;Sf&quot;) wb_met_stats[&quot;Species&quot;] &lt;- c(&quot;Wb&quot;) gb_met_stats$metabolite &lt;- c(&quot;arsenobetaine&quot;, &quot;histamine&quot;, &quot;creatine&quot;, &quot;phosphocholine&quot;, &quot;carnitine&quot;, &quot;acetylcholine&quot;, &quot;2-methyl-butyroyl-carnitine&quot;, &quot;choline sulfate&quot;, &quot;betaine&quot;, &quot;choline&quot;, &quot;uranidine&quot;) sf_met_stats$metabolite &lt;- c(&quot;arsenobetaine&quot;, &quot;histamine&quot;, &quot;creatine&quot;, &quot;phosphocholine&quot;, &quot;carnitine&quot;, &quot;acetylcholine&quot;, &quot;serotonin&quot;, &quot;2-methyl-butyroyl-carnitine&quot;, &quot;choline sulfate&quot;, &quot;betaine&quot;, &quot;choline&quot;, &quot;uranidine&quot;) wb_met_stats$metabolite &lt;- c(&quot;arsenobetaine&quot;, &quot;histamine&quot;, &quot;creatine&quot;, &quot;phosphocholine&quot;, &quot;carnitine&quot;, &quot;acetylcholine&quot;, &quot;serotonin&quot;, &quot;2-methyl-butyroyl-carnitine&quot;, &quot;choline sulfate&quot;, &quot;betaine&quot;, &quot;choline&quot;, &quot;uranidine&quot;) results &lt;- rbind(gb_met_stats, sf_met_stats, wb_met_stats) kable(results, digits = 4, longtable = T, booktabs = T, caption = &quot;Depth response for VIP metabolites as approximated by different methods.&quot;) %&gt;% add_header_above(c(&quot;&quot;, `depth correlation` = 3, `depth groups` = 4, &quot;&quot;)) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), font_size = 12, full_width = F, latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;)) Table 2.3: Depth response for VIP metabolites as approximated by different methods. depth correlation depth groups metabolite cor_rho cor_p.value cor_fdr t.test_stat t.test_param t.test_p.val t.test_fdr Species arsenobetaine -0.7214 0.0033 0.0123 4.2663 10.4841 0.0015 0.0037 Gb histamine -0.1821 0.5151 0.5666 1.5297 5.6083 0.1804 0.2205 Gb creatine -0.5879 0.0381 0.0838 3.1653 6.2829 0.0183 0.0335 Gb phosphocholine 0.6429 0.0117 0.0321 -3.1536 4.0422 0.0339 0.0533 Gb carnitine -0.7214 0.0033 0.0123 9.8370 10.2165 0.0000 0.0000 Gb acetylcholine -0.0750 0.7926 0.7926 0.4637 10.5121 0.6523 0.7175 Gb 2-methyl-butyroyl-carnitine -0.2357 0.3966 0.5308 1.7835 9.4808 0.1065 0.1464 Gb choline sulfate -0.5036 0.0582 0.1067 6.6420 11.1460 0.0000 0.0002 Gb betaine 0.4786 0.0735 0.1154 -4.0423 11.7684 0.0017 0.0037 Gb choline 0.2179 0.4343 0.5308 0.0953 4.4250 0.9282 0.9282 Gb uranidine -0.8321 0.0002 0.0017 5.9710 10.5715 0.0001 0.0004 Gb arsenobetaine -0.1923 0.5292 0.6350 0.3633 10.9907 0.7233 0.8187 Sf histamine -0.4945 0.0889 0.1524 2.2668 10.7158 0.0452 0.0906 Sf creatine -0.7967 0.0018 0.0111 5.4903 10.3905 0.0002 0.0028 Sf phosphocholine 0.7637 0.0036 0.0144 -4.5545 5.4943 0.0048 0.0290 Sf carnitine 0.5275 0.0673 0.1347 -2.3572 8.2289 0.0453 0.0906 Sf acetylcholine -0.0165 0.9639 0.9639 0.2366 8.4117 0.8187 0.8187 Sf serotonin 0.5824 0.0403 0.0967 -1.7377 5.6343 0.1361 0.2296 Sf 2-methyl-butyroyl-carnitine 0.8681 0.0001 0.0015 -3.3615 6.5364 0.0134 0.0534 Sf choline sulfate -0.3147 0.3195 0.4260 1.6828 5.0121 0.1531 0.2296 Sf betaine -0.3516 0.2392 0.3588 -0.2726 9.6152 0.7909 0.8187 Sf choline 0.7198 0.0075 0.0225 -2.5623 10.9543 0.0265 0.0795 Sf uranidine -0.1264 0.6826 0.7447 0.7749 7.8221 0.4612 0.6149 Sf arsenobetaine 0.6059 0.0148 0.1772 -2.0577 4.8196 0.0968 0.2782 Wb histamine -0.1765 0.5122 0.6028 0.5960 9.6416 0.5649 0.6510 Wb creatine 0.4000 0.1259 0.3776 -1.7483 9.3690 0.1130 0.2782 Wb phosphocholine 0.2363 0.4367 0.6028 -0.5809 3.4914 0.5967 0.6510 Wb carnitine 0.5294 0.0373 0.2237 -1.9944 4.0553 0.1159 0.2782 Wb acetylcholine -0.1882 0.4839 0.6028 1.1729 11.0101 0.2656 0.4963 Wb serotonin -0.1736 0.5526 0.6028 0.6508 3.3959 0.5565 0.6510 Wb 2-methyl-butyroyl-carnitine 0.2029 0.4496 0.6028 -1.2445 3.5316 0.2895 0.4963 Wb choline sulfate -0.0143 0.9642 0.9642 -0.3684 3.8040 0.7321 0.7321 Wb betaine 0.3382 0.2000 0.4801 -0.8833 10.8282 0.3963 0.5944 Wb choline 0.2588 0.3319 0.6028 -2.0538 7.2597 0.0776 0.2782 Wb uranidine -0.4294 0.0985 0.3776 2.0573 11.8063 0.0624 0.2782 Wb cmp &lt;- read.csv(&quot;data/metabolite_master_20190605.csv&quot;, header = T, sep = &quot;,&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) md &lt;- meta_data[c(&quot;Depth&quot;, &quot;unified_ID&quot;, &quot;Species&quot;)] # 6-bromo-8-hydroxy-conicamin = cp4 cyclo Pro-Arg = solterin or soelterin # Prepare df for Gb compounds metabolites &lt;- prep(cmp, md) mets &lt;- metabolites[, c(&quot;bar&quot;, &quot;dhb&quot;, &quot;dhhb&quot;, &quot;GeoA&quot;, &quot;GeoAi&quot;, &quot;GeoB&quot;, &quot;sol&quot;, &quot;brc&quot;, &quot;cp4&quot;, &quot;GeoC&quot;, &quot;L6bhp&quot;, &quot;barrettide_A&quot;, &quot;barrettide_B&quot;, &quot;unified_ID&quot;, &quot;Depth&quot;, &quot;Species&quot;)] mets &lt;- mets[mets$Species == &quot;Geodia barretti&quot;, ] colnames(mets) &lt;- c(&quot;barettin&quot;, &quot;8,9-dihydrobarettin&quot;, &quot;8,9−dihydro−8−hydroxy−barettin&quot;, &quot;Geobarrettin A&quot;, &quot;Geobarrettin A isomer&quot;, &quot;Geobarrettin B&quot;, &quot;cyclo Pro-Arg&quot;, &quot;6-bromo-conicamin&quot;, &quot;6-bromo-8-hydroxy-conicamin&quot;, &quot;Geobarrettin C&quot;, &quot;L-6-bromo- \\n hypaphorine&quot;, &quot;Barrettide A&quot;, &quot;Barrettide B&quot;, &quot;unified_ID&quot;, &quot;Depth&quot;, &quot;Species&quot;) mets[, c(&quot;unified_ID&quot;, &quot;Species&quot;)] &lt;- list(NULL) mets &lt;- reshape2::melt(mets, id.vars = c(&quot;Depth&quot;)) ggplot(mets, aes(x = Depth, y = value)) + geom_point() + facet_wrap(. ~ variable, scales = &quot;free&quot;) + theme_bw() mets &lt;- metabolites[, c(&quot;sfn&quot;, &quot;ian&quot;, &quot;unified_ID&quot;, &quot;Depth&quot;, &quot;Species&quot;)] mets &lt;- mets[mets$Species == &quot;Stryphnus fortis&quot;, ] mets[, c(&quot;unified_ID&quot;, &quot;Species&quot;)] &lt;- list(NULL) colnames(mets) &lt;- c(&quot;Stryphnusin&quot;, &quot;Ianthellin&quot;, &quot;Depth&quot;) mets &lt;- reshape2::melt(mets, id.vars = c(&quot;Depth&quot;)) ggplot(mets, aes(x = Depth, y = value)) + geom_point() + facet_wrap(. ~ variable, scales = &quot;free&quot;) + theme_bw() cmp &lt;- read.csv(&quot;data/metabolite_master_20190605.csv&quot;, header = T, sep = &quot;,&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) md &lt;- meta_data[c(&quot;Depth&quot;, &quot;unified_ID&quot;, &quot;Species&quot;)] # 6-bromo-8-hydroxy-conicamin = cp4 cyclo Pro-Arg = solterin or soelterin # Prepare df for Gb compounds metabolites &lt;- prep(cmp, md) mets &lt;- metabolites[, c(&quot;bar&quot;, &quot;dhb&quot;, &quot;dhhb&quot;, &quot;GeoA&quot;, &quot;GeoAi&quot;, &quot;GeoB&quot;, &quot;sol&quot;, &quot;brc&quot;, &quot;cp4&quot;, &quot;GeoC&quot;, &quot;L6bhp&quot;, &quot;barrettide_A&quot;, &quot;barrettide_B&quot;, &quot;unified_ID&quot;, &quot;Depth&quot;, &quot;Species&quot;)] mets &lt;- mets[mets$Species == &quot;Geodia barretti&quot;, ] mets[, c(&quot;unified_ID&quot;, &quot;Species&quot;)] &lt;- list(NULL) metabolite_stats &lt;- function(mets) { # Prepare df for collecting results results &lt;- data.frame(colnames(mets)[-length(colnames(mets))]) colnames(results) &lt;- c(&quot;metabolite&quot;) results[, c(&quot;cor_rho&quot;, &quot;cor_p.value&quot;, &quot;cor_fdr&quot;, &quot;t.test_stat&quot;, &quot;t.test_param&quot;, &quot;t.test_p.val&quot;, &quot;t.test_fdr&quot;)] &lt;- NA n &lt;- 0 k &lt;- dim(mets)[2] - 1 while (n &lt; k) { n &lt;- n + 1 corr &lt;- cor.test(mets[, n], mets$Depth, method = &quot;spearman&quot;) results$cor_rho[n] &lt;- corr$estimate results$cor_p.value[n] &lt;- corr$p.value tt &lt;- t.test(mets[, n][mets$Depth &lt; 1000], mets[, n][mets$Depth &gt; 1000]) results$t.test_stat[n] &lt;- tt$statistic results$t.test_param[n] &lt;- tt$parameter results$t.test_p.val[n] &lt;- tt$p.value } cor_fdr &lt;- c(p.adjust(results$cor_p.value, method = &quot;fdr&quot;)) # colnames(cor_fdr) &lt;- c(&#39;cor_fdr&#39;) results$cor_fdr &lt;- cor_fdr t.test_fdr &lt;- c(p.adjust(results$t.test_p.val, method = &quot;fdr&quot;)) # colnames(t.test_fdr) &lt;- c(&#39;t.test_fdr&#39;) results$t.test_fdr &lt;- t.test_fdr return(results) } mets_stats &lt;- metabolite_stats(mets) # better names Metabolites &lt;- c(&quot;barettin&quot;, &quot;8,9-dihydrobarettin&quot;, &quot;8,9−dihydro−8−hydroxy−barettin&quot;, &quot;Geobarrettin A&quot;, &quot;Geobarrettin A isomer&quot;, &quot;Geobarrettin B&quot;, &quot;cyclo Pro-Arg&quot;, &quot;6-bromo-conicamin&quot;, &quot;6-bromo-8-hydroxy-conicamin&quot;, &quot;Geobarrettin C&quot;, &quot;L-6-bromohypaphorine&quot;, &quot;Barrettide A&quot;, &quot;Barrettide B&quot;) mets_stats$metabolite &lt;- Metabolites kable(mets_stats, digits = 4, longtable = T, booktabs = T, caption = &quot;Depth response for _G. barretti_ metabolites as approximated by different methods.&quot;) %&gt;% add_header_above(c(&quot;&quot;, `depth correlation` = 3, `depth groups` = 4)) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), font_size = 12, full_width = F, latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;)) Table 2.4: Depth response for G. barretti metabolites as approximated by different methods. depth correlation depth groups metabolite cor_rho cor_p.value cor_fdr t.test_stat t.test_param t.test_p.val t.test_fdr barettin -0.7714 0.0012 0.0106 18.8053 10.2069 0.0000 0.0000 8,9-dihydrobarettin -0.5607 0.0323 0.0699 3.1319 11.1151 0.0094 0.0204 8,9−dihydro−8−hydroxy−barettin -0.7571 0.0016 0.0106 5.4313 9.4148 0.0004 0.0009 Geobarrettin A -0.6542 0.0082 0.0212 6.1540 10.9726 0.0001 0.0002 Geobarrettin A isomer -0.6792 0.0054 0.0189 6.3580 10.7370 0.0001 0.0002 Geobarrettin B -0.4571 0.0888 0.1649 2.7645 9.0011 0.0219 0.0408 cyclo Pro-Arg -0.6893 0.0058 0.0189 24.6970 12.8183 0.0000 0.0000 6-bromo-conicamin -0.2143 0.4420 0.6385 2.5377 11.5902 0.0266 0.0433 6-bromo-8-hydroxy-conicamin 0.2857 0.3011 0.4893 -0.0953 10.0910 0.9260 0.9693 Geobarrettin C -0.0179 0.9540 0.9540 0.8415 8.9868 0.4219 0.5485 L-6-bromohypaphorine -0.0857 0.7630 0.9017 1.1275 10.1386 0.2855 0.4124 Barrettide A -0.1571 0.5756 0.7482 0.4319 5.4940 0.6823 0.8063 Barrettide B 0.0788 0.8380 0.9078 -0.0399 6.7499 0.9693 0.9693 cmp &lt;- read.csv(&quot;data/metabolite_master_20190605.csv&quot;, header = T, sep = &quot;,&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) md &lt;- meta_data[c(&quot;Depth&quot;, &quot;unified_ID&quot;, &quot;Species&quot;)] # Prepare df for Gb compounds metabolites &lt;- prep(cmp, md) mets &lt;- metabolites[, c(&quot;sfn&quot;, &quot;ian&quot;, &quot;unified_ID&quot;, &quot;Depth&quot;, &quot;Species&quot;)] mets &lt;- mets[mets$Species == &quot;Stryphnus fortis&quot;, ] mets[, c(&quot;unified_ID&quot;, &quot;Species&quot;)] &lt;- list(NULL) mets_stats &lt;- metabolite_stats(mets) Metabolites &lt;- c(&quot;Stryphnusin&quot;, &quot;Ianthellin&quot;) mets_stats$metabolite &lt;- Metabolites kable(mets_stats, longtable = T, booktabs = T, caption = &quot;Depth response fro _S. fortis_ metabolites as approximated by different methods.&quot;) %&gt;% add_header_above(c(&quot;&quot;, `depth correlation` = 3, `depth groups` = 4)) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), font_size = 12, full_width = F, latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;)) Table 2.5: Depth response fro S. fortis metabolites as approximated by different methods. depth correlation depth groups metabolite cor_rho cor_p.value cor_fdr t.test_stat t.test_param t.test_p.val t.test_fdr Stryphnusin 0.9393939 0 0 -2.137291 5.107436 0.0844508 0.0844508 Ianthellin 0.9393939 0 0 -2.414288 5.000016 0.0605425 0.0844508 sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS 10.15.5 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggExtra_0.9 plotly_4.9.2 usdm_1.1-18 raster_3.0-12 sp_1.4-1 rnaturalearthdata_0.1.0 rnaturalearth_0.1.0 ## [8] sf_0.8-1 marmap_1.0.3 mapdata_2.3.0 maps_3.3.0 ggmap_3.0.0.901 igraph_1.2.5 ropls_1.14.1 ## [15] reshape_0.8.8 ggrepel_0.8.2 metap_1.3 gtools_3.8.2 seqinr_3.6-1 picante_1.8.1 nlme_3.1-145 ## [22] ape_5.3 phyloseq_1.28.0 vegan_2.5-6 lattice_0.20-41 permute_0.9-5 DT_0.13 gridExtra_2.3 ## [29] kableExtra_1.1.0.9000 RColorBrewer_1.1-2 reshape2_1.4.3 forcats_0.5.0 dplyr_0.8.5 purrr_0.3.3 readr_1.3.1 ## [36] tidyr_1.0.2 tibble_3.0.0 ggplot2_3.3.0 tidyverse_1.3.0 knitr_1.28 stringr_1.4.0 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.1.5 sn_1.6-1 plyr_1.8.6 lazyeval_0.2.2 splines_3.5.1 crosstalk_1.1.0.1 TH.data_1.0-10 ## [9] digest_0.6.25 foreach_1.5.0 htmltools_0.4.0 fansi_0.4.1 memoise_1.1.0 magrittr_1.5 cluster_2.1.0 Biostrings_2.50.2 ## [17] modelr_0.1.6 sandwich_2.5-1 jpeg_0.1-8.1 colorspace_1.4-1 blob_1.2.1 rvest_0.3.5 haven_2.2.0 xfun_0.13.1 ## [25] crayon_1.3.4 jsonlite_1.6.1 survival_3.1-11 zoo_1.8-7 iterators_1.0.12 glue_1.3.2 gtable_0.3.0 zlibbioc_1.28.0 ## [33] XVector_0.22.0 webshot_0.5.2 Rhdf5lib_1.4.3 shape_1.4.4 BiocGenerics_0.28.0 scales_1.1.0 mvtnorm_1.1-0 DBI_1.1.0 ## [41] bibtex_0.4.2.2 miniUI_0.1.1.1 Rcpp_1.0.4 isoband_0.2.0 plotrix_3.7-7 xtable_1.8-4 viridisLite_0.3.0 units_0.6-6 ## [49] bit_1.1-15.2 stats4_3.5.1 htmlwidgets_1.5.1 httr_1.4.1 TFisher_0.2.0 ellipsis_0.3.0 pkgconfig_2.0.3 farver_2.0.3 ## [57] dbplyr_1.4.2 later_1.0.0 tidyselect_1.0.0 labeling_0.3 rlang_0.4.5 munsell_0.5.0 cellranger_1.1.0 tools_3.5.1 ## [65] cli_2.0.2 RSQLite_2.2.0 generics_0.0.2 ade4_1.7-15 broom_0.5.5 fastmap_1.0.1 evaluate_0.14 biomformat_1.10.1 ## [73] yaml_2.2.1 bit64_0.9-7 fs_1.4.0 ncdf4_1.17 RgoogleMaps_1.4.5.3 mime_0.9 formatR_1.7 xml2_1.3.0 ## [81] compiler_3.5.1 rstudioapi_0.11 png_0.1-7 e1071_1.7-3 reprex_0.3.0 stringi_1.4.6 highr_0.8 rgeos_0.5-2 ## [89] Matrix_1.2-18 classInt_0.4-2 multtest_2.38.0 vctrs_0.2.4 mutoss_0.1-12 pillar_1.4.3 lifecycle_0.2.0 Rdpack_0.11-1 ## [97] data.table_1.12.8 bitops_1.0-6 gbRd_0.4-11 httpuv_1.5.2 R6_2.4.1 promises_1.1.0 bookdown_0.18 KernSmooth_2.23-16 ## [105] IRanges_2.16.0 codetools_0.2-16 MASS_7.3-51.5 assertthat_0.2.1 rhdf5_2.26.2 rjson_0.2.20 withr_2.1.2 mnormt_1.5-6 ## [113] multcomp_1.4-12 S4Vectors_0.20.1 adehabitatMA_0.3.14 mgcv_1.8-31 parallel_3.5.1 hms_0.5.3 grid_3.5.1 class_7.3-16 ## [121] rmarkdown_2.1 shiny_1.4.0.2 numDeriv_2016.8-1.1 Biobase_2.42.0 lubridate_1.7.4 "],
["microbiota.html", "3 Microbiota 3.1 OTU table overview 3.2 Alpha diversity 3.3 Beta diversity 3.4 Environmental modelling 3.5 Relative abundances 3.6 Depth response: OTU perspective 3.7 Sequence similarity 3.8 Annotated phylogeny", " 3 Microbiota 3.1 OTU table overview Let’s start by taking a look at the OTU table and get an overall impression of our data. library(tidyverse) library(reshape2) library(stringr) library(ggplot2) library(RColorBrewer) library(forcats) library(kableExtra) options(kableExtra.html.bsTable = T) library(gridExtra) library(DT) library(vegan) library(phyloseq) library(picante) library(seqinr) library(gtools) # install.packages(&#39;webshot&#39;) webshot::install_phantomjs() set.seed(1984) microbiome &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) # meta_data &lt;- meta_data[!str_sub(meta_data$unified_ID,1,2)==&#39;QC&#39;,] # remove QC samples meta_data &lt;- meta_data[meta_data$unified_ID %in% microbiome$Sample_ID, ] microbiome &lt;- microbiome[order(microbiome$Sample_ID), ] meta_data &lt;- meta_data[order(meta_data$unified_ID), ] # dropping factors from full data: meta_data[] &lt;- lapply(meta_data, function(x) if (is.factor(x)) factor(x) else x) microbiome[] &lt;- lapply(microbiome, function(x) if (is.factor(x)) factor(x) else x) # all(meta_data$unified_ID==microbiome$Sample_ID) rownames(microbiome) &lt;- microbiome[, 1] microbiome[, 1] &lt;- NULL microbiome[&quot;total_OTUs&quot;] &lt;- apply(microbiome, 1, sum) #total_OTUs = Cummulative read count micro_fig1 &lt;- data.frame(microbiome[, &quot;total_OTUs&quot;]) micro_fig1[&quot;unified_ID&quot;] &lt;- rownames(microbiome) micro_fig1[&quot;normalisation&quot;] &lt;- &quot;none&quot; microbiome$total_OTUs &lt;- NULL microbiome &lt;- sqrt(microbiome) microbiome[&quot;total_OTUs&quot;] &lt;- apply(microbiome, 1, sum) micro_fig2 &lt;- data.frame(microbiome[, &quot;total_OTUs&quot;]) micro_fig2[&quot;unified_ID&quot;] &lt;- rownames(microbiome) micro_fig2[&quot;normalisation&quot;] &lt;- &quot;sqrt&quot; microbiome$total_OTUs &lt;- NULL microbiome &lt;- wisconsin(microbiome) microbiome[&quot;total_OTUs&quot;] &lt;- apply(microbiome, 1, sum) micro_fig3 &lt;- data.frame(microbiome[, &quot;total_OTUs&quot;]) micro_fig3[&quot;unified_ID&quot;] &lt;- rownames(microbiome) micro_fig3[&quot;normalisation&quot;] &lt;- &quot;sqrt wisconsin&quot; micro_fig &lt;- rbind(micro_fig1, micro_fig2, micro_fig3) colnames(micro_fig) &lt;- c(&quot;total_OTUs&quot;, &quot;unified_ID&quot;, &quot;normalisation&quot;) micro_fig[&quot;Species&quot;] &lt;- str_sub(micro_fig$unified_ID, 1, 2) ggplot(micro_fig, aes(x = unified_ID, y = total_OTUs)) + geom_bar(stat = &quot;identity&quot;) + facet_grid(vars(normalisation), vars(Species), scales = &quot;free&quot;) + xlab(&quot;Samples&quot;) + ylab(&quot;Number of OTUs&quot;) + ggtitle(&quot;Cummulative read count after normalisation&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) Figure 3.1: Read count overview of the OTU table before and after the normalisation applied in the data analysis for this study. # see https://chrischizinski.github.io/SNR_R_Group/2016-08-10-Data-Transformations Do the normalisations have affect how well we can discriminate between the microbiota? microbiome &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- meta_data[meta_data$unified_ID %in% microbiome$Sample_ID, ] rownames(microbiome) &lt;- microbiome[, 1] microbiome[, 1] &lt;- NULL pca_plot &lt;- function(microbiome, meta_data, my_title) { micro.pca &lt;- prcomp(microbiome, scale = T) k &lt;- summary(micro.pca)[[&quot;importance&quot;]] micro_pca_df &lt;- data.frame(micro.pca$x) #scores, i.e. principal components of the sponge sample micro_pca_df[&quot;unified_ID&quot;] &lt;- as.factor(rownames(micro_pca_df)) x1 &lt;- paste(&quot;PC1&quot;, round(k[2, 1], digits = 3) * 100, &quot;%&quot;) y1 &lt;- paste(&quot;PC2&quot;, round(k[2, 2], digits = 3) * 100, &quot;%&quot;) micro_pca_df &lt;- left_join(micro_pca_df[, c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC3&quot;, &quot;unified_ID&quot;)], meta_data[, c(&quot;Species&quot;, &quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;MeanBottomTemp_Cdeg&quot;, &quot;MeanBotSalinity_PSU&quot;, &quot;unified_ID&quot;)]) p &lt;- ggplot(micro_pca_df, aes(x = PC1, y = PC2)) + geom_point(size = 3, mapping = aes(shape = factor(Species))) + ggtitle(my_title) + xlab(x1) + ylab(y1) + labs(shape = &quot;Species&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) return(p) } untransformed &lt;- pca_plot(microbiome, meta_data, &quot;untransformed&quot;) sqrt_transformed &lt;- pca_plot(sqrt(microbiome), meta_data, &quot;sqrt&quot;) sqrt_wisc_transformed &lt;- pca_plot(wisconsin(sqrt(microbiome)), meta_data, &quot;sqrt and wisconsin&quot;) grid.arrange(untransformed, sqrt_transformed, sqrt_wisc_transformed, nrow = 1, top = &quot;PCA: Impact of normalisation on discrimination of the microbiota&quot;) Figure 3.2: PCA of the data sets with and without transformation/normalisation. Circles are Gb, triangles are Sf, squares are Wb microbiome &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- meta_data[meta_data$unified_ID %in% microbiome$Sample_ID, ] rownames(microbiome) &lt;- microbiome[, 1] microbiome[, 1] &lt;- NULL nmds_plot &lt;- function(microbiome, meta_data, my_title) { micro.mds &lt;- metaMDS(microbiome, k = 2, trymax = 100, distance = &quot;bray&quot;, trace = FALSE) nmds_points &lt;- as.data.frame(micro.mds$points) samples &lt;- data.frame(nmds_points$MDS1, nmds_points$MDS2) samples[&quot;unified_ID&quot;] &lt;- rownames(microbiome) meta_data &lt;- meta_data[, c(&quot;unified_ID&quot;, &quot;Depth&quot;, &quot;Species&quot;)] samples &lt;- left_join(samples, meta_data) stress &lt;- paste(&quot;Stress=&quot;, round(micro.mds$stress, digits = 3)) p &lt;- ggplot(samples, aes(x = nmds_points.MDS1, y = nmds_points.MDS2)) + geom_point(aes(shape = Species, alpha = 0.5), size = 4) + ggtitle(my_title) + labs(shape = &quot;Sponge species&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) + xlab(&quot;NMDS 1&quot;) + ylab(&quot;NMDS 2&quot;) + annotate(&quot;text&quot;, x = 0, y = 1, label = stress) return(p) } untransformed &lt;- nmds_plot(microbiome, meta_data, &quot;untransformed&quot;) sqrt_transformed &lt;- nmds_plot(sqrt(microbiome), meta_data, &quot;sqrt&quot;) sqrt_wisc_transformed &lt;- nmds_plot(wisconsin(sqrt(microbiome)), meta_data, &quot;sqrt and wisconsin&quot;) grid.arrange(untransformed, sqrt_transformed, sqrt_wisc_transformed, nrow = 1, top = &quot;NMDS: Impact of normalisation on discrimination of the microbiota&quot;) Figure 3.3: NMDS of the data sets with and without transformation/normalisation. Circles are Gb, triangles are Sf, squares are Wb It seems that we don’t lose any relevant information and the normalisations in fact increase the discriminatory power. 3.2 Alpha diversity The microbiota are composed of 420 OTUs in G. barretti, 461 OTUs in S. fortis and 135 OTUs in W. bursa. While G. barretti and S. fortis share 316, respectively they each only share 2 and 8 OTUs with W. bursa. Only a single OTU (OTU4 or X1969004, an Archaeon) is shared among the three sponge hosts. How does that reflect in their diversities? Below we show Shannon diversity, species richness (SR) and Faith’s phylogenetic distance (PD). library(vegan) library(reshape2) library(phyloseq) library(picante) microbiome &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) taxonomy &lt;- read.csv(&quot;data/microbiome_taxonomy.csv&quot;, header = T, sep = &quot;;&quot;) tree &lt;- read.nexus(&quot;data/infile.nex.con.X.tre&quot;) rownames(microbiome) &lt;- microbiome$Sample_ID microbiome$Sample_ID &lt;- NULL otumat &lt;- t(microbiome) otumat &lt;- wisconsin(sqrt(otumat)) colnames(otumat) &lt;- rownames(microbiome) rownames(taxonomy) &lt;- paste0(&quot;X&quot;, taxonomy$OTU_ID) taxonomy$OTU_ID &lt;- NULL # all(rownames(taxonomy)==rownames(otumat)) # relative abundance otumat &lt;- apply(otumat, 2, function(i) i/sum(i)) OTU &lt;- otu_table(otumat, taxa_are_rows = TRUE) # remove low abundance taxa OTU &lt;- filter_taxa(OTU, function(x) mean(x) &gt; 0.0025, TRUE) taxmat &lt;- as.matrix(taxonomy) TAX &lt;- tax_table(taxmat) biom_data &lt;- phyloseq(OTU, TAX) # Merge into phyloseq object pso &lt;- merge_phyloseq(biom_data, tree) #merging a phyloseq and a tree file pso &lt;- prune_taxa(taxa_sums(pso) &gt; 0, pso) # Calculate Phylogenetic Distance (PD) of the dataset, ALPHA DIVERSITY otu_table_pso &lt;- as.data.frame(pso@otu_table) df.pd &lt;- pd(t(otu_table_pso), tree, include.root = F) df.pd[&quot;unified_ID&quot;] &lt;- rownames(df.pd) # df.pd: PD = Faith&#39;s Phylogenetic diversity, SR= species richness div &lt;- as.data.frame(vegan::diversity(t(otumat), index = &quot;shannon&quot;)) div[&quot;spec&quot;] &lt;- str_sub(rownames(div), 1, 2) colnames(div) &lt;- c(&quot;Shannon_diversity&quot;, &quot;spec&quot;) div[&quot;unified_ID&quot;] &lt;- rownames(div) div_indices &lt;- full_join(div, df.pd) md &lt;- meta_data[, c(&quot;unified_ID&quot;, &quot;Depth&quot;)] div_indices &lt;- left_join(div_indices, md) div_indices &lt;- reshape2::melt(div_indices, id.vars = c(&quot;spec&quot;, &quot;unified_ID&quot;, &quot;Depth&quot;)) ggplot(div_indices, aes(x = Depth, y = value)) + geom_point() + facet_grid(vars(variable), vars(spec), scales = &quot;free&quot;) + theme(axis.text.x = element_text(angle = -90, vjust = 0.5, hjust = 1), legend.position = &quot;none&quot;) + ylab(&quot;Diversity metrics&quot;) + xlab(&quot;Sample depth&quot;) + theme_bw() Figure 3.4: Microbiota diversity indices grouped by sponge species and ordered by sample depth. We see that the HMA sponges G. barretti and S. fortis not only have more OTUs but also a higher diversity in their prokaryotic communities then the LMA sponge W. bursa. 3.3 Beta diversity In a way, the Fig. 3.2 and 3.3 have already shown us the beta diversity in our samples. Looking more into the data, we found that the two dimensional representation can be misleading at times and so we provide the first three axes components for exploration below. library(plot3D) library(rgl) library(plotly) ### PCA microbiome &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- meta_data[meta_data$unified_ID %in% microbiome$Sample_ID, ] rownames(microbiome) &lt;- microbiome[, 1] microbiome[, 1] &lt;- NULL microbiome &lt;- sqrt(microbiome) micro.pca &lt;- prcomp(microbiome, scale = T) k &lt;- summary(micro.pca)[[&quot;importance&quot;]] micro_pca_df &lt;- data.frame(micro.pca$x) #scores, i.e. principal components of the sponge sample micro_pca_df[&quot;unified_ID&quot;] &lt;- as.factor(rownames(micro_pca_df)) x1 &lt;- paste(&quot;PC1&quot;, round(k[2, 1], digits = 3) * 100, &quot;%&quot;) y1 &lt;- paste(&quot;PC2&quot;, round(k[2, 2], digits = 3) * 100, &quot;%&quot;) z1 &lt;- paste(&quot;PC3&quot;, round(k[2, 3], digits = 3) * 100, &quot;%&quot;) micro_pca_df &lt;- left_join(micro_pca_df[, c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC3&quot;, &quot;unified_ID&quot;)], meta_data[, c(&quot;Species&quot;, &quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;MeanBottomTemp_Cdeg&quot;, &quot;MeanBotSalinity_PSU&quot;, &quot;unified_ID&quot;)]) ## rgl/plot3D: static 3D plot with(micro_pca_df, text3D(PC1, PC2, PC3, colvar = micro_pca_df$Depth, theta = 60, phi = 20, xlab = x1, ylab = y1, zlab =z1, main = &#39;3D microbiome ## PCA&#39;, labels = micro_pca_df$unified_ID, cex = 0.9, bty = &#39;g&#39;, ticktype = &#39;detailed&#39;, d = 2, clab = c(&#39;Depth [m]&#39;), adj = 0.5, font = 2)) ## plotly axx &lt;- list(backgroundcolor = &quot;rgb(211,211,211)&quot;, gridcolor = &quot;rgb(255,255,255)&quot;, title = x1, showbackground = TRUE) axy &lt;- list(backgroundcolor = &quot;rgb(211,211,211)&quot;, gridcolor = &quot;rgb(255,255,255)&quot;, title = y1, showbackground = TRUE) axz &lt;- list(backgroundcolor = &quot;rgb(211,211,211)&quot;, gridcolor = &quot;rgb(255,255,255)&quot;, title = z1, showbackground = TRUE) mic_i &lt;- plot_ly(micro_pca_df, x = ~micro_pca_df$PC1, y = ~micro_pca_df$PC2, z = ~micro_pca_df$PC3, symbol = ~Species, symbols = c(&quot;diamond&quot;, &quot;x&quot;, &quot;circle&quot;), color = ~micro_pca_df$Depth) %&gt;% add_markers() %&gt;% layout(scene = list(xaxis = axx, yaxis = axy, zaxis = axz)) mic_i Figure 3.5: PCA of the met # for saving locally f&lt;- basename(tempfile(&#39;PCA_microbiome_plotly&#39;, &#39;.&#39;, &#39;.html&#39;)) on.exit(unlink(f), add = TRUE) html &lt;- htmlwidgets::saveWidget(mic_i, f) rm(mic_i, f, html, k, x1, y1, z1, micro.pca, axx, axy, axz, micro_pca_df) library(plot3D) library(rgl) library(plotly) ### NMDS microbiome &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- meta_data[meta_data$unified_ID %in% microbiome$Sample_ID, ] rownames(microbiome) &lt;- microbiome[, 1] microbiome[, 1] &lt;- NULL # microbiome &lt;- sqrt(microbiome) micro.mds &lt;- metaMDS(microbiome, k = 3, trymax = 100, distance = &quot;bray&quot;, trace = FALSE) nmds_points &lt;- as.data.frame(micro.mds$points) samples &lt;- data.frame(nmds_points$MDS1, nmds_points$MDS2, nmds_points$MDS3) samples[&quot;unified_ID&quot;] &lt;- rownames(microbiome) meta_data &lt;- meta_data[, c(&quot;unified_ID&quot;, &quot;Depth&quot;, &quot;Species&quot;)] samples &lt;- left_join(samples, meta_data) colnames(samples) &lt;- c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC3&quot;, &quot;unified_ID&quot;, &quot;Depth&quot;, &quot;Species&quot;) stress &lt;- paste(&quot;Stress=&quot;, round(micro.mds$stress, digits = 6)) x1 &lt;- c(&quot;MDS1&quot;) y1 &lt;- c(&quot;MDS2&quot;) z1 &lt;- c(&quot;MDS3&quot;) # rgl/plot3D with(samples, text3D(PC1, PC2, PC3, colvar = samples$Depth, theta = 60, phi = 20, xlab = x1, ylab = y1, zlab =z1, main = &#39;3D microbiome NMDS&#39;, labels = # samples$unified_ID, cex = 0.9, bty = &#39;g&#39;, ticktype = &#39;detailed&#39;, d = 2, clab = c(&#39;Depth [m]&#39;), adj = 0.5, font = 2)) ## plotly axx &lt;- list(backgroundcolor = &quot;rgb(211,211,211)&quot;, gridcolor = &quot;rgb(255,255,255)&quot;, title = x1, showbackground = TRUE) axy &lt;- list(backgroundcolor = &quot;rgb(211,211,211)&quot;, gridcolor = &quot;rgb(255,255,255)&quot;, title = y1, showbackground = TRUE) axz &lt;- list(backgroundcolor = &quot;rgb(211,211,211)&quot;, gridcolor = &quot;rgb(255,255,255)&quot;, title = z1, showbackground = TRUE) mic_i &lt;- plot_ly(samples, x = ~samples$PC1, y = ~samples$PC2, z = ~samples$PC3, symbol = ~Species, symbols = c(&quot;diamond&quot;, &quot;x&quot;, &quot;circle&quot;), color = samples$Depth) %&gt;% add_markers() %&gt;% layout(scene = list(xaxis = axx, yaxis = axy, zaxis = axz)) mic_i Figure 3.6: NMDS based on Bray-Curtis dissimilarity. # for saving locally f&lt;- basename(tempfile(&#39;NMDS_microbiome_plotly&#39;, &#39;.&#39;, &#39;.html&#39;)) on.exit(unlink(f), add = TRUE) html &lt;- htmlwidgets::saveWidget(mic_i, f) rm(mic_i, f, html, k, x1, y1, z1, micro.mds, axx, axy, axz, samples, nmds_points) NMDS Stress= 8.6e-05. In some of the downstream analyses, we distinguish between common/abundant OTUs and rare OTUs. We use a cutoff of 0.25% average relative abundance per OTU for the classification. That implies drastically modifying the original numbers of OTUs per sponge as outlined below. micro &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) emp &lt;- read.csv(&quot;data/SpongeEMP.csv&quot;, header = T, sep = &quot;;&quot;) emp[&quot;XOTU_id&quot;] &lt;- str_replace(emp$OTU_ID, &quot;OTU&quot;, &quot;X196900&quot;) # The full data set had the entries listed by sponge host, so there are 62 duplicates in the OTU list length(emp$XOTU_id) # 207 length(unique(emp$XOTU_id)) # 145 emp[&quot;num&quot;] &lt;- as.numeric(str_replace(emp$OTU_ID, &quot;OTU&quot;, &quot;&quot;)) emp &lt;- emp[order(emp$num), ] emp[&quot;dup&quot;] &lt;- duplicated(emp$num) # dim(emp[emp$dup==&#39;TRUE&#39;,]) #62 emp &lt;- emp[emp$dup == &quot;FALSE&quot;, ] emp[, c(&quot;sponge&quot;, &quot;num&quot;, &quot;dup&quot;)] &lt;- list(NULL) OTU_prep_sqrt &lt;- function(micro) { rownames(micro) &lt;- micro$Sample_ID micro$Sample_ID &lt;- NULL # micro &lt;- sqrt(micro) micro_gb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Gb&quot;), ] micro_sf &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Sf&quot;), ] micro_wb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Wb&quot;), ] micro_gb &lt;- micro_gb[, colSums(micro_gb != 0) &gt; 0] #removes columns that only contain 0 micro_sf &lt;- micro_sf[, colSums(micro_sf != 0) &gt; 0] micro_wb &lt;- micro_wb[, colSums(micro_wb != 0) &gt; 0] micros &lt;- list(gb = micro_gb, sf = micro_sf, wb = micro_wb) return(micros) } micro_ds &lt;- OTU_prep_sqrt(micro) overall_rabdc &lt;- function(micro) { mic &lt;- micro n &lt;- 0 k &lt;- dim(mic)[1] mic[&quot;rowsum&quot;] &lt;- apply(mic, 1, sum) while (n &lt; k) { n &lt;- n + 1 mic[n, ] &lt;- mic[n, ]/(mic$rowsum[n]) } mic$rowsum &lt;- NULL mic &lt;- data.frame(t(mic)) mic[&quot;avg_rel_abdc&quot;] &lt;- apply(mic, 1, mean) mic[&quot;occurrence&quot;] &lt;- ifelse(mic$avg &gt; 0.0025, &quot;common&quot;, &quot;rare&quot;) return(mic) } gb_occurrence &lt;- overall_rabdc(micro_ds$gb) sf_occurrence &lt;- overall_rabdc(micro_ds$sf) wb_occurrence &lt;- overall_rabdc(micro_ds$wb) gb_occurrence &lt;- gb_occurrence[, c(&quot;avg_rel_abdc&quot;, &quot;occurrence&quot;)] gb_occurrence[&quot;XOTU_id&quot;] &lt;- rownames(gb_occurrence) gb_occ_emp &lt;- left_join(gb_occurrence, emp) sf_occurrence &lt;- sf_occurrence[, c(&quot;avg_rel_abdc&quot;, &quot;occurrence&quot;)] sf_occurrence[&quot;XOTU_id&quot;] &lt;- rownames(sf_occurrence) sf_occ_emp &lt;- left_join(sf_occurrence, emp) wb_occurrence &lt;- wb_occurrence[, c(&quot;avg_rel_abdc&quot;, &quot;occurrence&quot;)] wb_occurrence[&quot;XOTU_id&quot;] &lt;- rownames(wb_occurrence) wb_occ_emp &lt;- left_join(wb_occurrence, emp) gb_aggr &lt;- aggregate(gb_occ_emp$avg_rel_abdc, by = list(gb_occ_emp$occurrence), FUN = &quot;length&quot;) sf_aggr &lt;- aggregate(sf_occ_emp$avg_rel_abdc, by = list(sf_occ_emp$occurrence), FUN = &quot;length&quot;) wb_aggr &lt;- aggregate(wb_occ_emp$avg_rel_abdc, by = list(wb_occ_emp$occurrence), FUN = &quot;length&quot;) aggr &lt;- cbind(gb_aggr, sf_aggr$x, wb_aggr$x) colnames(aggr) &lt;- c(&quot;OTU classification&quot;, &quot;count Gb&quot;, &quot;count Sf&quot;, &quot;count Wb&quot;) options(kableExtra.html.bsTable = T) kable(aggr, col.names = c(&quot;OTU classification&quot;, &quot;count (Gb)&quot;, &quot;count (Sf)&quot;, &quot;count (Wb)&quot;), booktabs = T, caption = &quot;Number of OTUs being excluded and retained in the three sponges&#39; microbiota when filtering for average relative abundance &gt; 0.25%.&quot;, row.names = FALSE) %&gt;% kable_styling(bootstrap_options = c(&quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), full_width = F, latex_options = c(&quot;scale_down&quot;)) Table 3.1: Number of OTUs being excluded and retained in the three sponges’ microbiota when filtering for average relative abundance &gt; 0.25%. OTU classification count (Gb) count (Sf) count (Wb) common 96 89 20 rare 324 372 115 That means, for G. barretti we exclude 77.1 % of OTUs, for S. fortis 80.7 % and for W. bursa 85.2%. gb_common &lt;- gb_occ_emp[gb_occ_emp$occurrence == &quot;common&quot;, ] sf_common &lt;- sf_occ_emp[sf_occ_emp$occurrence == &quot;common&quot;, ] wb_common &lt;- wb_occ_emp[wb_occ_emp$occurrence == &quot;common&quot;, ] gb_aggr &lt;- aggregate(gb_common$avg_rel_abdc, by = list(gb_common$spongeEMP_enriched), FUN = &quot;length&quot;) sf_aggr &lt;- aggregate(sf_common$avg_rel_abdc, by = list(sf_common$spongeEMP_enriched), FUN = &quot;length&quot;) wb_aggr &lt;- aggregate(wb_common$avg_rel_abdc, by = list(wb_common$spongeEMP_enriched), FUN = &quot;length&quot;) aggr &lt;- cbind(gb_aggr, sf_aggr$x) aggr &lt;- left_join(aggr, wb_aggr, by = &quot;Group.1&quot;) colnames(aggr) &lt;- c(&quot;EMP OTU count&quot;, &quot;Gb&quot;, &quot;Sf&quot;, &quot;Wb&quot;) options(kableExtra.html.bsTable = T) kable(aggr, col.names = c(&quot;OTU classification&quot;, &quot;count (Gb)&quot;, &quot;count (Sf)&quot;, &quot;count (Wb)&quot;), booktabs = T, caption = &quot;Number of common/abundant OTUs found in the SpongeEMP data base&quot;, row.names = FALSE) %&gt;% kable_styling(bootstrap_options = c(&quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), full_width = F, latex_options = c(&quot;scale_down&quot;)) Table 3.2: Number of common/abundant OTUs found in the SpongeEMP data base OTU classification count (Gb) count (Sf) count (Wb) no 23 25 9 yes 72 64 11 3.4 Environmental modelling In order to investigate whether and which of the environmental parameter might “explain”/correlate with variation the the microbiota, we apply constrained and unconstrained ecological data analysis methods. 3.4.1 Constrained modelling approach Automatic stepwise model building We use canonical correspondence analysis as method for ordination. According to the manual for the R package vegan, “a good dissimilarity index for multidimensional scaling should have a high rank-order similarity with gradient separation” (Oksanen et al. 2019). Thus interpreting the results of Tab 3.3, we find that square root transformation and Wisconsin standardisation increase the rank correlation between the microbial community dissimilarity matrix and the environmental gradient separation in all three sponge microbiota for a number of ecological dissimilarity indices. library(vegan) microbiome &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- meta_data[meta_data$unified_ID %in% microbiome$Sample_ID, ] # Gb12 has no salinity and temperature: impute from Gb11 &amp; Gb13: Salinity:34.92; Temp:3.59 row &lt;- which(meta_data$unified_ID == &quot;Gb12&quot;) temp &lt;- which(colnames(meta_data) == &quot;MeanBottomTemp_Cdeg&quot;) sal &lt;- which(colnames(meta_data) == &quot;MeanBotSalinity_PSU&quot;) meta_data[row, temp] &lt;- 3.59 meta_data[row, sal] &lt;- 34.92 rm(row, temp, sal) OTU_prep_sqrt &lt;- function(micro) { rownames(micro) &lt;- micro$Sample_ID micro$Sample_ID &lt;- NULL micro &lt;- sqrt(micro) micro_gb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Gb&quot;), ] micro_sf &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Sf&quot;), ] micro_wb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Wb&quot;), ] micro_gb &lt;- micro_gb[, colSums(micro_gb != 0) &gt; 0] micro_sf &lt;- micro_sf[, colSums(micro_sf != 0) &gt; 0] micro_wb &lt;- micro_wb[, colSums(micro_wb != 0) &gt; 0] micros &lt;- list(gb = micro_gb, sf = micro_sf, wb = micro_wb) return(micros) } microbiomes &lt;- OTU_prep_sqrt(microbiome) md_prep &lt;- function(microbiomes, meta_data) { meta_data &lt;- meta_data[, c(&quot;unified_ID&quot;, &quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;MeanBottomTemp_Cdeg&quot;, &quot;MeanBotSalinity_PSU&quot;, &quot;YEAR&quot;)] gb &lt;- microbiomes$gb sf &lt;- microbiomes$sf wb &lt;- microbiomes$wb gb_md &lt;- meta_data[meta_data$unified_ID %in% rownames(gb), ] rownames(gb_md) &lt;- gb_md$unified_ID gb_md &lt;- gb_md[order(gb_md$unified_ID), ] all(rownames(microbiomes$gb) == rownames(gb_md)) gb_md &lt;- gb_md[, c(&quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;MeanBottomTemp_Cdeg&quot;, &quot;MeanBotSalinity_PSU&quot;, &quot;YEAR&quot;)] sf_md &lt;- meta_data[meta_data$unified_ID %in% rownames(sf), ] rownames(sf_md) &lt;- sf_md$unified_ID sf_md &lt;- sf_md[order(sf_md$unified_ID), ] sf_md$unified_ID &lt;- NULL wb_md &lt;- meta_data[meta_data$unified_ID %in% rownames(wb), ] rownames(wb_md) &lt;- wb_md$unified_ID wb_md &lt;- wb_md[order(wb_md$unified_ID), ] wb_md$unified_ID &lt;- NULL mds &lt;- list(gb_md = gb_md, sf_md = sf_md, wb_md = wb_md) return(mds) } mds &lt;- md_prep(microbiomes, meta_data) # Standardization If there is a large difference between smallest non-zero abundance and largest abundance, we want to reduce this difference. Usually square root transformation # is sufficient to balance the data. Wisconsin double standardization often improves the gradient detection ability of dissimilarity indices. # Which dissimilarity index is best? gb_ri1 &lt;- rankindex(scale(mds$gb_md), (microbiomes$gb)^2, c(&quot;euc&quot;, &quot;man&quot;, &quot;bray&quot;, &quot;jac&quot;, &quot;kul&quot;)) #unstandardized gb_ri2 &lt;- rankindex(scale(mds$gb_md), microbiomes$gb, c(&quot;euc&quot;, &quot;man&quot;, &quot;bray&quot;, &quot;jac&quot;, &quot;kul&quot;)) #sqrt gb_ri3 &lt;- rankindex(scale(mds$gb_md), wisconsin(microbiomes$gb), c(&quot;euc&quot;, &quot;man&quot;, &quot;bray&quot;, &quot;jac&quot;, &quot;kul&quot;)) #sqrt and wisconsin sf_ri1 &lt;- rankindex(scale(mds$sf_md), (microbiomes$sf)^2, c(&quot;euc&quot;, &quot;man&quot;, &quot;bray&quot;, &quot;jac&quot;, &quot;kul&quot;)) sf_ri2 &lt;- rankindex(scale(mds$sf_md), microbiomes$sf, c(&quot;euc&quot;, &quot;man&quot;, &quot;bray&quot;, &quot;jac&quot;, &quot;kul&quot;)) sf_ri3 &lt;- rankindex(scale(mds$sf_md), wisconsin(microbiomes$sf), c(&quot;euc&quot;, &quot;man&quot;, &quot;bray&quot;, &quot;jac&quot;, &quot;kul&quot;)) wb_ri1 &lt;- rankindex(scale(mds$wb_md), (microbiomes$wb)^2, c(&quot;euc&quot;, &quot;man&quot;, &quot;bray&quot;, &quot;jac&quot;, &quot;kul&quot;)) wb_ri2 &lt;- rankindex(scale(mds$wb_md), microbiomes$wb, c(&quot;euc&quot;, &quot;man&quot;, &quot;bray&quot;, &quot;jac&quot;, &quot;kul&quot;)) wb_ri3 &lt;- rankindex(scale(mds$wb_md), wisconsin(microbiomes$wb), c(&quot;euc&quot;, &quot;man&quot;, &quot;bray&quot;, &quot;jac&quot;, &quot;kul&quot;)) rankindices &lt;- rbind(gb_ri1, gb_ri2, gb_ri3, sf_ri1, sf_ri2, sf_ri3, wb_ri1, wb_ri2, wb_ri3) rankindices &lt;- as.data.frame(rankindices) rankindices[&quot;Sponge species&quot;] &lt;- c(rep(&quot;G. barretti&quot;, 3), rep(&quot;S. fortis&quot;, 3), rep(&quot;W. bursa&quot;, 3)) rankindices[&quot;Normalisation&quot;] &lt;- c(rep(c(&quot;none&quot;, &quot;sqrt&quot;, &quot;sqrt &amp; wisconsin&quot;), 3)) options(kableExtra.html.bsTable = T) kable(rankindices, col.names = c(&quot;Euclidean&quot;, &quot;Manhattan&quot;, &quot;Bray–Curtis&quot;, &quot;Jaccard&quot;, &quot;Kulczynski&quot;, &quot;Sponge species&quot;, &quot;Normalisation&quot;), booktabs = T, caption = &quot;Rank correlation between dissimilarity indices and gradient separation. The higher the number the stronger the correlation, i.e. the better the fit.&quot;, row.names = FALSE) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), full_width = F, latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;)) Table 3.3: Rank correlation between dissimilarity indices and gradient separation. The higher the number the stronger the correlation, i.e. the better the fit. Euclidean Manhattan Bray–Curtis Jaccard Kulczynski Sponge species Normalisation -0.0512024 -0.0445135 0.3979615 0.3979615 0.4509158 G. barretti none 0.0930562 0.1658067 0.3967033 0.3967033 0.4010193 G. barretti sqrt 0.4072305 0.4026915 0.4026915 0.4026915 0.4026915 G. barretti sqrt &amp; wisconsin 0.0816401 0.1826975 0.3111549 0.3111549 0.4108128 S. fortis none 0.3285092 0.3476778 0.4620775 0.4620775 0.4877255 S. fortis sqrt 0.5056811 0.5471905 0.5471905 0.5471905 0.5471905 S. fortis sqrt &amp; wisconsin 0.0280922 0.0994375 0.0008681 0.0008681 0.0048476 W. bursa none 0.0376971 0.4854434 0.2801306 0.2801306 0.2656573 W. bursa sqrt -0.0833252 0.3617890 0.3617890 0.3617890 0.3617890 W. bursa sqrt &amp; wisconsin We adapt the microbial data sets accordingly applying square root transformation and Wisconsin standardisation of the microbiota in all subsequent ecological analyses. Then, we build a model (null model) without any environmental parameters, and one (full model) with the maximum number of environmental parameters (terms) possible so that none of them have a VIF &gt; 10. Finally, we use the stepwise model building function ordistep to determine, which of the environmental parameters is a significant constraint for the microbiomes. The function compares the null model and adds and removes terms from the full model to find (combinations of) significant constraints. The significance of the parameters is then tested in an ANOVA. G. barretti: first the VIFs of all terms included in the full model, second the result of the stepwise model building, third the ANOVA of the suggested model. # mod0 has no terms, intercept only mod1 includes all terms possible with a VIF &lt; 10. Available: &#39;Depth&#39;, &#39;Latitude&#39;, &#39;Longitude&#39;, &#39;MeanBottomTemp_Cdeg&#39;, &#39;MeanBotSalinity_PSU&#39;, # &#39;YEAR&#39; ## Gb mod0 &lt;- cca(wisconsin(microbiomes$gb) ~ 1, mds$gb_md) mod1 &lt;- cca(wisconsin(microbiomes$gb) ~ Depth + Latitude + MeanBottomTemp_Cdeg + YEAR, mds$gb_md) vif.cca(mod1) ## Depth Latitude MeanBottomTemp_Cdeg YEAR ## 1.230382 1.775034 1.929029 1.651565 mod &lt;- ordistep(mod0, scope = formula(mod1), trace = F) #0 mod$anova ## Df AIC F Pr(&gt;F) ## + Depth 1 6.7097 3.4066 0.005 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(mod) ## Permutation test for cca under reduced model ## Permutation: free ## Number of permutations: 999 ## ## Model: cca(formula = wisconsin(microbiomes$gb) ~ Depth, data = mds$gb_md) ## Df ChiSquare F Pr(&gt;F) ## Model 1 0.34451 3.4066 0.001 *** ## Residual 12 1.21355 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 S. fortis: first the VIFs of all terms included in the full model, second the result of the stepwise model building, third the ANOVA of the suggested model. ## Sf mod0 &lt;- cca(wisconsin(microbiomes$sf) ~ 1, mds$sf_md) mod1 &lt;- cca(wisconsin(microbiomes$sf) ~ Depth + Latitude + YEAR + MeanBotSalinity_PSU + Longitude, mds$sf_md) vif.cca(mod1) ## Depth Latitude YEAR MeanBotSalinity_PSU Longitude ## 4.218950 5.079261 1.082453 3.648340 5.071690 mod &lt;- ordistep(mod0, scope = formula(mod1), trace = F) mod$anova ## Df AIC F Pr(&gt;F) ## + Depth 1 13.547 2.1462 0.005 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(mod) ## Permutation test for cca under reduced model ## Permutation: free ## Number of permutations: 999 ## ## Model: cca(formula = wisconsin(microbiomes$sf) ~ Depth, data = mds$sf_md) ## Df ChiSquare F Pr(&gt;F) ## Model 1 0.31199 2.1462 0.001 *** ## Residual 13 1.88978 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 W. bursa: first the VIFs of all terms included in the full model, second the result of the stepwise model building, third the ANOVA of the suggested model. ## Wb mod0 &lt;- cca(wisconsin(microbiomes$wb) ~ 1, mds$wb_md) mod1 &lt;- cca(wisconsin(microbiomes$wb) ~ Depth + Latitude + MeanBottomTemp_Cdeg + YEAR, mds$wb_md) vif.cca(mod1) ## Depth Latitude MeanBottomTemp_Cdeg YEAR ## 2.216302 1.217879 2.402357 1.153565 mod &lt;- ordistep(mod0, scope = formula(mod1), trace = F) mod$anova ## Df AIC F Pr(&gt;F) ## + Depth 1 25.701 1.6576 0.005 ** ## + Latitude 1 25.993 1.4649 0.030 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(mod) ## Permutation test for cca under reduced model ## Permutation: free ## Number of permutations: 999 ## ## Model: cca(formula = wisconsin(microbiomes$wb) ~ Depth + Latitude, data = mds$wb_md) ## Df ChiSquare F Pr(&gt;F) ## Model 2 0.8527 1.5888 0.001 *** ## Residual 13 3.4888 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For all G. barretti and S. fortis depth is a significant constraint of the microbial community. For W. bursa we find temperature to be a significant constraint. 3.4.2 Unconstrained modelling approach In an alternative approach, we fit environmental vectors onto an ordination of the microbiota. This method allows to include all environmental parameters (regardless of collinearity). Length of the arrow indicates strength of the predictor (environmental parameter). set.seed(1984) microbiome &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- meta_data[meta_data$unified_ID %in% microbiome$Sample_ID, ] row &lt;- which(meta_data$unified_ID == &quot;Gb12&quot;) temp &lt;- which(colnames(meta_data) == &quot;MeanBottomTemp_Cdeg&quot;) sal &lt;- which(colnames(meta_data) == &quot;MeanBotSalinity_PSU&quot;) meta_data[row, temp] &lt;- 3.59 meta_data[row, sal] &lt;- 34.92 rm(row, temp, sal) microbiomes &lt;- OTU_prep_sqrt(microbiome) mds &lt;- md_prep(microbiomes, meta_data) colnames(mds$gb_md) &lt;- c(&quot;Depth&quot;, &quot;Lat&quot;, &quot;Lon&quot;, &quot;Temp&quot;, &quot;Sal&quot;, &quot;Year&quot;) colnames(mds$sf_md) &lt;- c(&quot;Depth&quot;, &quot;Lat&quot;, &quot;Lon&quot;, &quot;Temp&quot;, &quot;Sal&quot;, &quot;Year&quot;) colnames(mds$wb_md) &lt;- c(&quot;Depth&quot;, &quot;Lat&quot;, &quot;Lon&quot;, &quot;Temp&quot;, &quot;Sal&quot;, &quot;Year&quot;) # Gb dist_micro &lt;- vegdist(wisconsin(microbiomes$gb)) #distance matrix ordi_micro &lt;- metaMDS(dist_micro, trace = F) # ordination ef &lt;- envfit(ordi_micro, mds$gb_md, permutations = 999) # fitting arrows; STRATA? ef ## ## ***VECTORS ## ## NMDS1 NMDS2 r2 Pr(&gt;r) ## Depth 0.96701 -0.25474 0.8945 0.001 *** ## Lat 0.16703 0.98595 0.1627 0.411 ## Lon 0.77996 0.62583 0.4251 0.047 * ## Temp -0.00596 -0.99998 0.2302 0.240 ## Sal 0.70615 -0.70806 0.5171 0.025 * ## Year 0.53961 0.84192 0.1444 0.418 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Permutation: free ## Number of permutations: 999 # r Goodness of fit statistic: Squared correlation coefficient I will report r^2 i.e. goodness of fit rather than correlaiton because correlation with a distance matrix is not # meaningful for understanding plot(ordi_micro, display = &quot;sites&quot;) #plot plot(ef, p.max = 0.05) #arrows Figure 3.7: Fitting significant (p&lt;0.05) environmental vectors onto ordination of G. barretti microbiome. set.seed(1984) dist_micro &lt;- vegdist(wisconsin(microbiomes$sf)) # distance matrix ordi_micro &lt;- metaMDS(dist_micro, trace = F) # ordination ef &lt;- envfit(ordi_micro, mds$sf_md, permutations = 999) # fitting arrows ef ## ## ***VECTORS ## ## NMDS1 NMDS2 r2 Pr(&gt;r) ## Depth 0.84791 0.53014 0.6976 0.002 ** ## Lat 0.13416 0.99096 0.1261 0.393 ## Lon 0.68725 0.72642 0.6101 0.007 ** ## Temp -0.09375 -0.99560 0.3200 0.107 ## Sal 0.84644 -0.53249 0.1908 0.258 ## Year 0.96551 0.26038 0.0114 0.932 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Permutation: free ## Number of permutations: 999 plot(ordi_micro, display = &quot;sites&quot;) # plot plot(ef, p.max = 0.05) # arrows Figure 3.8: Fitting significant (p&lt;0.05) environmental vectors onto ordination of S. fortis microbiome. set.seed(1984) dist_micro &lt;- vegdist(wisconsin(microbiomes$wb)) # distance matrix ordi_micro &lt;- metaMDS(dist_micro, trace = F) # ordination ef &lt;- envfit(ordi_micro, mds$wb_md, permutations = 999) # fitting arrows ef ## ## ***VECTORS ## ## NMDS1 NMDS2 r2 Pr(&gt;r) ## Depth 0.15403 0.98807 0.7720 0.001 *** ## Lat 0.73298 0.68025 0.2144 0.196 ## Lon 0.28260 0.95924 0.5937 0.006 ** ## Temp 0.14099 0.99001 0.7231 0.003 ** ## Sal 0.21497 0.97662 0.8490 0.001 *** ## Year -0.21466 -0.97669 0.0422 0.749 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Permutation: free ## Number of permutations: 999 plot(ordi_micro, display = &quot;sites&quot;) # plot plot(ef, p.max = 0.05) # arrows Figure 3.9: Fitting significant (p&lt;0.05) environmental vectors onto ordination of W. bursa microbiome. 3.4.3 Hierarchical clustering So far, we’ve treated depth as a linear variable. With these clustering method, we’re asking whether there are particular groups standing out. ## Clustering par(mfrow = c(1, 3)) dist_micro &lt;- vegdist(wisconsin(microbiomes$gb)) #distance matrix clua &lt;- hclust(dist_micro, &quot;average&quot;) #average= UPGMA plot(clua, sub = &quot;Gb&quot;, xlab = &quot;UPGMA&quot;) rect.hclust(clua, 2) grp1 &lt;- cutree(clua, 2) dist_micro &lt;- vegdist(wisconsin(microbiomes$sf)) #distance matrix clua &lt;- hclust(dist_micro, &quot;average&quot;) #average= UPGMA plot(clua, sub = &quot;Sf&quot;, xlab = &quot;UPGMA&quot;) grp2 &lt;- cutree(clua, 2) dist_micro &lt;- vegdist(wisconsin(microbiomes$wb)) #distance matrix clua &lt;- hclust(dist_micro, &quot;average&quot;) #average= UPGMA plot(clua, sub = &quot;Wb&quot;, xlab = &quot;UPGMA&quot;) Figure 3.10: Hclust grp3 &lt;- cutree(clua, 2) par(mfrow = c(1, 1)) # ord &lt;- cca(wisconsin(microbiomes$gb)) plot(ord, display = &#39;sites&#39;) ordihull(ord, grp1, lty = 2, col = &#39;red&#39;) # ord &lt;- cca(wisconsin(microbiomes$sf)) plot(ord, display = &#39;sites&#39;) ordihull(ord, grp2, lty = 2, col = &#39;red&#39;) # ord &lt;- cca(wisconsin(microbiomes$wb)) plot(ord, display = &#39;sites&#39;) ordihull(ord, grp3, lty = 2, col = &#39;red&#39;) library(pheatmap) library(RColorBrewer) library(viridis) microbiome &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- meta_data[meta_data$unified_ID %in% microbiome$Sample_ID, ] micro_ds &lt;- OTU_prep_sqrt(microbiome) mds &lt;- md_prep(microbiomes, meta_data) gb_md &lt;- mds$gb_md sf_md &lt;- mds$sf_md wb_md &lt;- mds$wb_md gb_md[&quot;depth_category&quot;] &lt;- ifelse(gb_md$Depth &lt; 1000, &quot;shallow&quot;, &quot;deep&quot;) sf_md[&quot;depth_category&quot;] &lt;- ifelse(sf_md$Depth &lt; 1000, &quot;shallow&quot;, &quot;deep&quot;) wb_md[&quot;depth_category&quot;] &lt;- ifelse(wb_md$Depth &lt; 1000, &quot;shallow&quot;, &quot;deep&quot;) k &lt;- vegdist(wisconsin(micro_ds$gb)) mat_col &lt;- data.frame(group = gb_md$depth_category) rownames(mat_col) &lt;- rownames(micro_ds$gb) col_groups &lt;- gb_md$depth_category mat_colors &lt;- list(group = c(&quot;gold&quot;, &quot;dodgerblue&quot;)) names(mat_colors$group) &lt;- unique(col_groups) pheatmap(k, color = magma(10), border_color = NA, show_colnames = FALSE, show_rownames = FALSE, annotation_col = mat_col, annotation_colors = mat_colors, drop_levels = TRUE, fontsize = 10, main = &quot;G. barretti heatmap&quot;) k &lt;- vegdist(wisconsin(micro_ds$sf)) mat_col &lt;- data.frame(group = sf_md$depth_category) rownames(mat_col) &lt;- rownames(micro_ds$sf) col_groups &lt;- sf_md$depth_category mat_colors &lt;- list(group = c(&quot;gold&quot;, &quot;dodgerblue&quot;)) names(mat_colors$group) &lt;- unique(col_groups) pheatmap(k, color = magma(10), border_color = NA, show_colnames = FALSE, show_rownames = FALSE, annotation_col = mat_col, annotation_colors = mat_colors, drop_levels = TRUE, fontsize = 10, main = &quot;S. fortis heatmap&quot;) k &lt;- vegdist(wisconsin(micro_ds$wb)) mat_col &lt;- data.frame(group = wb_md$depth_category) rownames(mat_col) &lt;- rownames(micro_ds$wb) col_groups &lt;- wb_md$depth_category mat_colors &lt;- list(group = c(&quot;gold&quot;, &quot;dodgerblue&quot;)) names(mat_colors$group) &lt;- unique(col_groups) pheatmap(k, color = magma(10), border_color = NA, show_colnames = TRUE, show_rownames = TRUE, annotation_col = mat_col, annotation_colors = mat_colors, drop_levels = TRUE, fontsize = 10, main = &quot;W. bursa heatmap&quot;) # library(dendsort) sort_hclust &lt;- function(...) as.hclust(dendsort(as.dendrogram(...))) mat_cluster_rows &lt;- sort_hclust(hclust(dist(k))) mat_cluster_cols &lt;- hclust(dist(t(k))) # mat_cluster_cols &lt;- sort_hclust(mat_cluster_cols) mat_breaks &lt;- seq(min(k), max(k), length.out = 10) # pheatmap( mat = k, color = inferno(length(mat_breaks) - 1), breaks = mat_breaks, border_color = NA, cluster_cols = mat_cluster_cols, cluster_rows = mat_cluster_rows, # show_colnames = TRUE, show_rownames = TRUE, annotation_col = mat_col, annotation_colors = mat_colors, drop_levels = TRUE, fontsize = 14, main = &#39;Sorted heatmap&#39; ) 3.4.4 Oceanographic setting: water masses Finally, we test whether there are statistic differences in the prokaryotic community compositions between above and below 1000 m depth. # ========== Testing differences of microbiome in different water masses =========== microbiome &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- meta_data[meta_data$unified_ID %in% microbiome$Sample_ID, ] # No data transformation/normalisation, as Vegantutor (Oksanen, 2013) p. 32 doesn&#39;t do it either. betadiver (x, ...); x = community data matrix OTU_prep &lt;- function(micro) { rownames(micro) &lt;- micro$Sample_ID micro$Sample_ID &lt;- NULL # micro &lt;- sqrt(micro) micro_gb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Gb&quot;), ] micro_sf &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Sf&quot;), ] micro_wb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Wb&quot;), ] micro_gb &lt;- micro_gb[, colSums(micro_gb != 0) &gt; 0] micro_sf &lt;- micro_sf[, colSums(micro_sf != 0) &gt; 0] micro_wb &lt;- micro_wb[, colSums(micro_wb != 0) &gt; 0] micros &lt;- list(gb = micro_gb, sf = micro_sf, wb = micro_wb) return(micros) } micro_ds &lt;- OTU_prep(microbiome) meta_data &lt;- meta_data_prep(meta_data) # Gb gb_md &lt;- meta_data[meta_data$unified_ID %in% rownames(micro_ds$gb), ] gb_md[&quot;depth_category&quot;] &lt;- ifelse(gb_md$Depth &lt; 1000, &quot;shallow&quot;, &quot;deep&quot;) # all(rownames(micro_ds$gb)==gb_md$unified_ID) bd &lt;- betadiver(micro_ds$gb, &quot;z&quot;) adonis(bd ~ gb_md$depth_category) ## ## Call: ## adonis(formula = bd ~ gb_md$depth_category) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## gb_md$depth_category 1 0.48654 0.48654 9.6668 0.44616 0.004 ** ## Residuals 12 0.60398 0.05033 0.55384 ## Total 13 1.09052 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Sf sf_md &lt;- meta_data[meta_data$unified_ID %in% rownames(micro_ds$sf), ] sf_md[&quot;depth_category&quot;] &lt;- ifelse(sf_md$Depth &lt; 1000, &quot;shallow&quot;, &quot;deep&quot;) # all(rownames(micro_ds$sf)==sf_md$unified_ID) bd &lt;- betadiver(micro_ds$sf, &quot;z&quot;) adonis(bd ~ sf_md$depth_category) ## ## Call: ## adonis(formula = bd ~ sf_md$depth_category) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## sf_md$depth_category 1 0.3649 0.3649 4.05 0.23753 0.001 *** ## Residuals 13 1.1713 0.0901 0.76247 ## Total 14 1.5362 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Wb wb_md &lt;- meta_data[meta_data$unified_ID %in% rownames(micro_ds$wb), ] wb_md[&quot;depth_category&quot;] &lt;- ifelse(wb_md$Depth &lt; 1000, &quot;shallow&quot;, &quot;deep&quot;) # all(rownames(micro_ds$wb)==wb_md$unified_ID) bd &lt;- betadiver(micro_ds$wb, &quot;z&quot;) adonis(bd ~ wb_md$depth_category) ## ## Call: ## adonis(formula = bd ~ wb_md$depth_category) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## wb_md$depth_category 1 0.27974 0.27974 1.37 0.08914 0.135 ## Residuals 14 2.85866 0.20419 0.91086 ## Total 15 3.13841 1.00000 To summarise, in this section on environmental modelling, we’ve shown that the prokaryotic communities in G. barretti and S. fortis seem to be influenced by depth both in contrained and unconstrained methods. For W. bursa we get mixed results. Hence at this point, the picture of a depth effect on LMA sponge prokaryotic communities remains ambiguous. Our intention with the hierarchical clustering was to see if we could group the variation in the community composition in order to see if we can take hints from that about which aspect of the (a)biotic environment most likely causes/links to the changes. For G. barretti, we see the two main clusters representing the “shallow” (Gb1-Gb10: 407-801 m) versus the “deep” (Gb11-Gb14: 1213-1427 m) specimens. These two groups match the two water masses detected in this part of the North Atlantic. The clustering also yiels a distinct clade/group of deep specimens in S. fortis (Sf9-Sf15: 1036-1476 m). Again, this highlights the distinctiveness of the prokatyotic commmunity of the deep specimens, potentially linked to the differences in the surrounding water masses. In W. bursa the specimens originating from depths greater than 1000 m (Wb13-Wb16) cluster with shallow samples and thus, we cannot deduce any stratifying effect of depth or water masses on its microbiome. This section has given us a general impression that depth affects (mainly HMA) sponge prokarytoic community compositions. But can we be more specific and identify which OTUs are behind those patterns? 3.5 Relative abundances 3.5.1 Bar plots Enough of the anonymous modelling and data fitting, let’s take a look at the taxonomy of the OTUs to see who is there. micro &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- read.csv(&quot;data/PANGAEA_Final.csv&quot;, header = T, sep = &quot;;&quot;) taxonomy &lt;- read.csv(&quot;data/microbiome_taxonomy.csv&quot;, header = T, sep = &quot;;&quot;) OTU_prep_sqrt &lt;- function(micro) { rownames(micro) &lt;- micro$Sample_ID micro$Sample_ID &lt;- NULL # micro &lt;- sqrt(micro) # can be toggled on/off micro_gb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Gb&quot;), ] micro_sf &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Sf&quot;), ] micro_wb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Wb&quot;), ] micro_gb &lt;- micro_gb[, colSums(micro_gb != 0) &gt; 0] micro_sf &lt;- micro_sf[, colSums(micro_sf != 0) &gt; 0] micro_wb &lt;- micro_wb[, colSums(micro_wb != 0) &gt; 0] micros &lt;- list(gb = micro_gb, sf = micro_sf, wb = micro_wb) return(micros) } micro_ds &lt;- OTU_prep_sqrt(micro) overall_rabdc &lt;- function(micros) { mic &lt;- micros n &lt;- 0 k &lt;- dim(mic)[1] mic[&quot;rowsum&quot;] &lt;- apply(mic, 1, sum) while (n &lt; k) { n &lt;- n + 1 mic[n, ] &lt;- mic[n, ]/(mic$rowsum[n]) } mic$rowsum &lt;- NULL mic &lt;- data.frame(t(mic)) mic[&quot;avg_rel_abdc&quot;] &lt;- apply(mic, 1, mean) mic[&quot;occurrence&quot;] &lt;- ifelse(mic$avg &gt; 0.0025, &quot;common&quot;, &quot;rare&quot;) return(mic) } occurrence &lt;- lapply(micro_ds, overall_rabdc) # PHYLUM adonis_prep &lt;- function(taxonomy, occurrence) { occurrence$gb[&quot;XOTU&quot;] &lt;- rownames(occurrence$gb) occurrence$sf[&quot;XOTU&quot;] &lt;- rownames(occurrence$sf) occurrence$wb[&quot;XOTU&quot;] &lt;- rownames(occurrence$wb) tax &lt;- taxonomy[, c(&quot;OTU_ID&quot;, &quot;Phylum&quot;, &quot;Class&quot;)] n &lt;- 0 k &lt;- dim(tax)[1] tax[&quot;XOTU&quot;] &lt;- NA while (n &lt; k) { n &lt;- n + 1 tax$XOTU[n] &lt;- paste0(&quot;X&quot;, tax$OTU_ID[n]) } tax_gb &lt;- inner_join(tax, occurrence$gb) tax_sf &lt;- inner_join(tax, occurrence$sf) tax_wb &lt;- inner_join(tax, occurrence$wb) taxes &lt;- list(gb = tax_gb, sf = tax_sf, wb = tax_wb) return(taxes) } taxes &lt;- adonis_prep(taxonomy, occurrence) cleaning &lt;- function(taxes) { gb &lt;- taxes$gb sf &lt;- taxes$sf wb &lt;- taxes$wb # Renaming &amp; removing whitespaces gb$Phylum &lt;- as.character(str_trim(as.character(gb$Phylum))) sf$Phylum &lt;- as.character(str_trim(as.character(sf$Phylum))) wb$Phylum &lt;- as.character(str_trim(as.character(wb$Phylum))) gb$Class &lt;- as.character(str_trim(as.character(gb$Class))) sf$Class &lt;- as.character(str_trim(as.character(sf$Class))) wb$Class &lt;- as.character(str_trim(as.character(wb$Class))) ## GB gb$Class[(gb$Phylum == &quot;PAUC34f&quot;)] &lt;- &quot;PAUC34f_unclassified&quot; gb$Class[(gb$Phylum == &quot;&quot;)] &lt;- &quot;unclassified&quot; gb$Phylum[(gb$Phylum == &quot;&quot;)] &lt;- &quot;unclassified&quot; gb$Class[(gb$Phylum == &quot;Tectomicrobia&quot;)] &lt;- &quot;Tectomicrobia_unclassified&quot; gb$Class[(gb$Phylum == &quot;SBR1093&quot;)] &lt;- &quot;SBR1093_unclassified&quot; gb$Class[(gb$Phylum == &quot;Poribacteria&quot;)] &lt;- &quot;Poribacteria_unclassified&quot; gb$Class[gb$Phylum == &quot;Chloroflexi&quot; &amp; gb$Class == &quot;&quot;] &lt;- &quot;Chloroflexi_unclassified&quot; ## SF sf$Class[(sf$Phylum == &quot;&quot;)] &lt;- &quot;unclassified&quot; sf$Phylum[(sf$Phylum == &quot;&quot;)] &lt;- &quot;unclassified&quot; sf$Class[(sf$Phylum == &quot;PAUC34f&quot;)] &lt;- &quot;PAUC34f_unclassified&quot; sf$Class[(sf$Phylum == &quot;Proteobacteria&quot; &amp; sf$Class == &quot;&quot;)] &lt;- &quot;Proteobacteria_unclassified&quot; sf$Class[(sf$Phylum == &quot;Tectomicrobia&quot;)] &lt;- &quot;Tectomicrobia_unclassified&quot; sf$Class[(sf$Phylum == &quot;SBR1093&quot;)] &lt;- &quot;SBR1093_unclassified&quot; sf$Class[(sf$Phylum == &quot;Poribacteria&quot;)] &lt;- &quot;Poribacteria_unclassified&quot; ## WB wb$Class[(wb$Phylum == &quot;&quot;)] &lt;- &quot;unclassified&quot; wb$Phylum[(wb$Phylum == &quot;&quot;)] &lt;- &quot;unclassified&quot; # merge back taxes &lt;- list(gb = gb, sf = sf, wb = wb) return(taxes) } taxes &lt;- cleaning(taxes) taxes &lt;- lapply(taxes, function(x) { rownames(x) &lt;- x$XOTU x }) taxes &lt;- lapply(taxes, function(x) { x[c(&quot;avg_rel_abdc&quot;, &quot;occurrence&quot;, &quot;OTU_ID&quot;, &quot;XOTU&quot;)] &lt;- NULL x }) taxes &lt;- lapply(taxes, function(x) { x[&quot;Class&quot;] &lt;- NULL x }) # aggregate sum of relative abundance per phylum in each sponge sample gb &lt;- aggregate(taxes$gb[, 2:dim(taxes$gb)[2]], list(taxes$gb[, &quot;Phylum&quot;]), sum) #works sf &lt;- aggregate(taxes$sf[, 2:dim(taxes$sf)[2]], list(taxes$sf[, &quot;Phylum&quot;]), sum) #works wb &lt;- aggregate(taxes$wb[, 2:dim(taxes$wb)[2]], list(taxes$wb[, &quot;Phylum&quot;]), sum) #works taxes_phy &lt;- full_join(gb, sf) taxes_phy &lt;- full_join(taxes_phy, wb) taxes_phy[is.na(taxes_phy)] &lt;- 0 # check there are no weird names aggregate(.~ Group.1, data=taxes_phy, sum) rownames(taxes_phy) &lt;- taxes_phy$Group.1 taxes_phy$Group.1 &lt;- NULL df_phylum &lt;- data.frame(t(taxes_phy)) #taxes_phy_t &lt;- df_phylum df_phylum[&quot;Sample_ID&quot;] &lt;- rownames(df_phylum) df_phylum &lt;- melt(df_phylum, id.vars = c(&quot;Sample_ID&quot;)) colnames(df_phylum) &lt;- c(&quot;Sample_ID&quot;, &quot;Phylum&quot;, &quot;variable&quot;) ggplot(df_phylum, aes(x = Sample_ID, y = variable, fill = Phylum)) + geom_bar(stat = &quot;identity&quot;) + theme_classic() + theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = &quot;bottom&quot;) + xlab(&quot;Samples ordered by depth&quot;) + ylab(&quot;Relative abundance&quot;) + scale_x_discrete(limits = c(&quot;Gb1&quot;, &quot;Gb2&quot;, &quot;Gb3&quot;, &quot;Gb4&quot;, &quot;Gb5&quot;, &quot;Gb6&quot;, &quot;Gb7&quot;, &quot;Gb8&quot;, &quot;Gb9&quot;, &quot;Gb10&quot;, &quot;Gb11&quot;, &quot;Gb12&quot;, &quot;Gb13&quot;, &quot;Gb14&quot;, &quot;Sf1&quot;, &quot;Sf2&quot;, &quot;Sf3&quot;, &quot;Sf4&quot;, &quot;Sf5&quot;, &quot;Sf6&quot;, &quot;Sf7&quot;, &quot;Sf8&quot;, &quot;Sf9&quot;, &quot;Sf10&quot;, &quot;Sf11&quot;, &quot;Sf12&quot;, &quot;Sf13&quot;, &quot;Sf14&quot;, &quot;Sf15&quot;, &quot;Wb1&quot;, &quot;Wb2&quot;, &quot;Wb3&quot;, &quot;Wb4&quot;, &quot;Wb5&quot;, &quot;Wb6&quot;, &quot;Wb7&quot;, &quot;Wb8&quot;, &quot;Wb9&quot;, &quot;Wb10&quot;, &quot;Wb11&quot;, &quot;Wb12&quot;, &quot;Wb13&quot;, &quot;Wb14&quot;, &quot;Wb15&quot;, &quot;Wb16&quot;)) + scale_fill_manual(&quot;&quot;, breaks = c(&quot;Acidobacteria&quot;, &quot;Actinobacteria&quot;, &quot;Bacteroidetes&quot;, &quot;Chlamydiae&quot;, &quot;Chloroflexi&quot;, &quot;Cyanobacteria&quot;, &quot;Deferribacteres&quot;, &quot;Deinococcus.Thermus&quot;, &quot;Firmicutes&quot;, &quot;Gemmatimonadetes&quot;, &quot;Nitrospinae&quot;, &quot;Nitrospirae&quot;, &quot;PAUC34f&quot;, &quot;Planctomycetes&quot;, &quot;Poribacteria&quot;, &quot;Proteobacteria&quot;, &quot;SBR1093&quot;, &quot;Spirochaetae&quot;, &quot;Tectomicrobia&quot;, &quot;Thaumarchaeota&quot;, &quot;unclassified&quot;, &quot;Verrucomicrobia&quot;), values = c(&quot;#b8c4f6&quot;, &quot;#ffaaaf&quot;, &quot;#3d1349&quot;, &quot;#B6B2A9&quot;, &quot;#01559d&quot;, &quot;#CACAC8&quot;, &quot;#E1DED7&quot;, &quot;#CEC7C1&quot;, &quot;#9A9B9D&quot;, &quot;#019c51&quot;, &quot;#b10060&quot;, &quot;#49ca00&quot;, &quot;#dd8e00&quot;, &quot;#f282ff&quot;, &quot;#AFA79D&quot;, &quot;#ff633f&quot;, &quot;#ec0040&quot;, &quot;#010b92&quot;, &quot;#cf00aa&quot;, &quot;#aba900&quot;, &quot;#ffffff&quot;, &quot;#fce300&quot;), labels = c(&quot;Acidobacteria&quot;, &quot;Actinobacteria&quot;, &quot;Bacteroidetes&quot;, &quot;Chlamydiae&quot;, &quot;Chloroflexi&quot;, &quot;Cyanobacteria&quot;, &quot;Deferribacteres&quot;, &quot;Deinococcus.Thermus&quot;, &quot;Firmicutes&quot;, &quot;Gemmatimonadetes&quot;, &quot;Nitrospinae&quot;, &quot;Nitrospirae&quot;, &quot;PAUC34f&quot;, &quot;Planctomycetes&quot;, &quot;Poribacteria&quot;, &quot;Proteobacteria&quot;, &quot;SBR1093&quot;, &quot;Spirochaetae&quot;, &quot;Tectomicrobia&quot;, &quot;Thaumarchaeota&quot;, &quot;unclassified&quot;, &quot;Verrucomicrobia&quot;)) Figure 3.11: Relative abundance of prokaryotic phyla per sponge sample. # facet df_phylum[&#39;spec&#39;] &lt;- str_sub(df_phylum$Sample_ID,1,2) md &lt;- meta_data[,c(&#39;unified_ID&#39;, &#39;Depth&#39;)] colnames(md) &lt;- c(&#39;Sample_ID&#39;, &#39;Depth&#39;) df_phylum &lt;- left_join(df_phylum, # md) # ggplot(df_phylum, aes(x=as.factor(Depth), y=variable, fill=Phylum))+geom_bar(stat=&#39;identity&#39;)+facet_wrap(.~spec, scales=&#39;free&#39;)+theme_classic()+theme(axis.text.x = # element_text(angle = 90, hjust = 1),legend.position = &#39;bottom&#39;)+xlab(&#39;Samples ordered by depth&#39;)+ylab(&#39;Relative abundance&#39;)+scale_fill_manual(&#39;&#39;, breaks = c(&#39;Acidobacteria&#39;, # &#39;Actinobacteria&#39;, &#39;Bacteroidetes&#39;, &#39;Chlamydiae&#39;, &#39;Chloroflexi&#39;, &#39;Cyanobacteria&#39;, &#39;Deferribacteres&#39;, &#39;Deinococcus.Thermus&#39;, &#39;Firmicutes&#39;, &#39;Gemmatimonadetes&#39;, &#39;Nitrospinae&#39;, # &#39;Nitrospirae&#39;, &#39;PAUC34f&#39;, &#39;Planctomycetes&#39;, &#39;Poribacteria&#39;, &#39;Proteobacteria&#39;, &#39;SBR1093&#39;, &#39;Spirochaetae&#39;, &#39;Tectomicrobia&#39;, &#39;Thaumarchaeota&#39;, &#39;unclassified&#39;, &#39;Verrucomicrobia&#39;), # values = # c(&#39;#b8c4f6&#39;,&#39;#ffaaaf&#39;,&#39;#3d1349&#39;,&#39;#ffffff&#39;,&#39;#01559d&#39;,&#39;#ffffff&#39;,&#39;#ffffff&#39;,&#39;#ffffff&#39;,&#39;#ffffff&#39;,&#39;#019c51&#39;,&#39;#b10060&#39;,&#39;#49ca00&#39;,&#39;#dd8e00&#39;,&#39;#f282ff&#39;,&#39;#ffffff&#39;,&#39;#ff633f&#39;,&#39;#ec0040&#39;,&#39;#010b92&#39;,&#39;#cf00aa&#39;,&#39;#aba900&#39;,&#39;#ffffff&#39;,&#39;#fce300&#39;), # labels = c(&#39;Acidobacteria&#39;, &#39;Actinobacteria&#39;, &#39;Bacteroidetes&#39;, &#39;Chlamydiae&#39;, &#39;Chloroflexi&#39;, &#39;Cyanobacteria&#39;, &#39;Deferribacteres&#39;, &#39;Deinococcus.Thermus&#39;, &#39;Firmicutes&#39;, # &#39;Gemmatimonadetes&#39;, &#39;Nitrospinae&#39;, &#39;Nitrospirae&#39;, &#39;PAUC34f&#39;, &#39;Planctomycetes&#39;, &#39;Poribacteria&#39;, &#39;Proteobacteria&#39;, &#39;SBR1093&#39;, &#39;Spirochaetae&#39;, &#39;Tectomicrobia&#39;, &#39;Thaumarchaeota&#39;, # &#39;unclassified&#39;, &#39;Verrucomicrobia&#39;) ) We see in this bar plot that the relative abundance of phyla remains fairly stable across the different depths. Generally, the prokaryotic community of sponges is described as somewhat species specific and stable across virtually any measured gradient. At this taxonomic resolution, these findings hold true. The composition of the prokaryotic communities in the HMA sponges G. barretti and S. fortis is similar across all samples, while the composition in the LMA sponge W. bursa differs in composition but also displaying only minor variations in phyla proportions. The phyla left white/blank are present at very low abundance and we thought the figure might be visually easier without too many colours. The phyla Chloroflexi, Actinobacteria, Acidobacteria, PAUC34f, and Gemmatimonadetes were described as HMA indicator phyla (Moitinho-Silva et al. 2017) and are present in the HMA sponges G. barretti and S. fortis (although Actinobacteria are also present in W. bursa).The phyla Proteobacteria, Bacteroidetes, Planctomycetes, and Firmicutes were disgnated LMA indicator phyla and are found in W. bursa (although Proteobacteria are also present in the HMA sponges). 3.5.2 Tabular overview If you prefer numbers, this is what it breaks down to. You can sort the tables. microbiome &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) OTU_prep_sqrt &lt;- function(micro) { rownames(micro) &lt;- micro$Sample_ID micro$Sample_ID &lt;- NULL micro &lt;- sqrt(micro) micro_gb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Gb&quot;), ] micro_sf &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Sf&quot;), ] micro_wb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Wb&quot;), ] micro_gb &lt;- micro_gb[, colSums(micro_gb != 0) &gt; 0] micro_sf &lt;- micro_sf[, colSums(micro_sf != 0) &gt; 0] micro_wb &lt;- micro_wb[, colSums(micro_wb != 0) &gt; 0] micros &lt;- list(gb = micro_gb, sf = micro_sf, wb = micro_wb) return(micros) } micro_ds &lt;- OTU_prep_sqrt(microbiome) # calculate relative abundance of OTU across sponge samples overall_rabdc &lt;- function(micros) { mic &lt;- micros n &lt;- 0 k &lt;- dim(mic)[1] mic[&quot;rowsum&quot;] &lt;- apply(mic, 1, sum) while (n &lt; k) { n &lt;- n + 1 mic[n, ] &lt;- mic[n, ]/(mic$rowsum[n]) } mic$rowsum &lt;- NULL mic &lt;- data.frame(t(mic)) mic[&quot;avg_rel_abdc&quot;] &lt;- apply(mic, 1, mean) mic[&quot;occurrence&quot;] &lt;- ifelse(mic$avg &gt; 0.0025, &quot;common&quot;, &quot;rare&quot;) return(mic) } # gb_occurrence &lt;- overall_rabdc(micro_ds$gb) sf_occurrence &lt;- overall_rabdc(micro_ds$sf) wb_occurrence &lt;- overall_rabdc(micro_ds$wb) occurrence &lt;- list(gb=gb_occurrence, # sf=sf_occurrence, wb=wb_occurrence) occurrence &lt;- lapply(micro_ds, overall_rabdc) aggregations &lt;- function(taxonomy, occurrence) { occurrence$gb[&quot;XOTU&quot;] &lt;- rownames(occurrence$gb) occurrence$sf[&quot;XOTU&quot;] &lt;- rownames(occurrence$sf) occurrence$wb[&quot;XOTU&quot;] &lt;- rownames(occurrence$wb) tax &lt;- taxonomy[, c(&quot;OTU_ID&quot;, &quot;Phylum&quot;, &quot;Class&quot;)] n &lt;- 0 k &lt;- dim(tax)[1] tax[&quot;XOTU&quot;] &lt;- NA while (n &lt; k) { n &lt;- n + 1 tax$XOTU[n] &lt;- paste0(&quot;X&quot;, tax$OTU_ID[n]) } tax_gb &lt;- inner_join(tax, occurrence$gb[, c(&quot;XOTU&quot;, &quot;avg_rel_abdc&quot;)]) tax_sf &lt;- inner_join(tax, occurrence$sf[, c(&quot;XOTU&quot;, &quot;avg_rel_abdc&quot;)]) tax_wb &lt;- inner_join(tax, occurrence$wb[, c(&quot;XOTU&quot;, &quot;avg_rel_abdc&quot;)]) taxes &lt;- list(gb = tax_gb, sf = tax_sf, wb = tax_wb) gb1 &lt;- aggregate(tax_gb$Phylum, by = list(tax_gb$Phylum), FUN = &quot;length&quot;) gb2 &lt;- aggregate(tax_gb$avg_rel_abdc, by = list(tax_gb$Phylum), FUN = &quot;sum&quot;) gb_p &lt;- full_join(gb1, gb2, by = (&quot;Group.1&quot; = &quot;Group.1&quot;)) colnames(gb_p) &lt;- c(&quot;Phylum&quot;, &quot;OTU_number&quot;, &quot;avg_rel_abdc&quot;) gb1 &lt;- aggregate(tax_gb$Class, by = list(tax_gb$Class), FUN = &quot;length&quot;) gb2 &lt;- aggregate(tax_gb$avg_rel_abdc, by = list(tax_gb$Class), FUN = &quot;sum&quot;) gb_c &lt;- full_join(gb1, gb2, by = (&quot;Group.1&quot; = &quot;Group.1&quot;)) colnames(gb_c) &lt;- c(&quot;Class&quot;, &quot;OTU_number&quot;, &quot;avg_rel_abdc&quot;) test1 &lt;- lapply(taxes, function(x) aggregate(avg_rel_abdc ~ Phylum, data = x, FUN = &quot;length&quot;)) lapply(taxes, function(x) aggregate(avg_rel_abdc ~ Phylum, data = x, FUN = &quot;sum&quot;)) return(taxes) } taxes &lt;- aggregations(taxonomy, occurrence) cleaning &lt;- function(taxes) { gb &lt;- taxes$gb sf &lt;- taxes$sf wb &lt;- taxes$wb # Renaming &amp; removing whitespaces gb$Phylum &lt;- as.character(str_trim(as.character(gb$Phylum))) sf$Phylum &lt;- as.character(str_trim(as.character(sf$Phylum))) wb$Phylum &lt;- as.character(str_trim(as.character(wb$Phylum))) gb$Class &lt;- as.character(str_trim(as.character(gb$Class))) sf$Class &lt;- as.character(str_trim(as.character(sf$Class))) wb$Class &lt;- as.character(str_trim(as.character(wb$Class))) ## GB gb$Class[(gb$Phylum == &quot;PAUC34f&quot;)] &lt;- &quot;PAUC34f_unclassified&quot; gb$Class[(gb$Phylum == &quot;&quot;)] &lt;- &quot;unclassified&quot; gb$Phylum[(gb$Phylum == &quot;&quot;)] &lt;- &quot;unclassified&quot; gb$Class[(gb$Phylum == &quot;Tectomicrobia&quot;)] &lt;- &quot;Tectomicrobia_unclassified&quot; gb$Class[(gb$Phylum == &quot;SBR1093&quot;)] &lt;- &quot;SBR1093_unclassified&quot; gb$Class[(gb$Phylum == &quot;Poribacteria&quot;)] &lt;- &quot;Poribacteria_unclassified&quot; gb$Class[gb$Phylum == &quot;Chloroflexi&quot; &amp; gb$Class == &quot;&quot;] &lt;- &quot;Chloroflexi_unclassified&quot; ## SF sf$Class[(sf$Phylum == &quot;&quot;)] &lt;- &quot;unclassified&quot; sf$Phylum[(sf$Phylum == &quot;&quot;)] &lt;- &quot;unclassified&quot; sf$Class[(sf$Phylum == &quot;PAUC34f&quot;)] &lt;- &quot;PAUC34f_unclassified&quot; sf$Class[(sf$Phylum == &quot;Proteobacteria&quot; &amp; sf$Class == &quot;&quot;)] &lt;- &quot;Proteobacteria_unclassified&quot; sf$Class[(sf$Phylum == &quot;Tectomicrobia&quot;)] &lt;- &quot;Tectomicrobia_unclassified&quot; sf$Class[(sf$Phylum == &quot;SBR1093&quot;)] &lt;- &quot;SBR1093_unclassified&quot; sf$Class[(sf$Phylum == &quot;Poribacteria&quot;)] &lt;- &quot;Poribacteria_unclassified&quot; ## WB wb$Class[(wb$Phylum == &quot;&quot;)] &lt;- &quot;unclassified&quot; wb$Phylum[(wb$Phylum == &quot;&quot;)] &lt;- &quot;unclassified&quot; # merge back taxes &lt;- list(gb = gb, sf = sf, wb = wb) return(taxes) } taxes &lt;- cleaning(taxes) phy_OTU &lt;- lapply(taxes, function(x) aggregate(avg_rel_abdc ~ Phylum, data = x, FUN = &quot;length&quot;)) #OTU count phy_rabdc &lt;- lapply(taxes, function(x) aggregate(avg_rel_abdc ~ Phylum, data = x, FUN = &quot;sum&quot;)) #sums relative abundance class_OTU &lt;- lapply(taxes, function(x) aggregate(avg_rel_abdc ~ Class, data = x, FUN = &quot;length&quot;)) class_rabdc &lt;- lapply(taxes, function(x) aggregate(avg_rel_abdc ~ Class, data = x, FUN = &quot;sum&quot;)) 3.5.2.1 G. barretti: Phyla and classes present # https://rstudio.github.io/DT/ gb_P &lt;- cbind(phy_OTU$gb, phy_rabdc$gb[, (&quot;avg_rel_abdc&quot;)]) colnames(gb_P) &lt;- c(&quot;Phylum&quot;, &quot;OTU count&quot;, &quot;cum. avg. abundance&quot;) # gb_P$`cummulative average abundance` &lt;- round(c(gb_P$`cummulative average abundance`, 6)) DT::datatable(gb_P, rownames = FALSE) gb_C &lt;- cbind(class_OTU$gb, class_rabdc$gb[, (&quot;avg_rel_abdc&quot;)]) colnames(gb_C) &lt;- c(&quot;Class&quot;, &quot;OTU count&quot;, &quot;cummulative average abundance&quot;) # gb_C$`cummulative average abundance` &lt;- round(c(gb_C$`cummulative average abundance`, digits = 6)) DT::datatable(gb_C, rownames = FALSE) 3.5.2.2 S. fortis: Phyla and classes present sf_P &lt;- cbind(phy_OTU$sf, phy_rabdc$sf[, (&quot;avg_rel_abdc&quot;)]) colnames(sf_P) &lt;- c(&quot;Phylum&quot;, &quot;OTU count&quot;, &quot;cum. avg. abundance&quot;) # sf_P$`cummulative average abundance` &lt;- round(c(sf_P$`cummulative average abundance`, digits = 6)) DT::datatable(sf_P, rownames = FALSE) sf_C &lt;- cbind(class_OTU$sf, class_rabdc$sf[, (&quot;avg_rel_abdc&quot;)]) colnames(sf_C) &lt;- c(&quot;Class&quot;, &quot;OTU count&quot;, &quot;cummulative average abundance&quot;) # sf_C$`cummulative average abundance` &lt;- round(c(sf_C$`cummulative average abundance`, digits = 6)) DT::datatable(sf_C, rownames = FALSE) 3.5.2.3 W. bursa: Phyla and classes present wb_P &lt;- cbind(phy_OTU$wb, phy_rabdc$wb[, (&quot;avg_rel_abdc&quot;)]) colnames(wb_P) &lt;- c(&quot;Phylum&quot;, &quot;OTU count&quot;, &quot;cummulative average abundance&quot;) # wb_P$`cummulative average abundance` &lt;- round(c(wb_P$`cummulative average abundance`, digits = 6)) DT::datatable(wb_P, rownames = FALSE) wb_C &lt;- cbind(class_OTU$wb, class_rabdc$wb[, (&quot;avg_rel_abdc&quot;)]) colnames(wb_C) &lt;- c(&quot;Class&quot;, &quot;OTU count&quot;, &quot;cummulative average abundance&quot;) # wb_C$`cummulative average abundance` &lt;- round(c(wb_C$`cummulative average abundance`, digits = 6)) DT::datatable(wb_C, rownames = FALSE) rm(gb_P, sf_P, wb_P, gb_C, sf_C, wb_C) 3.5.3 Statistical comparison at phylum and class level 3.6 Depth response: OTU perspective Summarising the OTU diversity by phylum or class only shows what was previously known. The composition of the sponges’ prokaryotic community is stable. However, summarising doesn’t do the data justice. We therefore correlate the average relative abundance of every OTU with depth. The Otus yielding a significant correlation with depth are shown by their respecive relative abundance across the depths. # ====================== INC-DEC CORRELATION ================== Categorises the OTUs by their response to depth. micro &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) meta_data_prep &lt;- function(meta_data) { meta_data &lt;- meta_data[, c(&quot;unified_ID&quot;, &quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;MeanBottomTemp_Cdeg&quot;, &quot;MeanBotSalinity_PSU&quot;, &quot;YEAR&quot;)] colnames(meta_data) &lt;- c(&quot;unified_ID&quot;, &quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;Temperature&quot;, &quot;Salinity&quot;, &quot;Year&quot;) meta_data &lt;- meta_data[!(str_sub(meta_data$unified_ID, 1, 2) == &quot;QC&quot;), ] meta_data[] &lt;- lapply(meta_data, function(x) if (is.factor(x)) factor(x) else x) # Gb12, Gb20 and Gb21 are missing temperature and salinity. Imputing data from closeby samples: meta_data$Salinity[meta_data$unified_ID == &quot;Gb12&quot;] &lt;- 34.92 meta_data$Salinity[meta_data$unified_ID == &quot;Gb20&quot;] &lt;- 34.92 meta_data$Salinity[meta_data$unified_ID == &quot;Gb21&quot;] &lt;- 34.56 meta_data$Temperature[meta_data$unified_ID == &quot;Gb12&quot;] &lt;- 3.71 meta_data$Temperature[meta_data$unified_ID == &quot;Gb20&quot;] &lt;- 3.65 meta_data$Temperature[meta_data$unified_ID == &quot;Gb21&quot;] &lt;- 2.32 meta_data[&quot;spec&quot;] &lt;- str_sub(meta_data$unified_ID, 1, 2) meta_data &lt;- meta_data[order(meta_data$unified_ID), ] return(meta_data) } meta_data &lt;- meta_data_prep(meta_data) OTU_prep_sqrt &lt;- function(micro) { rownames(micro) &lt;- micro$Sample_ID micro$Sample_ID &lt;- NULL micro &lt;- sqrt(micro) #sqrt could be toggled on/off here micro_gb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Gb&quot;), ] micro_sf &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Sf&quot;), ] micro_wb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Wb&quot;), ] micro_gb &lt;- micro_gb[, colSums(micro_gb != 0) &gt; 0] micro_sf &lt;- micro_sf[, colSums(micro_sf != 0) &gt; 0] micro_wb &lt;- micro_wb[, colSums(micro_wb != 0) &gt; 0] micros &lt;- list(gb = micro_gb, sf = micro_sf, wb = micro_wb) return(micros) } micro_ds &lt;- OTU_prep_sqrt(micro) ### overall_rabdc &lt;- function(micros) { mic &lt;- micros n &lt;- 0 k &lt;- dim(mic)[1] mic[&quot;rowsum&quot;] &lt;- apply(mic, 1, sum) while (n &lt; k) { n &lt;- n + 1 mic[n, ] &lt;- mic[n, ]/(mic$rowsum[n]) } mic$rowsum &lt;- NULL mic &lt;- data.frame(t(mic)) # mic[&#39;avg_rel_abdc&#39;] &lt;- apply(mic, 1, mean) mic[&#39;occurrence&#39;] &lt;- ifelse(mic$avg&gt;0.0025, &#39;common&#39;, &#39;rare&#39;) return(mic) } rabdc &lt;- lapply(micro_ds, overall_rabdc) # CORRELATION inc_dec &lt;- function(rabdc_df, meta_data) { md &lt;- meta_data[meta_data$unified_ID %in% colnames(rabdc_df), ] inc_dec &lt;- data.frame(rownames(rabdc_df)) colnames(inc_dec) &lt;- &quot;XOTU&quot; inc_dec[&quot;inc_dec_estimate&quot;] &lt;- NA inc_dec[&quot;inc_dec_p_val&quot;] &lt;- NA inc_dec[&quot;fdr&quot;] &lt;- NA n &lt;- 0 k &lt;- dim(inc_dec)[1] while (n &lt; k) { n &lt;- n + 1 inc_dec$inc_dec_estimate[n] &lt;- cor.test(as.numeric(rabdc_df[n, ]), md$Depth)$estimate inc_dec$inc_dec_p_val[n] &lt;- cor.test(as.numeric(rabdc_df[n, ]), md$Depth)$p.value } inc_dec[&quot;classification&quot;] &lt;- NA inc_dec$classification[inc_dec$inc_dec_estimate &lt; 0] &lt;- &quot;dec.trend&quot; inc_dec$classification[inc_dec$inc_dec_estimate &gt; 0] &lt;- &quot;inc.trend&quot; inc_dec$classification[inc_dec$inc_dec_estimate &lt; 0 &amp; inc_dec$inc_dec_p &lt;= 0.05] &lt;- &quot;decreasing&quot; inc_dec$classification[inc_dec$inc_dec_estimate &gt; 0 &amp; inc_dec$inc_dec_p &lt;= 0.05] &lt;- &quot;increasing&quot; inc_dec$fdr &lt;- p.adjust(inc_dec$inc_dec_p_val, method = &quot;fdr&quot;) return(inc_dec) } response &lt;- lapply(rabdc, inc_dec, meta_data = meta_data) scale_viz &lt;- function(micro) { mic &lt;- micro mic[&quot;max&quot;] &lt;- apply(mic, 1, max) n &lt;- 0 k &lt;- dim(mic)[1] while (n &lt; k) { n &lt;- n + 1 mic[n, ] &lt;- mic[n, ]/(mic$max[n]) } mic$max &lt;- NULL return(mic) } rabdc &lt;- lapply(rabdc, scale_viz) #to scale between {0,1} for visualisation rabdc_gb &lt;- rabdc$gb rabdc_sf &lt;- rabdc$sf rabdc_wb &lt;- rabdc$wb gb_response &lt;- response$gb sf_response &lt;- response$sf wb_response &lt;- response$wb # gb rabdc_gb[&quot;XOTU&quot;] &lt;- rownames(rabdc_gb) gb_heatmap &lt;- full_join(rabdc_gb, gb_response) gb_heatmap &lt;- melt(gb_heatmap, id.vars = c(&quot;XOTU&quot;, &quot;inc_dec_estimate&quot;, &quot;inc_dec_p_val&quot;, &quot;classification&quot;)) md &lt;- meta_data[, c(&quot;unified_ID&quot;, &quot;Depth&quot;)] colnames(md) &lt;- c(&quot;variable&quot;, &quot;Depth&quot;) gb_heatmap &lt;- left_join(gb_heatmap, md) gb_heatmap[&quot;name&quot;] &lt;- str_sub(gb_heatmap$XOTU, -3) gb_heatmap_i &lt;- gb_heatmap[gb_heatmap$classification == &quot;increasing&quot;, ] gb_heatmap_d &lt;- gb_heatmap[gb_heatmap$classification == &quot;decreasing&quot;, ] # gb_i &lt;- ggplot(gb_heatmap_i, aes(x=as.factor(gb_heatmap_i$Depth), y=gb_heatmap_i$name, fill=gb_heatmap_i$value))+geom_tile()+ theme(axis.text.x = element_text(angle = 90, hjust # = 1))+xlab(&#39;Depth&#39;)+ylab(&#39;OTUs&#39;)+ggtitle(&#39;GB OTUs increasing&#39;)+scale_fill_viridis_c(option = &#39;plasma&#39;)+coord_equal()+theme(plot.background=element_blank(), # panel.border=element_blank(), legend.title=element_blank(),legend.position=&#39;bottom&#39;) gb_d &lt;- ggplot(gb_heatmap_d, aes(x=as.factor(gb_heatmap_d$Depth), y=gb_heatmap_d$name, # fill=gb_heatmap_d$value))+geom_tile()+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+xlab(&#39;Depth&#39;)+ylab(&#39;OTUs&#39;)+ggtitle(&#39;GB OTUs # decreasing&#39;)+scale_fill_viridis_c(option = &#39;plasma&#39;)+coord_equal()+theme(plot.background=element_blank(), panel.border=element_blank(), # legend.title=element_blank(),legend.position=&#39;bottom&#39;) # sf rabdc_sf[&quot;XOTU&quot;] &lt;- rownames(rabdc_sf) sf_heatmap &lt;- full_join(rabdc_sf, sf_response) sf_heatmap &lt;- melt(sf_heatmap, id.vars = c(&quot;XOTU&quot;, &quot;inc_dec_estimate&quot;, &quot;inc_dec_p_val&quot;, &quot;classification&quot;)) md &lt;- meta_data[, c(&quot;unified_ID&quot;, &quot;Depth&quot;)] colnames(md) &lt;- c(&quot;variable&quot;, &quot;Depth&quot;) sf_heatmap &lt;- left_join(sf_heatmap, md) sf_heatmap[&quot;name&quot;] &lt;- str_sub(sf_heatmap$XOTU, -3) sf_heatmap_i &lt;- sf_heatmap[sf_heatmap$classification == &quot;increasing&quot;, ] sf_heatmap_d &lt;- sf_heatmap[sf_heatmap$classification == &quot;decreasing&quot;, ] # sf_i &lt;- ggplot(sf_heatmap_i, aes(x=as.factor(sf_heatmap_i$Depth), y=sf_heatmap_i$name, fill=sf_heatmap_i$value))+geom_tile()+ theme(axis.text.x = element_text(angle = 90, hjust # = 1))+xlab(&#39;Depth&#39;)+ylab(&#39;OTUs&#39;)+ggtitle(&#39;SF OTUs increasing&#39;)+scale_fill_viridis_c(option = &#39;plasma&#39;)+coord_equal()+theme(plot.background=element_blank(), # panel.border=element_blank(), legend.title=element_blank(),legend.position=&#39;bottom&#39;) sf_d &lt;- ggplot(sf_heatmap_d, aes(x=as.factor(sf_heatmap_d$Depth), y=sf_heatmap_d$name, # fill=sf_heatmap_d$value))+geom_tile()+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+xlab(&#39;Depth&#39;)+ylab(&#39;OTUs&#39;)+ggtitle(&#39;SF OTUs # decreasing&#39;)+scale_fill_viridis_c(option = &#39;plasma&#39;)+coord_equal()+theme(plot.background=element_blank(), panel.border=element_blank(), # legend.title=element_blank(),legend.position=&#39;bottom&#39;) # wb rabdc_wb[&quot;XOTU&quot;] &lt;- rownames(rabdc_wb) wb_heatmap &lt;- full_join(rabdc_wb, wb_response) wb_heatmap &lt;- melt(wb_heatmap, id.vars = c(&quot;XOTU&quot;, &quot;inc_dec_estimate&quot;, &quot;inc_dec_p_val&quot;, &quot;classification&quot;)) md &lt;- meta_data[, c(&quot;unified_ID&quot;, &quot;Depth&quot;)] colnames(md) &lt;- c(&quot;variable&quot;, &quot;Depth&quot;) wb_heatmap &lt;- left_join(wb_heatmap, md) wb_heatmap[&quot;name&quot;] &lt;- str_sub(wb_heatmap$XOTU, -3) wb_heatmap_i &lt;- wb_heatmap[wb_heatmap$classification == &quot;increasing&quot;, ] wb_heatmap_d &lt;- wb_heatmap[wb_heatmap$classification == &quot;decreasing&quot;, ] # wb_i &lt;- ggplot(wb_heatmap_i, aes(x=as.factor(wb_heatmap_i$Depth), y=wb_heatmap_i$name, fill=wb_heatmap_i$value))+geom_tile()+ theme(axis.text.x = element_text(angle = 90, hjust # = 1))+xlab(&#39;Depth&#39;)+ylab(&#39;OTUs&#39;)+ggtitle(&#39;WB OTUs increasing&#39;)+scale_fill_viridis_c(option = &#39;plasma&#39;)+coord_equal()+theme(plot.background=element_blank(), # panel.border=element_blank(), legend.title=element_blank(),legend.position=&#39;bottom&#39;) wb_d &lt;- ggplot(wb_heatmap_d, aes(x=as.factor(wb_heatmap_d$Depth), y=wb_heatmap_d$name, # fill=wb_heatmap_d$value))+geom_tile()+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+xlab(&#39;Depth&#39;)+ylab(&#39;OTUs&#39;)+ggtitle(&#39;WB OTUs # decreasing&#39;)+scale_fill_viridis_c(option = &#39;plasma&#39;)+coord_equal()+theme(plot.background=element_blank(), panel.border=element_blank(), # legend.title=element_blank(),legend.position=&#39;bottom&#39;) # Facetting for the figure in the publication gb_heatmap_i[&quot;spec&quot;] &lt;- c(&quot;Geodia barretti&quot;) sf_heatmap_i[&quot;spec&quot;] &lt;- c(&quot;Stryphnus fortis&quot;) wb_heatmap_i[&quot;spec&quot;] &lt;- c(&quot;Weberella bursa&quot;) increase &lt;- rbind(gb_heatmap_i, sf_heatmap_i, wb_heatmap_i) inc &lt;- ggplot(increase, aes(x = as.factor(Depth), y = name, fill = value)) + facet_grid(. ~ spec, space = &quot;free&quot;, scales = &quot;free&quot;) + geom_tile() + theme_classic() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab(&quot;Depth&quot;) + ylab(&quot;OTUs&quot;) + ggtitle(&quot;OTUs increasing&quot;) + scale_fill_viridis_c(option = &quot;plasma&quot;) + theme(plot.background = element_blank(), panel.border = element_blank(), legend.title = element_blank(), legend.position = &quot;bottom&quot;) gb_heatmap_d[&quot;spec&quot;] &lt;- c(&quot;Geodia barretti&quot;) sf_heatmap_d[&quot;spec&quot;] &lt;- c(&quot;Stryphnus fortis&quot;) wb_heatmap_d[&quot;spec&quot;] &lt;- c(&quot;Weberella bursa&quot;) decrease &lt;- rbind(gb_heatmap_d, sf_heatmap_d, wb_heatmap_d) dec &lt;- ggplot(decrease, aes(x = as.factor(Depth), y = name, fill = value)) + facet_grid(. ~ spec, space = &quot;free&quot;, scales = &quot;free&quot;, drop = T) + geom_tile() + theme_classic() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab(&quot;Depth&quot;) + ylab(&quot;OTUs&quot;) + ggtitle(&quot;OTUs decreasing&quot;) + scale_fill_viridis_c(option = &quot;plasma&quot;) + theme(plot.background = element_blank(), panel.border = element_blank(), legend.title = element_blank(), legend.position = &quot;bottom&quot;) library(gridExtra) grid.arrange(inc, dec, nrow = 1) Figure 3.12: OTUs significantly increasing and decreasing with depth in the three sponge species. gb_nums &lt;- c(length(unique(gb_heatmap$name)), length(unique(gb_heatmap_i$name)), length(unique(gb_heatmap_d$name))) sf_nums &lt;- c(length(unique(sf_heatmap$name)), length(unique(sf_heatmap_i$name)), length(unique(sf_heatmap_d$name))) wb_nums &lt;- c(length(unique(wb_heatmap$name)), length(unique(wb_heatmap_i$name)), length(unique(wb_heatmap_d$name))) overview &lt;- rbind(gb_nums, sf_nums, wb_nums) colnames(overview) &lt;- c(&quot;Total&quot;, &quot;increasing&quot;, &quot;decreasing&quot;) rownames(overview) &lt;- c(&quot;G. barretti microbiota&quot;, &quot;S. fortis microbiota&quot;, &quot;W. bursa microbiota&quot;) overview &lt;- data.frame(overview) overview[&quot;unaffected&quot;] &lt;- overview$Total - (overview$increasing + overview$decreasing) kable(overview, col.names = c(&quot;Total&quot;, &quot;N OTUs increasing&quot;, &quot;N OTUs decreasing&quot;, &quot;N OTUs unaffected&quot;), escape = F, align = &quot;c&quot;, booktabs = T, caption = &quot;Microbiota response to depth&quot;, &quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;, latex_options = &quot;striped&quot;, full_width = F)) Table 3.4: Microbiota response to depth Total N OTUs increasing N OTUs decreasing N OTUs unaffected G. barretti microbiota 420 86 63 271 S. fortis microbiota 461 62 37 362 W. bursa microbiota 135 12 11 112 write.csv(gb_response, &quot;data/gb_response.csv&quot;) write.csv(sf_response, &quot;data/sf_response.csv&quot;) write.csv(wb_response, &quot;data/wb_response.csv&quot;) We’ve shown that thre is a substantial fraction of OTUs changing with depth in the three sponges. How much do these OTUs contribute the the respecive samples’ microbiota? (Are these just rare OTU?) micro &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- meta_data_prep(meta_data) micro_ds &lt;- OTU_prep_sqrt(micro) rabdc &lt;- lapply(micro_ds, overall_rabdc) response &lt;- lapply(rabdc, inc_dec, meta_data = meta_data) # until here same as in previous chunk (increade-decrease) # How much in terms of relative abundance do OTUs increasing/decreasing contribute? resp_subset &lt;- response$gb resp_subset &lt;- resp_subset[resp_subset$classification == &quot;increasing&quot; | resp_subset$classification == &quot;decreasing&quot;, ] rabdc$gb[&quot;XOTU&quot;] &lt;- rownames(rabdc$gb) changes &lt;- left_join(resp_subset, rabdc$gb) apply(changes[, 6:19], 2, sum) ## Gb1 Gb10 Gb11 Gb12 Gb13 Gb14 Gb2 Gb3 Gb4 Gb5 Gb6 Gb7 Gb8 Gb9 ## 0.4679053 0.4411581 0.4345952 0.4582772 0.4704101 0.4945428 0.3999313 0.4571735 0.4597044 0.4270513 0.4573897 0.4399143 0.4510839 0.4368421 q &lt;- apply(changes[, 6:19], 2, sum) resp_subset &lt;- response$sf resp_subset &lt;- resp_subset[resp_subset$classification == &quot;increasing&quot; | resp_subset$classification == &quot;decreasing&quot;, ] rabdc$sf[&quot;XOTU&quot;] &lt;- rownames(rabdc$sf) changes &lt;- left_join(resp_subset, rabdc$sf) apply(changes[, 6:20], 2, sum) ## Sf1 Sf10 Sf11 Sf12 Sf13 Sf14 Sf15 Sf2 Sf3 Sf4 Sf5 Sf6 Sf7 Sf8 Sf9 ## 0.2000647 0.2239502 0.2180442 0.2386535 0.2624164 0.2961391 0.2657806 0.2339318 0.1792833 0.1874675 0.1782178 0.1864880 0.1414710 0.1749049 0.1950652 w &lt;- apply(changes[, 6:20], 2, sum) resp_subset &lt;- response$wb resp_subset &lt;- resp_subset[resp_subset$classification == &quot;increasing&quot; | resp_subset$classification == &quot;decreasing&quot;, ] rabdc$wb[&quot;XOTU&quot;] &lt;- rownames(rabdc$wb) changes &lt;- left_join(resp_subset, rabdc$wb) apply(changes[, 6:21], 2, sum) ## Wb1 Wb10 Wb11 Wb12 Wb13 Wb14 Wb15 Wb16 Wb2 Wb3 Wb4 Wb5 Wb6 Wb7 Wb8 Wb9 ## 0.28250794 0.08677336 0.10016788 0.10674218 0.10963458 0.15246853 0.14389634 0.21255564 0.23991810 0.15771368 0.12690033 0.12182423 0.09622011 0.06991101 0.04877248 0.10178304 e &lt;- apply(changes[, 6:21], 2, sum) It turns out they contribute substantially: in G. barretti mean 0.45 (0.023 SD), in S. fortis mean 0.212 (0.042 SD), in W. bursa mean 0.135 (0.063 SD). In Fig.3.12 as well as in Tab. 3.4, we see that at the OTU level, we observe shifts. While sometimes more gradual, there seem to OTUs exclusively present in the “shallow” or the “deep” samples in all three sponges. In fact, in G. barretti and S. fortis, the number of OTUs increasing with depth (“deep water mass microbiome”) is greater than the number of OTUs decreasing. This leads us to believe that the deep water mass contains microbes yet to discover. The OTUs increasing and decreasing also represent substantial parts of the three sponge microbiota. 3.7 Sequence similarity library(seqinr) library(tidyverse) # fasta files generation micro &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) fastas &lt;- read.fasta(&quot;data/all_otus_artic_691.fasta&quot;) split_fastas &lt;- function(micro, fastas) { rownames(micro) &lt;- micro$Sample_ID micro$Sample_ID &lt;- NULL micro_gb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Gb&quot;), ] micro_sf &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Sf&quot;), ] micro_wb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Wb&quot;), ] micro_gb &lt;- micro_gb[, colSums(micro_gb != 0) &gt; 0] micro_sf &lt;- micro_sf[, colSums(micro_sf != 0) &gt; 0] micro_wb &lt;- micro_wb[, colSums(micro_wb != 0) &gt; 0] micro_gb &lt;- as.data.frame(colnames(micro_gb)) micro_sf &lt;- as.data.frame(colnames(micro_sf)) micro_wb &lt;- as.data.frame(colnames(micro_wb)) colnames(micro_gb) &lt;- c(&quot;XOTU&quot;) colnames(micro_sf) &lt;- c(&quot;XOTU&quot;) colnames(micro_wb) &lt;- c(&quot;XOTU&quot;) micro_gb[&quot;OTU&quot;] &lt;- str_sub(micro_gb$XOTU, 2, 13) micro_sf[&quot;OTU&quot;] &lt;- str_sub(micro_sf$XOTU, 2, 13) micro_wb[&quot;OTU&quot;] &lt;- str_sub(micro_wb$XOTU, 2, 13) gb_fastas &lt;- fastas[names(fastas) %in% micro_gb$OTU] sf_fastas &lt;- fastas[names(fastas) %in% micro_sf$OTU] wb_fastas &lt;- fastas[names(fastas) %in% micro_wb$OTU] fasta_sets &lt;- list(gb = gb_fastas, sf = sf_fastas, wb = wb_fastas) return(fasta_sets) } fasta_sets &lt;- split_fastas(micro, fastas) write.fasta(sequences = fasta_sets$gb, names = names(fasta_sets$gb), file.out = &quot;data/gb_OTU_seqs.fasta&quot;) write.fasta(sequences = fasta_sets$sf, names = names(fasta_sets$sf), file.out = &quot;data/sf_OTU_seqs.fasta&quot;) write.fasta(sequences = fasta_sets$wb, names = names(fasta_sets$wb), file.out = &quot;data/wb_OTU_seqs.fasta&quot;) rm(fatsta_sets, micro, fastas) We produced fasta files, i.e. files containing the DNA sequences of the OTUs in the three sponges. The fasta sequences were aligned with MAFFT. From the aligned seqeunces, we calculated the sequence similarity comparing all versus all, and retain in a file all comparisons yielding a sequence similarity \\(\\geq\\) 97%. # Load one set of files at a time ali &lt;- read.alignment(&quot;data/gb_reads_for_phylogeny_MAFFT.fasta&quot;, &quot;fasta&quot;) anno &lt;- read.csv(&quot;data/gb_OTUs_overall_rabdc_annotated.csv&quot;, header = T, sep = &quot;,&quot;) # ali &lt;- read.alignment(&#39;data/sf_reads_for_phylogeny_MAFFT.fasta&#39;, &#39;fasta&#39;) anno &lt;- read.csv(&#39;data/sf_OTUs_overall_rabdc_annotated.csv&#39;, header=T, sep=&#39;,&#39;) # ali &lt;- read.alignment(&#39;data/wb_reads_for_phylogeny_MAFFT.fasta&#39;, &#39;fasta&#39;) anno &lt;- read.csv(&#39;data/wb_OTUs_overall_rabdc_annotated.csv&#39;, header=T, sep=&#39;,&#39;) # FUN calculate pairwise distance, melt, keeps only entries with &gt;97% and &lt;1 similarity with significant opposing trends in both partners. pw_dist &lt;- function(ali, anno) { dist &lt;- as.matrix(dist.alignment(ali, matrix = &quot;similarity&quot;)) #https://www.researchgate.net/post/Homology_similarity_and_identity-can_anyone_help_with_these_terms # dist.alignment: matrix contains the squared root of the pairwise distances. For example, if identity between 2 sequences is 80 the squared root of (1.0 - 0.8) i.e. 0.4472136. dist &lt;- dist^2 dist &lt;- 1 - dist # dist is now &#39;%&#39; identity, 1=100 % dist &lt;- melt(dist) # Remove irrelevant entries, i.e. self-comparison and similarities below the threshold, this step now speeds up later steps, but not mandatory dist &lt;- dist[dist$value &gt;= 0.97, ] dist &lt;- dist[!dist$value == 1, ] # This removes AB - BA duplicates cols &lt;- c(&quot;Var1&quot;, &quot;Var2&quot;) newdf &lt;- dist[, cols] for (i in 1:nrow(newdf)) { newdf[i, ] = sort(newdf[i, cols]) } newdf &lt;- newdf[!duplicated(newdf), ] # add back similarity values to the remaining comparisons/pairs dist &lt;- left_join(newdf, dist, by = c(Var1 = &quot;Var1&quot;, Var2 = &quot;Var2&quot;)) # annotate OTUs anno[&quot;OTU_num&quot;] &lt;- str_replace(anno$XOTU, &quot;X&quot;, &quot;&quot;) anno &lt;- anno[, c(&quot;OTU_num&quot;, &quot;shallow_mean&quot;, &quot;deep_mean&quot;, &quot;ttest_pval&quot;, &quot;ttest_fdr&quot;)] anno[&quot;cat&quot;] &lt;- c(&quot;none&quot;) anno$cat[anno$ttest_fdr &lt; 0.05 &amp; anno$shallow_mean &gt; anno$deep_mean] &lt;- c(&quot;shallow&quot;) # can be set to &#39;ttest_pval&#39; instead anno$cat[anno$ttest_fdr &lt; 0.05 &amp; anno$shallow_mean &lt; anno$deep_mean] &lt;- c(&quot;deep&quot;) # can be set to &#39;ttest_pval&#39; instead anno &lt;- anno[, c(&quot;OTU_num&quot;, &quot;cat&quot;)] anno$OTU_num &lt;- as.numeric(anno$OTU_num) colnames(anno) &lt;- c(&quot;Var1&quot;, &quot;Var1_cat&quot;) dist &lt;- left_join(dist, anno) colnames(anno) &lt;- c(&quot;Var2&quot;, &quot;Var2_cat&quot;) dist &lt;- left_join(dist, anno) # remove not significant comparisons dist &lt;- dist[!dist$Var1_cat == &quot;none&quot; &amp; !dist$Var2_cat == &quot;none&quot; &amp; !dist$Var1_cat == dist$Var2_cat, ] # order dist &lt;- dist[order(dist$Var1_cat, dist$Var2_cat), ] return(dist) } dist &lt;- pw_dist(ali, anno) # create csv file write.csv(dist, &#39;data/gb_similarity.csv&#39;, row.names = F) # GB write.csv(dist, &#39;data/sf_similarity.csv&#39;, row.names = F) # SF In W. bursa, none of the OTUs are # different (ttest_fdr &lt; 0.05) comparing shallow and deep specimens rm(ali, anno, dist) # Sister OTUs / Ecotypes dist_gb &lt;- read.csv(&quot;data/gb_similarity.csv&quot;) dist_gb$Var1 &lt;- str_replace(dist_gb$Var1, &quot;X196900&quot;, &quot;&quot;) dist_gb$Var2 &lt;- str_replace(dist_gb$Var2, &quot;X196900&quot;, &quot;&quot;) colnames(dist_gb) &lt;- c(&quot;OTU1&quot;, &quot;OTU2&quot;, &quot;sequence similarity&quot;, &quot;category OTU1&quot;, &quot;category OTU2&quot;) dist_gb ## OTU1 OTU2 sequence similarity category OTU1 category OTU2 ## 1 196900323 196900418 0.9857143 deep shallow ## 2 196900236 196900310 0.9714286 deep shallow ## 3 196900280 196900363 0.9928571 deep shallow ## 4 196900231 196900367 0.9928571 shallow deep ## 5 196900589 196900596 0.9928571 shallow deep ## 6 196900192 196900527 0.9857143 shallow deep ## 7 196900144 196900578 0.9714286 shallow deep ## 8 196900144 196900337 0.9928571 shallow deep ## 9 196900253 196900445 0.9928571 shallow deep ## 10 196900208 196900323 0.9857143 shallow deep ## 11 196900418 196900546 0.9714286 shallow deep ## 12 196900196 196900236 0.9928571 shallow deep ## 13 196900180 196900236 0.9785714 shallow deep ## 14 196900284 196900658 0.9928571 shallow deep ## 15 196900363 196900384 0.9857143 shallow deep ## 16 196900213 196900242 0.9928571 shallow deep ## 17 196900161 196900242 0.9857143 shallow deep ## 18 196900307 196900537 0.9785714 shallow deep ## 19 196900331 196900528 0.9857143 shallow deep dist_sf &lt;- read.csv(&quot;data/sf_similarity.csv&quot;) dist_sf$Var1 &lt;- str_replace(dist_sf$Var1, &quot;X196900&quot;, &quot;&quot;) dist_sf$Var2 &lt;- str_replace(dist_sf$Var2, &quot;X196900&quot;, &quot;&quot;) colnames(dist_sf) &lt;- c(&quot;OTU1&quot;, &quot;OTU2&quot;, &quot;sequence similarity&quot;, &quot;category OTU1&quot;, &quot;category OTU2&quot;) dist_sf ## OTU1 OTU2 sequence similarity category OTU1 category OTU2 ## 1 196900137 196900337 0.9785714 shallow deep ## 2 196900137 196900498 0.9714286 shallow deep In the G. barretti prokaryotic community, there are 19 pairs of OTUs with a sequence similarity \\(\\geq\\) 97% and difference in abundance above/below 1000 m (p FDR \\(\\leq\\) 0.05). In the S. fortis prokaryotic community, there are 2 pairs of OTUs with a sequence similarity \\(\\geq\\) 97% and difference in abundance above/below 1000 m (p FDR \\(\\leq\\) 0.05). In the W. bursa prokaryotic community, none of the members have a significant different abundance in the two water masses. We interpret these instances as ecotypes, that is, sister OTUs (closely related OTUs) adapted/affiliated to the two different water masses. ### For Ecotype/ sister OTU discussion gb_OTUs &lt;- read.csv(&quot;data/gb_OTUs_overall_rabdc_annotated.csv&quot;, header=T, sep=&quot;,&quot;) sf_OTUs &lt;- read.csv(&quot;data/sf_OTUs_overall_rabdc_annotated.csv&quot;, header=T, sep=&quot;,&quot;) rownames(gb_OTUs) &lt;- gb_OTUs$XOTU gb_OTUs[&quot;OTU_num&quot;] &lt;- str_replace(gb_OTUs$XOTU,&quot;X196900&quot;, &quot;&quot;) rownames(sf_OTUs) &lt;- sf_OTUs$XOTU sf_OTUs[&quot;OTU_num&quot;] &lt;- str_replace(sf_OTUs$XOTU,&quot;X196900&quot;, &quot;&quot;) #gb_sis &lt;- read.csv(&quot;data/gb_similarity.csv&quot;, header = T, sep=&quot;,&quot;) #sf_sis &lt;- read.csv(&quot;data/sf_similarity.csv&quot;, header = T, sep=&quot;,&quot;) #unique(c(gb_sis$Var1, gb_sis$Var2)) %&gt;% # str_replace(&quot;196900&quot;,&quot;&quot;) # 31 OTUs: &quot;323&quot; &quot;236&quot; &quot;280&quot; &quot;231&quot; &quot;589&quot; &quot;192&quot; &quot;144&quot; &quot;253&quot; &quot;208&quot; &quot;418&quot; &quot;196&quot; &quot;180&quot; &quot;284&quot; &quot;363&quot; &quot;213&quot; &quot;161&quot; &quot;307&quot; &quot;331&quot; &quot;310&quot; &quot;367&quot; #&quot;596&quot; &quot;527&quot; &quot;578&quot; &quot;337&quot; &quot;445&quot; &quot;546&quot; &quot;658&quot; &quot;384&quot; &quot;242&quot; &quot;537&quot; &quot;528&quot; #unique(c(sf_sis$Var1, sf_sis$Var2)) %&gt;% # str_replace(&quot;196900&quot;,&quot;&quot;) # 3 OTUs: &quot;137&quot; &quot;337&quot; &quot;498&quot; gb_sis &lt;- read.csv(&quot;data/gb_similarity.csv&quot;, header = T, sep=&quot;,&quot;) gb_sis$Var1 &lt;- c(str_replace(gb_sis$Var1, &quot;196900&quot;, &quot;&quot;)) gb_sis$Var2 &lt;- c(str_replace(gb_sis$Var2, &quot;196900&quot;, &quot;&quot;)) OTU_num_gb &lt;- data.frame(unique(c(gb_sis$Var1, gb_sis$Var2))) colnames(OTU_num_gb) &lt;- c(&quot;sis&quot;) OTU_num_gb[&quot;sis_gb&quot;] &lt;- c(&quot;gb&quot;) gb_sis$X &lt;- NULL sf_sis &lt;- read.csv(&quot;data/sf_similarity.csv&quot;, header = T, sep=&quot;,&quot;) sf_sis$Var1 &lt;- c(str_replace(sf_sis$Var1, &quot;196900&quot;, &quot;&quot;)) sf_sis$Var2 &lt;- c(str_replace(sf_sis$Var2, &quot;196900&quot;, &quot;&quot;)) OTU_num_sf &lt;- data.frame(unique(c(sf_sis$Var1, sf_sis$Var2))) colnames(OTU_num_sf) &lt;- c(&quot;sis&quot;) OTU_num_sf[&quot;sis_sf&quot;] &lt;- c(&quot;sf&quot;) sf_sis$X &lt;- NULL sisters &lt;- full_join(OTU_num_gb, OTU_num_sf) sisters[&quot;combined&quot;] &lt;- c(&quot;both&quot;) sisters$combined[is.na(sisters$sis_sf)] &lt;- c(&quot;gb&quot;) sisters$combined[is.na(sisters$sis_gb)] &lt;- c(&quot;sf&quot;) gb_sis &lt;- left_join(sisters, gb_OTUs, by=c(&quot;sis&quot;=&quot;OTU_num&quot;)) gb_sis_m &lt;- gb_sis %&gt;% #instead of reshape2::melt pivot_longer( col=starts_with(&quot;Gb&quot;) ) gb_sis_m[&quot;order&quot;] &lt;- as.numeric(str_replace(gb_sis_m$name, &quot;Gb&quot;, &quot;&quot;)) ggplot(gb_sis_m, aes(x=order, y=value, col=combined))+geom_point()+facet_wrap(~sis, scales = &quot;free&quot;, ncol=4)+ggtitle(&quot;Gb sister OTUs&quot;)+geom_vline(xintercept = 10.5)+theme(legend.position = &quot;bottom&quot;) Figure 3.13: Relative abundance of all sister OTUs shown for G. barretti and S.fortis. sf_sis &lt;- left_join(sisters, sf_OTUs, by=c(&quot;sis&quot;=&quot;OTU_num&quot;)) sf_sis_m &lt;- sf_sis %&gt;% #instead of reshape2::melt pivot_longer( col=starts_with(&quot;Sf&quot;) ) sf_sis_m[&quot;order&quot;] &lt;- as.numeric(str_replace(sf_sis_m$name, &quot;Sf&quot;, &quot;&quot;)) ggplot(sf_sis_m, aes(x=order, y=value, col=combined))+geom_point()+facet_wrap(~sis, scales = &quot;free&quot;, ncol=4)+ggtitle(&quot;Sf sister OTUs&quot;)+geom_vline(xintercept = 8.5)+theme(legend.position = &quot;bottom&quot;) Figure 3.14: Relative abundance of all sister OTUs shown for G. barretti and S.fortis. It is important to note that only the category “both” and the respective sponge host (gb or sf) are relevant for each plot. The OTUs classified as sister OTUs in the other sponge are added for comparison only. This is meant to allow comparison as an OTU might be increasing in one sponge and decreasing in the other as it occurrs e.g. in OTU192 and potentially also OTU208. It also illustrates the difficulties of adequately assigning and testing for ecotypes. Which of these OTUs make up the pairs of correspnding ecotypes is displayed in the tables below for G. barretti and S. fortis respectively. ### ====================== Taxonomy table ================================ taxonomy &lt;- read.csv(&quot;data/microbiome_taxonomy.csv&quot;, header = T, sep = &quot;;&quot;) taxonomy[&quot;OTU_num&quot;] &lt;- as.numeric(str_replace(taxonomy$OTU_ID, &quot;196900&quot;, &quot;&quot;)) taxonomy &lt;- taxonomy[, c(&quot;OTU_num&quot;, &quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;)] OTU_num_gb$sis_gb &lt;- NULL colnames(OTU_num_gb) &lt;- c(&quot;OTU_num&quot;) OTU_num_gb$OTU_num &lt;- as.numeric(as.character(OTU_num_gb$OTU_num)) gb_sis_tax &lt;- left_join(OTU_num_gb, taxonomy) gb_sis_tax ## OTU_num Kingdom Phylum Class ## 1 323 Bacteria Acidobacteria Subgroup_6 ## 2 236 Bacteria Proteobacteria JTB23 ## 3 280 Bacteria Gemmatimonadetes BD2-11_terrestrial_group ## 4 231 Bacteria Chloroflexi SAR202_clade ## 5 589 Bacteria Chloroflexi SAR202_clade ## 6 192 Bacteria Chloroflexi TK10 ## 7 144 Bacteria Acidobacteria Subgroup_9 ## 8 253 Bacteria Acidobacteria Subgroup_15 ## 9 208 Bacteria Acidobacteria Subgroup_6 ## 10 418 Bacteria Acidobacteria Subgroup_6 ## 11 196 Bacteria Proteobacteria JTB23 ## 12 180 Bacteria Proteobacteria JTB23 ## 13 284 Bacteria Acidobacteria Subgroup_26 ## 14 363 Bacteria Gemmatimonadetes BD2-11_terrestrial_group ## 15 213 Bacteria Gemmatimonadetes BD2-11_terrestrial_group ## 16 161 Bacteria Gemmatimonadetes BD2-11_terrestrial_group ## 17 307 &lt;NA&gt; ## 18 331 Bacteria Chloroflexi Anaerolineae ## 19 310 Bacteria Proteobacteria JTB23 ## 20 367 Bacteria Chloroflexi SAR202_clade ## 21 596 Bacteria Chloroflexi SAR202_clade ## 22 527 Bacteria Chloroflexi TK10 ## 23 578 Bacteria Acidobacteria Subgroup_9 ## 24 337 Bacteria Acidobacteria Subgroup_9 ## 25 445 Bacteria Acidobacteria Subgroup_15 ## 26 546 Bacteria Acidobacteria Subgroup_6 ## 27 658 Bacteria Acidobacteria Subgroup_26 ## 28 384 Bacteria Gemmatimonadetes BD2-11_terrestrial_group ## 29 242 Bacteria Gemmatimonadetes BD2-11_terrestrial_group ## 30 537 &lt;NA&gt; ## 31 528 Bacteria Chloroflexi Anaerolineae OTU_num_sf$sis_sf &lt;- NULL colnames(OTU_num_sf) &lt;- c(&quot;OTU_num&quot;) OTU_num_sf$OTU_num &lt;- as.numeric(as.character(OTU_num_sf$OTU_num)) sf_sis_tax &lt;- left_join(OTU_num_sf, taxonomy) sf_sis_tax ## OTU_num Kingdom Phylum Class ## 1 137 Bacteria Acidobacteria Subgroup_9 ## 2 337 Bacteria Acidobacteria Subgroup_9 ## 3 498 Bacteria Acidobacteria Subgroup_9 3.8 Annotated phylogeny To combinethe different categories and classifications with the OTUs, we annotated a maximum likelihood phylogeny with the depth response, SpongeEMP enrichment and barettin correlation. The tree can also be accessed at iTol under iTOL login karin_steffen. # $ grep &#39;&gt;&#39; gb_420_mafft.fasta | cut -d &#39; &#39; -f 1 | sed &#39;s/&gt;//g&#39; &gt; gb_420_mafft_OTU.csv leaves &lt;- read.csv(&quot;data/gb_420_mafft_OTU.csv&quot;, header = F, sep = &quot;,&quot;) # leaves$V1[leaves$V1==4] &lt;- c(&#39;004&#39;) leaves$V1[leaves$V1==97] &lt;- c(&#39;097&#39;) leaves[&quot;labels&quot;] &lt;- paste0(&quot;OTU&quot;, leaves$V1) # Labels write.csv(leaves, &#39;data/leave_labels.csv&#39;, row.names = F) # Then open in text editor and select the apropriate separator, i.e. &#39;SEPARATOR COMMA&#39; followed by &#39;DATA&#39; # Color range taxonomy &lt;- read.csv(&quot;data/microbiome_taxonomy.csv&quot;, header = T, sep = &quot;;&quot;) taxonomy[] &lt;- lapply(taxonomy, str_trim) taxonomy[&quot;labels&quot;] &lt;- str_replace(taxonomy$OTU_ID, &quot;196900&quot;, &quot;OTU&quot;) taxonomy$Phylum[taxonomy$Phylum == &quot;&quot;] &lt;- c(&quot;unclassified&quot;) leaves &lt;- left_join(leaves, taxonomy) unique(leaves$Phylum) phyla_cols &lt;- read.csv(&quot;data/phyla-hex-colour.csv&quot;, header = T, sep = &quot;;&quot;) phyla_cols[] &lt;- lapply(phyla_cols, str_trim) leaves &lt;- left_join(leaves, phyla_cols) leaves &lt;- leaves[, c(&quot;V1&quot;, &quot;hex&quot;)] leaves[&quot;label&quot;] &lt;- c(&quot;label&quot;) leaves &lt;- leaves[, c(1, 3, 2)] head(leaves) # Label color write.csv(leaves, &#39;data/leave_label-color.csv&#39;, row.names = F) # Then open in text editor and select the apropriate separator, i.e. &#39;SEPARATOR COMMA&#39; followed by # &#39;DATA&#39; # Lines for sister OTUs/ecotypes gb_sis &lt;- read.csv(&quot;data/gb_similarity.csv&quot;, header = T, sep = &quot;,&quot;) gb_sis$Var1 &lt;- c(str_replace(gb_sis$Var1, &quot;196900&quot;, &quot;&quot;)) gb_sis$Var2 &lt;- c(str_replace(gb_sis$Var2, &quot;196900&quot;, &quot;&quot;)) # For binary # depth gb_bin &lt;- read.csv(&quot;data/gb_OTUs_overall_rabdc_annotated.csv&quot;, header = T, sep = &quot;,&quot;) head(gb_bin) colnames(gb_bin) gb_bin[&quot;ttest.fdr&quot;] &lt;- c(-1) gb_bin$ttest.fdr[gb_bin$ttest_fdr &lt; 0.05 &amp; gb_bin$shallow_mean &gt; gb_bin$deep_mean] &lt;- c(1) # shallow gb_bin$ttest.fdr[gb_bin$ttest_fdr &lt; 0.05 &amp; gb_bin$shallow_mean &lt; gb_bin$deep_mean] &lt;- c(0) # deep gb_bin[&quot;cortest.p&quot;] &lt;- c(-1) gb_bin$cortest.p[gb_bin$classification == &quot;decreasing&quot;] &lt;- c(1) # shallow gb_bin$cortest.p[gb_bin$classification == &quot;increasing&quot;] &lt;- c(0) # deep gb_bin &lt;- gb_bin[, c(&quot;XOTU&quot;, &quot;ttest.fdr&quot;, &quot;cortest.p&quot;)] gb_bin[&quot;OTU&quot;] &lt;- str_replace(gb_bin$XOTU, &quot;X196900&quot;, &quot;&quot;) # emp emp &lt;- read.csv(&quot;data/SpongeEMP.csv&quot;, header = T, sep = &quot;;&quot;) emp[&quot;OTU&quot;] &lt;- str_replace(emp$OTU_ID, &quot;OTU&quot;, &quot;&quot;) emp[&quot;emp&quot;] &lt;- -1 emp$emp[emp$spongeEMP_enriched == &quot;yes&quot;] &lt;- 1 emp$emp[emp$spongeEMP_enriched == &quot;no&quot;] &lt;- 0 emp &lt;- emp[emp$sponge == &quot;Gb&quot;, ] emp[, c(&quot;OTU_ID&quot;, &quot;spongeEMP_enriched&quot;, &quot;sponge&quot;)] &lt;- list(NULL) str(emp) str(gb_bin) dim(emp) dim(gb_bin) gb_bin &lt;- left_join(gb_bin, emp) emp$OTU gb_bin$XOTU &lt;- NULL gb_bin &lt;- gb_bin[, c(3, 1, 2, 4)] # barettin correlation bar_cor &lt;- read.csv(&quot;data/GB_OTU_barettin_correlation.csv&quot;, head = T, sep = &quot;,&quot;) # negative, and not significant correlations need to be removed Summary Pearson pmcc &gt; 0 &amp; p &lt; 0.05 56 fdr &lt; 0.05 22 Spearman rho &gt; 0 &amp; p &lt; 0.05 42 fdr &lt; 0.05 1 # bar_cor %&gt;% filter(barettin_estimate_P&gt;0) %&gt;% filter(barettin_p_val_P&lt;0.05) bar_cor[&quot;OTU&quot;] &lt;- str_replace(bar_cor$XOTU, &quot;X196900&quot;, &quot;&quot;) bar_cor[&quot;bar_cat&quot;] &lt;- -1 bar_cor$bar_cat[bar_cor$barettin_estimate_P &lt; 0] &lt;- 0 bar_cor$bar_cat[bar_cor$barettin_estimate_P &gt; 0 &amp; bar_cor$barettin_p_val_P &gt; 0.05] &lt;- 0 bar_cor$bar_cat[bar_cor$barettin_estimate_P &gt; 0 &amp; bar_cor$barettin_p_val_P &lt; 0.05] &lt;- 1 bar_cor &lt;- bar_cor[, c(&quot;OTU&quot;, &quot;bar_cat&quot;)] gb_bin &lt;- left_join(gb_bin, bar_cor) head(gb_bin) # write.csv(gb_bin, &#39;data/leaf_bin.csv&#39;, row.names = F) knitr::include_graphics(&quot;data/annotated_tree.png&quot;) Figure 3.15: Annotated ML phylogeny. library(tidyverse) library(vegan) library(kableExtra) ############# FUNCTIONS # # Prepare dataframes aka &#39;data gymnastics&#39;. The output is a list with the three data set per experiment for the three different species gb, sf, wb gymnastics &lt;- function(df, my_colnames) { df[, c(&quot;df.1&quot;, &quot;df&quot;, &quot;mz&quot;, &quot;mzmin&quot;, &quot;mzmadf&quot;, &quot;rt&quot;, &quot;rtmin&quot;, &quot;rtmadf&quot;, &quot;npeaks&quot;, &quot;Gb&quot;, &quot;QC&quot;, &quot;Sf&quot;, &quot;Wb&quot;)] &lt;- list(NULL) df[, c(&quot;isotopes&quot;, &quot;adduct&quot;, &quot;pcgroup&quot;, &quot;SD&quot;, &quot;MEAN&quot;, &quot;CV&quot;, &quot;fss&quot;, &quot;df.M.H..&quot;, &quot;raw_peaks.pcgroup&quot;)] &lt;- list(NULL) df &lt;- data.frame(t(df)) df[&quot;ID&quot;] &lt;- rownames(df) df[&quot;unified_ID&quot;] &lt;- masslynx$unified_ID[match(df$ID, masslynx[[my_colnames]])] df &lt;- na.omit(df) df &lt;- df[order(df$unified_ID), ] rownames(df) &lt;- df$unified_ID df[&quot;spec&quot;] &lt;- str_sub(df$unified_ID, 1, 2) df &lt;- df[!df$spec == &quot;QC&quot;, ] df[, c(&quot;ID&quot;, &quot;unified_ID&quot;)] &lt;- list(NULL) df_gb &lt;- df[df$spec == &quot;Gb&quot;, ] df_sf &lt;- df[df$spec == &quot;Sf&quot;, ] df_wb &lt;- df[df$spec == &quot;Wb&quot;, ] df_gb$spec &lt;- NULL df_sf$spec &lt;- NULL df_wb$spec &lt;- NULL # ready for mantel metabolome &lt;- list(gb = df_gb, sf = df_sf, wb = df_wb) return(metabolome) } # Prepares the microbiome data sets to match the metabolomes. The output is a list with all microbiomes and metabolomes congruency &lt;- function(metabolomes) { metabolome_gb &lt;- metabolomes$gb metabolome_sf &lt;- metabolomes$sf metabolome_wb &lt;- metabolomes$wb micro_gb &lt;- micro[micro$spec == &quot;Gb&quot;, ] micro_sf &lt;- micro[micro$spec == &quot;Sf&quot;, ] micro_wb &lt;- micro[micro$spec == &quot;Wb&quot;, ] micro_gb[, c(&quot;spec&quot;, &quot;unified_ID&quot;)] &lt;- list(NULL) micro_sf[, c(&quot;spec&quot;, &quot;unified_ID&quot;)] &lt;- list(NULL) micro_wb[, c(&quot;spec&quot;, &quot;unified_ID&quot;)] &lt;- list(NULL) micro_gb &lt;- micro_gb[, colSums(micro_gb != 0) &gt; 0] micro_sf &lt;- micro_sf[, colSums(micro_sf != 0) &gt; 0] micro_wb &lt;- micro_wb[, colSums(micro_wb != 0) &gt; 0] micro_gb &lt;- sqrt(micro_gb) micro_sf &lt;- sqrt(micro_sf) micro_wb &lt;- sqrt(micro_wb) micro_gb &lt;- wisconsin(micro_gb) micro_sf &lt;- wisconsin(micro_sf) micro_wb &lt;- wisconsin(micro_wb) micro_gb &lt;- micro_gb[rownames(micro_gb) %in% rownames(metabolome_gb), ] metabolome_gb &lt;- metabolome_gb[rownames(metabolome_gb) %in% rownames(micro_gb), ] all(rownames(micro_gb) == rownames(metabolome_gb)) micro_sf &lt;- micro_sf[rownames(micro_sf) %in% rownames(metabolome_sf), ] metabolome_sf &lt;- metabolome_sf[rownames(metabolome_sf) %in% rownames(micro_sf), ] all(rownames(micro_sf) == rownames(metabolome_sf)) micro_wb &lt;- micro_wb[rownames(micro_wb) %in% rownames(metabolome_wb), ] metabolome_wb &lt;- metabolome_wb[rownames(metabolome_wb) %in% rownames(micro_wb), ] all(rownames(micro_wb) == rownames(metabolome_wb)) congruent_dfs &lt;- list(micro_gb = micro_gb, micro_sf = micro_sf, micro_wb = micro_wb, metabolome_gb = metabolome_gb, metabolome_sf = metabolome_sf, metabolome_wb = metabolome_wb) return(congruent_dfs) } # MANTEL TEST Generates distance matrices &#39;Bray-Curtis&#39; for microbiome, scaled &#39;euclidean&#39; for metabolomes, runs a Mantel test with Spearman correlation, and saves the parameters # to a data frame. cloak &lt;- function(congruent_dfs, experiment, filtering) { gb_meta_dist &lt;- vegdist(scale(congruent_dfs$metabolome_gb), &quot;euclid&quot;) micro_gb_dist &lt;- vegdist(congruent_dfs$micro_gb, method = &quot;bray&quot;) sf_meta_dist &lt;- vegdist(scale(congruent_dfs$metabolome_sf), &quot;euclid&quot;) micro_sf_dist &lt;- vegdist(congruent_dfs$micro_sf, method = &quot;bray&quot;) wb_meta_dist &lt;- vegdist(scale(congruent_dfs$metabolome_wb), &quot;euclid&quot;) micro_wb_dist &lt;- vegdist(congruent_dfs$micro_wb, method = &quot;bray&quot;) m_gb &lt;- mantel(micro_gb_dist, gb_meta_dist, method = &quot;spearman&quot;) m_sf &lt;- mantel(micro_sf_dist, sf_meta_dist, method = &quot;spearman&quot;) m_wb &lt;- mantel(micro_wb_dist, wb_meta_dist, method = &quot;spearman&quot;) micro_dim &lt;- as.data.frame(t(dim(congruent_dfs$micro_gb))) micro_dim &lt;- rbind(micro_dim, as.data.frame(t(dim(congruent_dfs$micro_sf)))) micro_dim &lt;- rbind(micro_dim, as.data.frame(t(dim(congruent_dfs$micro_wb)))) colnames(micro_dim) &lt;- c(&quot;micro_samples&quot;, &quot;micro_OTUs&quot;) meta_dim &lt;- as.data.frame(t(dim(congruent_dfs$metabolome_gb))) meta_dim &lt;- rbind(meta_dim, as.data.frame(t(dim(congruent_dfs$metabolome_sf)))) meta_dim &lt;- rbind(meta_dim, as.data.frame(t(dim(congruent_dfs$metabolome_wb)))) colnames(meta_dim) &lt;- c(&quot;meta_samples&quot;, &quot;meta_features&quot;) stats &lt;- data.frame(m_gb$statistic) stats &lt;- rbind(stats, m_sf$statistic) stats &lt;- rbind(stats, m_wb$statistic) signif &lt;- data.frame(m_gb$signif) signif &lt;- rbind(signif, m_sf$signif) signif &lt;- rbind(signif, m_wb$signif) colnames(stats) &lt;- c(&quot;statistic&quot;) stats[&quot;signif&quot;] &lt;- signif stats[&quot;Sponge species&quot;] &lt;- c(&quot;Geodia barretti&quot;, &quot;Stryphnus fortis&quot;, &quot;Weberella bursa&quot;) stats &lt;- bind_cols(stats, micro_dim) stats &lt;- bind_cols(stats, meta_dim) stats[&quot;data set&quot;] &lt;- filtering stats[&quot;Experiment&quot;] &lt;- experiment return(stats) } # PROTEST NMDS ordination and protest ordination &lt;- function(congruent_dfs, experiment, filtering) { # If you do not have community data, you should probably set autotransform = FALSE. k: number of dimensions gb_meta_mds &lt;- metaMDS(congruent_dfs$metabolome_gb, trymax = 100, distance = &quot;euclid&quot;, autotransforme = F) micro_gb_mds &lt;- metaMDS(congruent_dfs$micro_gb, trymax = 100, distance = &quot;bray&quot;) sf_meta_mds &lt;- metaMDS(congruent_dfs$metabolome_sf, trymax = 100, distance = &quot;euclid&quot;, autotransforme = F) micro_sf_mds &lt;- metaMDS(congruent_dfs$micro_sf, trymax = 100, distance = &quot;bray&quot;) wb_meta_mds &lt;- metaMDS(congruent_dfs$metabolome_wb, trymax = 100, distance = &quot;euclid&quot;, autotransforme = F) micro_wb_mds &lt;- metaMDS(congruent_dfs$micro_wb, trymax = 100, distance = &quot;bray&quot;) gb_proc &lt;- procrustes(micro_gb_mds, gb_meta_mds, scores = &quot;sites&quot;) gb_prot &lt;- protest(micro_gb_mds, gb_meta_mds, scores = &quot;sites&quot;) sf_proc &lt;- procrustes(micro_sf_mds, sf_meta_mds, scores = &quot;sites&quot;) sf_prot &lt;- protest(micro_sf_mds, sf_meta_mds, scores = &quot;sites&quot;) wb_proc &lt;- procrustes(micro_wb_mds, wb_meta_mds, scores = &quot;sites&quot;) wb_prot &lt;- protest(micro_wb_mds, wb_meta_mds, scores = &quot;sites&quot;) micro_dim &lt;- as.data.frame(t(dim(congruent_dfs$micro_gb))) micro_dim &lt;- rbind(micro_dim, as.data.frame(t(dim(congruent_dfs$micro_sf)))) micro_dim &lt;- rbind(micro_dim, as.data.frame(t(dim(congruent_dfs$micro_wb)))) colnames(micro_dim) &lt;- c(&quot;micro_samples&quot;, &quot;micro_OTUs&quot;) meta_dim &lt;- as.data.frame(t(dim(congruent_dfs$metabolome_gb))) meta_dim &lt;- rbind(meta_dim, as.data.frame(t(dim(congruent_dfs$metabolome_sf)))) meta_dim &lt;- rbind(meta_dim, as.data.frame(t(dim(congruent_dfs$metabolome_wb)))) colnames(meta_dim) &lt;- c(&quot;meta_samples&quot;, &quot;meta_features&quot;) pss &lt;- data.frame(gb_prot$ss) pss &lt;- rbind(pss, sf_prot$ss) pss &lt;- rbind(pss, wb_prot$ss) colnames(pss) &lt;- c(&quot;Procrustes SS&quot;) cor &lt;- data.frame(gb_prot$scale) cor &lt;- rbind(cor, sf_prot$scale) cor &lt;- rbind(cor, wb_prot$scale) colnames(cor) &lt;- c(&quot;correlation in sym. rotation&quot;) signif &lt;- data.frame(gb_prot$signif) signif &lt;- rbind(signif, sf_prot$signif) signif &lt;- rbind(signif, wb_prot$signif) colnames(signif) &lt;- c(&quot;signif&quot;) stats &lt;- bind_cols(pss, cor, signif, micro_dim, meta_dim) stats[&quot;data set&quot;] &lt;- filtering stats[&quot;Experiment&quot;] &lt;- experiment stats[&quot;Sponge species&quot;] &lt;- c(&quot;Geodia barretti&quot;, &quot;Stryphnus fortis&quot;, &quot;Weberella bursa&quot;) return(stats) } References "],
["inter-omics.html", "4 Inter-omics 4.1 Mantel test and procrustes rotations 4.2 Correlating barettin and prokaryotic relative abundance 4.3 Microbial interaction network 4.4 Shortlist", " 4 Inter-omics The inter-omics analyses are comprised of three parts. In the first part, we evaluate congruency of the prokaryotic and metabolomic data set as a whole using numerical methods (Mantel test) and ordination (Procrustes rotation and Protest). The second part consists of generating a microbial interaction network and annotating it with depth response of the OTUs and correlation with the barettin signal. In the third part, we rank OTUs based on properties hypothesised to be true for the producer of barettin. 4.1 Mantel test and procrustes rotations 4.1.1 Libraries and functions 4.1.2 Data sets # LOAD DATA micro &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) colnames(micro)[colnames(micro) == &quot;Sample_ID&quot;] &lt;- &quot;unified_ID&quot; micro &lt;- micro[order(micro$unified_ID), ] rownames(micro) &lt;- micro$unified_ID micro[&quot;spec&quot;] &lt;- str_sub(micro$unified_ID, 1, 2) meta_data &lt;- read.csv(&quot;data/PANGAEA_Final.csv&quot;, header = T, sep = &quot;;&quot;) masslynx &lt;- meta_data masslynx &lt;- masslynx[c(&quot;unified_ID&quot;, &quot;LC.MS.HILIC.positive&quot;, &quot;LC.MS.HILIC.negative&quot;, &quot;LC.MS.RP.positive&quot;, &quot;LC.MS.RP.negative&quot;)] colnames(masslynx) &lt;- c(&quot;unified_ID&quot;, &quot;H_p&quot;, &quot;H_n&quot;, &quot;R_p&quot;, &quot;R_n&quot;) masslynx &lt;- na.omit(masslynx) masslynx[&quot;HILIC_pos&quot;] &lt;- str_sub(masslynx$H_p, 1, -3) masslynx[&quot;HILIC_neg&quot;] &lt;- str_sub(masslynx$H_n, 1, -3) masslynx[&quot;RP_pos&quot;] &lt;- str_sub(masslynx$R_p, 1, -3) masslynx[&quot;RP_neg&quot;] &lt;- str_sub(masslynx$R_n, 1, -3) # load one set of experiments at a time, i.e. CLEANED, ION or PC_GROUPS ### CLEANED hilic_pos &lt;- read.csv(&quot;data/HILIC_pos_20190417_cleaned.csv&quot;, header = T, sep = &quot;,&quot;) hilic_neg &lt;- read.csv(&quot;data/HILIC_neg_20190421_cleaned.csv&quot;, header = T, sep = &quot;,&quot;) rp_pos &lt;- read.csv(&quot;data/RP_pos_20190421_cleaned.csv&quot;, header = T, sep = &quot;,&quot;) rp_neg &lt;- read.csv(&quot;data/RP_neg_20190422_cleaned.csv&quot;, header = T, sep = &quot;,&quot;) ### ION hilic_pos &lt;- read.csv(&quot;data/HILIC_pos_20190417_cleaned_MH.csv&quot;, header = T, sep = &quot;,&quot;) hilic_neg &lt;- read.csv(&quot;data/HILIC_neg_20190421_cleaned_MH.csv&quot;, header = T, sep = &quot;,&quot;) rp_pos &lt;- read.csv(&quot;data/RP_pos_20190421_cleaned_MH.csv&quot;, header = T, sep = &quot;,&quot;) rp_neg &lt;- read.csv(&quot;data/RP_neg_20190422_cleaned_MH.csv&quot;, header = T, sep = &quot;,&quot;) ### PC_GROUPS hilic_pos &lt;- read.csv(&quot;data/HILIC_pos_20190417_cleaned_pcgroup.csv&quot;, header = T, sep = &quot;,&quot;) hilic_neg &lt;- read.csv(&quot;data/HILIC_neg_20190421_cleaned_pcgroup.csv&quot;, header = T, sep = &quot;,&quot;) rp_pos &lt;- read.csv(&quot;data/RP_pos_20190421_cleaned_pcgroup.csv&quot;, header = T, sep = &quot;,&quot;) rp_neg &lt;- read.csv(&quot;data/RP_neg_20190422_cleaned_pcgroup.csv&quot;, header = T, sep = &quot;,&quot;) 4.1.3 Mantel test code and tabular output ### =============================== MANTEL TEST================================= # run one of this at a time metabolomes &lt;- gymnastics(hilic_pos, &quot;H_p&quot;) metabolomes &lt;- gymnastics(hilic_neg, &quot;H_n&quot;) metabolomes &lt;- gymnastics(rp_pos, &quot;R_p&quot;) metabolomes &lt;- gymnastics(rp_neg, &quot;R_n&quot;) # run this congruent_dfs &lt;- congruency(metabolomes) # run one of these: MANTEL TEST diagnostics_hp_cleaned &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;HILIC pos&quot;, filtering = &quot;cleaned&quot;) diagnostics_hn_cleaned &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;HILIC neg&quot;, filtering = &quot;cleaned&quot;) diagnostics_rp_cleaned &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;RP pos&quot;, filtering = &quot;cleaned&quot;) diagnostics_rn_cleaned &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;RP neg&quot;, filtering = &quot;cleaned&quot;) diagnostics_hp_ion &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;HILIC pos&quot;, filtering = &quot;ion&quot;) diagnostics_hn_ion &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;HILIC neg&quot;, filtering = &quot;ion&quot;) diagnostics_rp_ion &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;RP pos&quot;, filtering = &quot;ion&quot;) diagnostics_rn_ion &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;RP neg&quot;, filtering = &quot;ion&quot;) diagnostics_hp_pc_group &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;HILIC pos&quot;, filtering = &quot;pc_group&quot;) diagnostics_hn_pc_group &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;HILIC neg&quot;, filtering = &quot;pc_group&quot;) diagnostics_rp_pc_group &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;RP pos&quot;, filtering = &quot;pc_group&quot;) diagnostics_rn_pc_group &lt;- cloak(congruent_dfs = congruent_dfs, experiment = &quot;RP neg&quot;, filtering = &quot;pc_group&quot;) ### Combine all test results into one file diagnostics &lt;- diagnostics_hp_cleaned diagnostics &lt;- rbind(diagnostics, diagnostics_hn_cleaned, diagnostics_rp_cleaned, diagnostics_rn_cleaned, diagnostics_hp_pc_group, diagnostics_hn_pc_group, diagnostics_rp_pc_group, diagnostics_rn_pc_group, diagnostics_hp_ion, diagnostics_hn_ion, diagnostics_rp_ion, diagnostics_rn_ion) diagnostics write.csv(diagnostics, &quot;mantel_stats_FUN.csv&quot;) diagnostics &lt;- read.csv(&quot;data/mantel_stats_FUN.csv&quot;) diagnostics$X &lt;- NULL diagnostics &lt;- diagnostics[, c(&quot;statistic&quot;, &quot;signif&quot;, &quot;micro_samples&quot;, &quot;micro_OTUs&quot;, &quot;meta_samples&quot;, &quot;meta_features&quot;, &quot;Sponge.species&quot;, &quot;Experiment&quot;, &quot;data.set&quot;)] options(kableExtra.html.bsTable = T) kable(diagnostics, col.names = c(&quot;Mantel statistic r&quot;, &quot;significance&quot;, &quot;N microbiome samples&quot;, &quot;N OTUs&quot;, &quot;N metabolome samples&quot;, &quot;N features&quot;, &quot;Sponge species&quot;, &quot;Experiment&quot;, &quot;data set&quot;), longtable = T, booktabs = T, caption = &quot;Mantel test diagnostics diagnositcs comparing the microbiome and metabolome of the same sponge specimens&quot;, row.names = FALSE) %&gt;% add_header_above(c(Diagnostics = 6, `Data set attribution` = 3)) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), font_size = 12, full_width = F, latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;)) Table 4.1: Mantel test diagnostics diagnositcs comparing the microbiome and metabolome of the same sponge specimens Diagnostics Data set attribution Mantel statistic r significance N microbiome samples N OTUs N metabolome samples N features Sponge species Experiment data set 0.6076416 0.004 10 420 10 3507 Geodia barretti HILIC pos cleaned 0.3857788 0.011 13 461 13 3507 Stryphnus fortis HILIC pos cleaned 0.4576405 0.008 15 135 15 3507 Weberella bursa HILIC pos cleaned 0.4516469 0.006 10 420 10 2808 Geodia barretti HILIC neg cleaned 0.1513297 0.186 13 461 13 2808 Stryphnus fortis HILIC neg cleaned 0.1854966 0.168 15 135 15 2808 Weberella bursa HILIC neg cleaned -0.2014493 0.833 10 420 10 4673 Geodia barretti RP pos cleaned 0.3127632 0.064 13 461 13 4673 Stryphnus fortis RP pos cleaned 0.3323657 0.035 15 135 15 4673 Weberella bursa RP pos cleaned -0.2465894 0.845 9 420 9 3166 Geodia barretti RP neg cleaned 0.1960950 0.173 13 461 13 3166 Stryphnus fortis RP neg cleaned 0.1721750 0.174 15 135 15 3166 Weberella bursa RP neg cleaned 0.5836627 0.009 10 420 10 2212 Geodia barretti HILIC pos pc_group 0.4082626 0.016 13 461 13 2212 Stryphnus fortis HILIC pos pc_group 0.4483931 0.012 15 135 15 2212 Weberella bursa HILIC pos pc_group 0.3951252 0.018 10 420 10 1351 Geodia barretti HILIC neg pc_group 0.3211093 0.040 13 461 13 1351 Stryphnus fortis HILIC neg pc_group 0.0315675 0.414 15 135 15 1351 Weberella bursa HILIC neg pc_group -0.1600791 0.774 10 420 10 2736 Geodia barretti RP pos pc_group 0.3181249 0.064 13 461 13 2736 Stryphnus fortis RP pos pc_group 0.3008501 0.067 15 135 15 2736 Weberella bursa RP pos pc_group -0.1879022 0.790 9 420 9 1678 Geodia barretti RP neg pc_group 0.2034548 0.156 13 461 13 1678 Stryphnus fortis RP neg pc_group 0.1507257 0.198 15 135 15 1678 Weberella bursa RP neg pc_group 0.3351779 0.050 10 420 10 105 Geodia barretti HILIC pos ion 0.2908357 0.086 13 461 13 105 Stryphnus fortis HILIC pos ion 0.4683599 0.010 15 135 15 105 Weberella bursa HILIC pos ion 0.4429513 0.005 10 420 10 123 Geodia barretti HILIC neg ion 0.0727247 0.320 13 461 13 123 Stryphnus fortis HILIC neg ion 0.2320236 0.115 15 135 15 123 Weberella bursa HILIC neg ion -0.2569170 0.908 10 420 10 171 Geodia barretti RP pos ion 0.3070980 0.083 13 461 13 171 Stryphnus fortis RP pos ion 0.2656853 0.072 15 135 15 171 Weberella bursa RP pos ion -0.1220077 0.703 9 420 9 105 Geodia barretti RP neg ion -0.0505950 0.597 13 461 13 105 Stryphnus fortis RP neg ion 0.1218225 0.217 15 135 15 105 Weberella bursa RP neg ion dig &lt;- diagnostics[diagnostics$signif &lt;= 0.05, ] a &lt;- aggregate(dig, by = list(dig$Sponge.species, dig$Experiment, dig$data.set), FUN = &quot;length&quot;) summary(a$Group.1) ## Geodia barretti Stryphnus fortis Weberella bursa ## 6 3 4 summary(a$Group.2) ## HILIC neg HILIC pos RP neg RP pos ## 4 8 0 1 summary(a$Group.3) ## cleaned ion pc_group ## 5 3 5 rm(diagnostics_hp_cleaned, diagnostics_hn_cleaned, diagnostics_rp_cleaned, diagnostics_rn_cleaned, diagnostics_hp_ion, diagnostics_hn_ion, diagnostics_rp_ion, diagnostics_rn_ion, diagnostics_hp_pc_group, diagnostics_hn_pc_group, diagnostics_rp_pc_group, diagnostics_rn_pc_group) rm(hilic_pos, hilic_neg, rp_pos, rp_neg) As we can see from the table, in 13 cases, the Mantel test returns a significant correlation between the two matrices. Above you can see the the significant tests broken down by sponge species, HPCL-experiment and filtering approach. 4.1.4 Procrustes rotation and protest code and tabular output ### =============================== PROTEST ==================================== # run one of this at a time metabolomes &lt;- gymnastics(hilic_pos, &quot;H_p&quot;) metabolomes &lt;- gymnastics(hilic_neg, &quot;H_n&quot;) metabolomes &lt;- gymnastics(rp_pos, &quot;R_p&quot;) metabolomes &lt;- gymnastics(rp_neg, &quot;R_n&quot;) # run this congruent_dfs &lt;- congruency(metabolomes) # run one of these: PROTEST TEST diagnostics_hp_cleaned &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;HILIC pos&quot;, filtering = &quot;cleaned&quot;) diagnostics_hn_cleaned &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;HILIC neg&quot;, filtering = &quot;cleaned&quot;) diagnostics_rp_cleaned &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;RP pos&quot;, filtering = &quot;cleaned&quot;) diagnostics_rn_cleaned &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;RP neg&quot;, filtering = &quot;cleaned&quot;) diagnostics_hp_ion &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;HILIC pos&quot;, filtering = &quot;ion&quot;) diagnostics_hn_ion &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;HILIC neg&quot;, filtering = &quot;ion&quot;) diagnostics_rp_ion &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;RP pos&quot;, filtering = &quot;ion&quot;) diagnostics_rn_ion &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;RP neg&quot;, filtering = &quot;ion&quot;) diagnostics_hp_pc_group &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;HILIC pos&quot;, filtering = &quot;pc_group&quot;) diagnostics_hn_pc_group &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;HILIC neg&quot;, filtering = &quot;pc_group&quot;) diagnostics_rp_pc_group &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;RP pos&quot;, filtering = &quot;pc_group&quot;) diagnostics_rn_pc_group &lt;- ordination(congruent_dfs = congruent_dfs, experiment = &quot;RP neg&quot;, filtering = &quot;pc_group&quot;) ### Combine all test results into one file diagnostics &lt;- diagnostics_hp_cleaned diagnostics &lt;- rbind(diagnostics, diagnostics_hn_cleaned, diagnostics_rp_cleaned, diagnostics_rn_cleaned, diagnostics_hp_pc_group, diagnostics_hn_pc_group, diagnostics_rp_pc_group, diagnostics_rn_pc_group, diagnostics_hp_ion, diagnostics_hn_ion, diagnostics_rp_ion, diagnostics_rn_ion) diagnostics write.csv(diagnostics, &quot;protest_stats_FUN.csv&quot;) rm(diagnostics_hp_cleaned, diagnostics_hn_cleaned, diagnostics_rp_cleaned, diagnostics_rn_cleaned, diagnostics_hp_ion, diagnostics_hn_ion, diagnostics_rp_ion, diagnostics_rn_ion, diagnostics_hp_pc_group, diagnostics_hn_pc_group, diagnostics_rp_pc_group, diagnostics_rn_pc_group, diagnostics) rm(hilic_pos, hilic_neg, rp_pos, rp_neg) diagnostics &lt;- read.csv(&quot;data/protest_stats_FUN.csv&quot;) diagnostics$X &lt;- NULL diagnostics &lt;- diagnostics[, c(&quot;Procrustes.SS&quot;, &quot;correlation.in.sym..rotation&quot;, &quot;signif&quot;, &quot;micro_samples&quot;, &quot;micro_OTUs&quot;, &quot;meta_samples&quot;, &quot;meta_features&quot;, &quot;Sponge.species&quot;, &quot;Experiment&quot;, &quot;data.set&quot;)] options(kableExtra.html.bsTable = T) kable(diagnostics, col.names = c(&quot;Procrustes sum of squares&quot;, &quot;correlation in symmetric rotation&quot;, &quot;significance&quot;, &quot;N microbiome samples&quot;, &quot;N OTUs&quot;, &quot;N metabolome samples&quot;, &quot;N features&quot;, &quot;Sponge species&quot;, &quot;Experiment&quot;, &quot;data set&quot;), longtable = T, booktabs = T, caption = &quot;Protest diagnostics comparing the microbiome and metabolome of the same sponge specimens&quot;, row.names = FALSE) %&gt;% add_header_above(c(Diagnostics = 7, `Data set attribution` = 3)) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), font_size = 12, full_width = F, latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;)) Table 4.2: Protest diagnostics comparing the microbiome and metabolome of the same sponge specimens Diagnostics Data set attribution Procrustes sum of squares correlation in symmetric rotation significance N microbiome samples N OTUs N metabolome samples N features Sponge species Experiment data set 0.4901473 0.7140397 0.009 10 420 10 3507 Geodia barretti HILIC pos cleaned 0.3360196 0.8148499 0.001 13 461 13 3507 Stryphnus fortis HILIC pos cleaned 0.6451196 0.5957184 0.009 15 135 15 3507 Weberella bursa HILIC pos cleaned 0.5056719 0.7030847 0.015 10 420 10 2808 Geodia barretti HILIC neg cleaned 0.7144829 0.5343380 0.043 13 461 13 2808 Stryphnus fortis HILIC neg cleaned 0.6454847 0.5954119 0.003 15 135 15 2808 Weberella bursa HILIC neg cleaned 0.7241690 0.5251961 0.124 10 420 10 4673 Geodia barretti RP pos cleaned 0.5535784 0.6681479 0.005 13 461 13 4673 Stryphnus fortis RP pos cleaned 0.9089831 0.3016901 0.513 15 135 15 4673 Weberella bursa RP pos cleaned 0.7744081 0.4749652 0.229 9 420 9 3166 Geodia barretti RP neg cleaned 0.7769037 0.4723307 0.107 13 461 13 3166 Stryphnus fortis RP neg cleaned 0.9426954 0.2393838 0.656 15 135 15 3166 Weberella bursa RP neg cleaned 0.4903060 0.7139285 0.008 10 420 10 2212 Geodia barretti HILIC pos pc_group 0.6122646 0.6226840 0.009 13 461 13 2212 Stryphnus fortis HILIC pos pc_group 0.5611609 0.6624494 0.002 15 135 15 2212 Weberella bursa HILIC pos pc_group 0.4442919 0.7454583 0.007 10 420 10 1351 Geodia barretti HILIC neg pc_group 0.6072203 0.6267214 0.013 13 461 13 1351 Stryphnus fortis HILIC neg pc_group 0.7647882 0.4849864 0.062 15 135 15 1351 Weberella bursa HILIC neg pc_group 0.7332277 0.5165001 0.146 10 420 10 2736 Geodia barretti RP pos pc_group 0.5557670 0.6665081 0.002 13 461 13 2736 Stryphnus fortis RP pos pc_group 0.9184545 0.2855616 0.579 15 135 15 2736 Weberella bursa RP pos pc_group 0.7952845 0.4524550 0.314 9 420 9 1678 Geodia barretti RP neg pc_group 0.7513407 0.4986575 0.089 13 461 13 1678 Stryphnus fortis RP neg pc_group 0.9669093 0.1819085 0.851 15 135 15 1678 Weberella bursa RP neg pc_group 0.7744285 0.4749437 0.254 9 420 9 3166 Geodia barretti HILIC pos ion 0.7768298 0.4724090 0.119 13 461 13 3166 Stryphnus fortis HILIC pos ion 0.9427350 0.2393010 0.671 15 135 15 3166 Weberella bursa HILIC pos ion 0.3977778 0.7760297 0.005 10 420 10 123 Geodia barretti HILIC neg ion 0.7620196 0.4878323 0.101 13 461 13 123 Stryphnus fortis HILIC neg ion 0.6263664 0.6112558 0.007 15 135 15 123 Weberella bursa HILIC neg ion 0.6879735 0.5585933 0.083 10 420 10 171 Geodia barretti RP pos ion 0.6735153 0.5713884 0.038 13 461 13 171 Stryphnus fortis RP pos ion 0.9830213 0.1303022 0.961 15 135 15 171 Weberella bursa RP pos ion 0.7788793 0.4702347 0.271 9 420 9 105 Geodia barretti RP neg ion 0.9754054 0.1568265 0.767 13 461 13 105 Stryphnus fortis RP neg ion 0.9595716 0.2010681 0.791 15 135 15 105 Weberella bursa RP neg ion dig &lt;- diagnostics[diagnostics$signif &lt;= 0.05, ] a &lt;- aggregate(dig, by = list(dig$Sponge.species, dig$Experiment, dig$data.set), FUN = &quot;length&quot;) summary(a$Group.1) ## Geodia barretti Stryphnus fortis Weberella bursa ## 5 7 4 summary(a$Group.2) ## HILIC neg HILIC pos RP neg RP pos ## 7 6 0 3 summary(a$Group.3) ## cleaned ion pc_group ## 7 3 6 Out of the 36 tests performed, 16 are significant (p \\(\\leq\\) 0.05). Immediately above you can see the the significant tests broken down by sponge species, HPCL-experiment and filtering approach. 4.2 Correlating barettin and prokaryotic relative abundance cmp &lt;- read.csv(&quot;data/metabolite_master_20190605.csv&quot;, header = T, sep = &quot;,&quot;) cmp$X &lt;- NULL meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) micro &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) # preparing meta data meta_data_prep &lt;- function(meta_data) { meta_data &lt;- meta_data[, c(&quot;unified_ID&quot;, &quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;MeanBottomTemp_Cdeg&quot;, &quot;MeanBotSalinity_PSU&quot;, &quot;YEAR&quot;)] colnames(meta_data) &lt;- c(&quot;unified_ID&quot;, &quot;Depth&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;Temperature&quot;, &quot;Salinity&quot;, &quot;Year&quot;) meta_data &lt;- meta_data[!(str_sub(meta_data$unified_ID, 1, 2) == &quot;QC&quot;), ] meta_data[] &lt;- lapply(meta_data, function(x) if (is.factor(x)) factor(x) else x) # Gb12, Gb20 and Gb21 are missing temperature and salinity. Imputing data from closeby samples: meta_data$Salinity[meta_data$unified_ID == &quot;Gb12&quot;] &lt;- 34.92 meta_data$Salinity[meta_data$unified_ID == &quot;Gb20&quot;] &lt;- 34.92 meta_data$Salinity[meta_data$unified_ID == &quot;Gb21&quot;] &lt;- 34.56 meta_data$Temperature[meta_data$unified_ID == &quot;Gb12&quot;] &lt;- 3.71 meta_data$Temperature[meta_data$unified_ID == &quot;Gb20&quot;] &lt;- 3.65 meta_data$Temperature[meta_data$unified_ID == &quot;Gb21&quot;] &lt;- 2.32 meta_data[&quot;spec&quot;] &lt;- str_sub(meta_data$unified_ID, 1, 2) meta_data &lt;- meta_data[order(meta_data$unified_ID), ] return(meta_data) } meta_data &lt;- meta_data_prep(meta_data) # separating OTU tables by sponge OTU_prep_sqrt &lt;- function(micro) { rownames(micro) &lt;- micro$Sample_ID micro$Sample_ID &lt;- NULL # micro &lt;- sqrt(micro) #sqrt could be toggled on/off here micro_gb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Gb&quot;), ] micro_sf &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Sf&quot;), ] micro_wb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Wb&quot;), ] micro_gb &lt;- micro_gb[, colSums(micro_gb != 0) &gt; 0] micro_sf &lt;- micro_sf[, colSums(micro_sf != 0) &gt; 0] micro_wb &lt;- micro_wb[, colSums(micro_wb != 0) &gt; 0] micros &lt;- list(gb = micro_gb, sf = micro_sf, wb = micro_wb) return(micros) } micro_ds &lt;- OTU_prep_sqrt(micro) # calculating overall relative abundance of each OTU per sample overall_rabdc &lt;- function(micros) { mic &lt;- micros n &lt;- 0 k &lt;- dim(mic)[1] mic[&quot;rowsum&quot;] &lt;- apply(mic, 1, sum) while (n &lt; k) { n &lt;- n + 1 mic[n, ] &lt;- mic[n, ]/(mic$rowsum[n]) } mic$rowsum &lt;- NULL mic &lt;- data.frame(t(mic)) # mic[&#39;avg_rel_abdc&#39;] &lt;- apply(mic, 1, mean) mic[&#39;occurrence&#39;] &lt;- ifelse(mic$avg&gt;0.0025, &#39;common&#39;, &#39;rare&#39;) return(mic) } rabdc &lt;- lapply(micro_ds, overall_rabdc) # preparing congruent data sets common_samples &lt;- intersect(colnames(rabdc$gb), cmp$unified_ID) rabdc &lt;- rabdc$gb[, colnames(rabdc$gb) %in% common_samples] rabdc &lt;- data.frame(t(rabdc)) cmp &lt;- cmp[cmp$unified_ID %in% common_samples, ] cmp[] &lt;- lapply(cmp, function(x) if (is.factor(x)) factor(x) else x) cmp &lt;- cmp[order((cmp$unified_ID)), ] # all(rownames(rabdc)==cmp$unified_ID) # CORRELATION for Gb and Sf bar_cor &lt;- function(rabdc_df, cmp) { barettin &lt;- data.frame(colnames(rabdc_df)) colnames(barettin) &lt;- &quot;XOTU&quot; barettin[&quot;barettin_estimate_P&quot;] &lt;- NA barettin[&quot;barettin_p_val_P&quot;] &lt;- NA barettin[&quot;barettin_estimate_S&quot;] &lt;- NA barettin[&quot;barettin_p_val_S&quot;] &lt;- NA n &lt;- 0 k &lt;- dim(barettin)[1] while (n &lt; k) { n &lt;- n + 1 barettin$barettin_estimate_P[n] &lt;- cor.test(as.numeric(rabdc_df[, n]), cmp$bar, method = &quot;pearson&quot;)$estimate barettin$barettin_p_val_P[n] &lt;- cor.test(as.numeric(rabdc_df[, n]), cmp$bar, method = &quot;pearson&quot;)$p.value barettin$barettin_estimate_S[n] &lt;- cor.test(as.numeric(rabdc_df[, n]), cmp$bar, method = &quot;spearman&quot;)$estimate barettin$barettin_p_val_S[n] &lt;- cor.test(as.numeric(rabdc_df[, n]), cmp$bar, method = &quot;spearman&quot;)$p.value } barettin[&quot;barettin_fdr_P&quot;] &lt;- NA barettin[&quot;barettin_fdr_S&quot;] &lt;- NA barettin$barettin_fdr_P &lt;- p.adjust(barettin$barettin_p_val_P, method = &quot;fdr&quot;) barettin$barettin_fdr_S &lt;- p.adjust(barettin$barettin_p_val_S, method = &quot;fdr&quot;) return(barettin) } barettin_cor &lt;- bar_cor(rabdc, cmp) # write.csv(barettin_cor, &#39;data/GB_OTU_barettin_correlation.csv&#39;, row.names = F) par(mfrow = c(1, 3)) hist(barettin_cor$barettin_estimate_P, main = &quot;Pearson&#39;s pmcc: \\n OTU rel abdc ~ barettin signal&quot;, xlab = &quot;Product moment \\n correlation coefficient&quot;) hist(barettin_cor$barettin_p_val_P, breaks = 20, main = &quot;Pearson&#39;s pmcc p-value&quot;, xlab = &quot;p-value&quot;) hist(barettin_cor$barettin_fdr_P, breaks = 20, main = &quot;Pearson&#39;s pmcc \\n fdr corrected p-value&quot;, xlab = &quot;FDR corrected p-value&quot;) par(mfrow = c(1, 1)) par(mfrow = c(1, 3)) hist(barettin_cor$barettin_estimate_S, main = &quot;Spearman&#39;s rho: \\n OTU rel abdc ~ barettin signal&quot;, xlab = &quot;Rho&quot;) hist(barettin_cor$barettin_p_val_S, breaks = 20, main = &quot;Spearman&#39;s rho p-value&quot;, xlab = &quot;p-value&quot;) hist(barettin_cor$barettin_fdr_S, breaks = 20, main = &quot;Spearman&#39;s rho \\n fdr corrected p-value&quot;, xlab = &quot;FDR corrected p-value&quot;) par(mfrow = c(1, 1)) # For Sf, for shortlist cmp &lt;- read.csv(&quot;data/metabolite_master_20190605.csv&quot;, header = T, sep = &quot;,&quot;) cmp$X &lt;- NULL rabdc &lt;- lapply(micro_ds, overall_rabdc) common_samples &lt;- intersect(colnames(rabdc$sf), cmp$unified_ID) rabdc &lt;- rabdc$sf[, colnames(rabdc$sf) %in% common_samples] rabdc &lt;- data.frame(t(rabdc)) cmp &lt;- cmp[cmp$unified_ID %in% common_samples, ] cmp[] &lt;- lapply(cmp, function(x) if (is.factor(x)) factor(x) else x) cmp &lt;- cmp[order((cmp$unified_ID)), ] all(rownames(rabdc) == cmp$unified_ID) ## [1] TRUE barettin_cor &lt;- bar_cor(rabdc, cmp) # write.csv(barettin_cor, &#39;data/SF_OTU_barettin_correlation.csv&#39;, row.names = F) 4.3 Microbial interaction network 4.3.1 Generating network based on different algorithms The overall goal of the subsequent seqctions of code is to produce a microbial interation network for Geodia barretti. Nodes will be OTUs/ASVs from Geodia barretti samples and edges represent an interaction between those OTUs/ASVs. In this first part, we will employ different algorithms for network building. Network building algorithms are mannifold and their results not uncontroversial, thus the recommended strategy is to use different methods and merge the resulting networks to one consensus representation (Weiss et al., 2016), which we will do in the second part. The original data set from 14 specimens of Geodia barretti contained 420 OTUs/ASVs. To reduce sparsity, we removed OTUs/ASVs with two or less non-zero values resulting in a data set containing 289 OTUs/ASVs. This data set was used for network inference with the following methods: MENA Pipeline fastLSA SparCC Maximal information coefficient MIC 4.3.1.1 Molecular Ecological Network Analysis (MENA) Pipeline The implementation of MENA (Deng et al., 2012; Zhou, Deng et al., 2010; Zhou, Deng et al.,2011) can be accessed at http://ieg4.rccc.ou.edu/mena. The data set was saved as tab separated values and all zeros were converted to blanks. No further filtering for non-zero values was done (more than two non-zero values). For data preparation, default settings were applied, i.e. missing data was only filled with 0.01 in blanks with paired valid values, logarithm was taken, Pearson correlation coefficient was selected. Likewise, Random matrix theory settings were kept at defaults, decreasing the cutoff from the top using Regress Poisson distribution only. The cutoff of 0.800 was chosen for the similarity matrix to construct the network, corresponding to a Chi-square test on Poisson distribution of 99.191 and a p-value of 0.001. This resulted in a network with 241 nodes and 3582 edges. A second analysis was produced with the same settings except building the similarity matrix based on Spearman’s Rho. The cutoff of 0.820 was chosen for the similarity matrix to construct the network, corresponding to a Chi-square test on Poisson distribution of 98.417 and a p-value of 0.001. This resulted in a network with 252 nodes and 2216 edges. Network properties and parameters are summarized in MENA_network_parameters_Feb2019.xlsx. 4.3.1.2 Local Similarity Analysis: fastLSA The command line program for calcularing local similarity (Durno et al., 2013) was downloaded from http://hallam.microbiology.ubc.ca/fastLSA/install/index.html and run specifying the input file, no time lag (-d 0) and significance level alpha (-a 0.05). All other paramters were kept at their default values. The input data set was a tab delimited text file stripped of OTU labels or sample IDs. $ ./fastLSA -i ../gb_289_feb2019.txt -d 0 -a 0.05 -o ../gb_289_feb2019.out The output file, as specified on the website, containes five columns. ‘index1’ and ‘index2’ represent the significant paired indices ranging from 0 to n-1 (OTUs/ASVs). LSA denotes the LSA statistic of each pair, lag was set to 0 with the -d flag and the p-valueBound column provides the p-value’s upper boundary for the significantly paired p-value. To produce comparable data sets, we replaced the indices with their OTU IDs and removed superfluous columns. fastLSA &lt;- read.csv(&quot;data/gb_289_feb2019.out&quot;, header = T, sep = &quot;&quot;) key &lt;- read.csv(&quot;data/fastLSA_index_otu_CORRECTEDfeb2019.csv&quot;, header = T, sep = &quot;;&quot;) fastLSA$index1_otu &lt;- key$fastLSA_OTU[match(fastLSA$index1, key$fastLSA_index)] fastLSA$index2_otu &lt;- key$fastLSA_OTU[match(fastLSA$index2, key$fastLSA_index)] fastLSA$index1 &lt;- NULL fastLSA$index2 &lt;- NULL fastLSA$lag &lt;- NULL fastLSA$X &lt;- NULL fastLSA &lt;- fastLSA[, c(3, 4, 1, 2)] fastLSA &lt;- fastLSA[order(fastLSA$p.valueBound), ] # write.csv(fastLSA, &#39;fastLSA_for_networks.csv&#39;) rm(key) LSA scores range from -1 for strong negatively correlations to 1, for strong positive correlations. There were no negative correlations in this data setand we refrained from scaleding the LSA score furhter. The resulting network contrained 129 nodes and 207 edges. 4.3.1.3 SparCC SparCC (Friedman and Alm, 2012) is a network building algorithm for compositional data and can be found at https://bitbucket.org/yonatanf/sparcc. Prior to running it I had to get help as there was a minor issue during compilation. SparCC needed specific versions of numpy, panda and python to run properly, which is easiest accomodated in a specific environment. The OTU table needs to be windows formatted text. The embedded code is an example, for the analysis, 500 iterations were combined. $ cd to working directory with SparCC and the data set gb_289.csv $ source activate sparcc python SparCC.py ../gb_sparcc.txt -c ../gb_sparcc_cor_file.txt -v ../gb_sparcc_coverage_file.txt -i 5 $ deactivate Create a results directory and redirect all the output there. Pseudo p-value Calculation, generates -n shuffled data sets: $ mkdir results #creates output directory $ python MakeBootstraps.py ../gb_sparcc.txt -n 5 -t permutation_#.txt -p ../results/ And run SparCC.py on all the re-shuffled data sets: $ python SparCC.py ../results/permutation_0.txt -i 5 --cor_file=../results/perm_cor_0.txt $ python SparCC.py ../results/permutation_1.txt -i 5 --cor_file=../results/perm_cor_1.txt $ python SparCC.py ../results/permutation_2.txt -i 5 --cor_file=../results/perm_cor_2.txt $ python SparCC.py ../results/permutation_3.txt -i 5 --cor_file=../results/perm_cor_3.txt $ python SparCC.py ../results/permutation_4.txt -i 5 --cor_file=../results/perm_cor_4.txt Generate p-values: $ python PseudoPvals.py ../results/gb_sparcc_cor_file.txt ../results/perm_cor_#.txt 5 -o ../results/pvals.two_sided.txt -t two_sided Formatting the resulting data set like so: library(reshape2) cor_file &lt;- read.csv(&quot;data/gb_sparcc_cor_file_289.csv&quot;, header = T, sep = &quot;;&quot;) p_vals &lt;- read.csv(&quot;data/gb_289_pvals.two_sided.csv&quot;, header = T, sep = &quot;;&quot;) # make OTU ID the rowname rownames(cor_file) &lt;- cor_file[, 1] cor_file[, 1] &lt;- NULL rownames(p_vals) &lt;- p_vals[, 1] p_vals[, 1] &lt;- NULL # check ds congruency all(colnames(cor_file) == rownames(cor_file)) all(colnames(p_vals) == rownames(p_vals)) # melt into long format, all vs all comparison: 289^2=83521 rows cor_file_m &lt;- melt(as.matrix(cor_file)) p_vals_m &lt;- melt(as.matrix(p_vals)) all(cor_file_m$Var1 == p_vals_m$Var1) all(cor_file_m$Var2 == p_vals_m$Var2) # complete data set with p_vals and &#39;correlation coeff&#39; cor_file_m[&quot;p_vals&quot;] &lt;- p_vals_m$value # This removes AB - BA duplicates but still contains self comprisons, AA, BB,CC etc. cols &lt;- c(&quot;Var1&quot;, &quot;Var2&quot;) newdf &lt;- cor_file_m[, cols] #generate new data set with just those two # a &lt;- Sys.time() for (i in 1:nrow(cor_file_m)) { newdf[i, ] = sort(cor_file_m[i, cols]) } # b &lt;- Sys.time() b-a cor_file_shortened &lt;- cor_file_m[!duplicated(newdf), ] #and can be removed with duplicate cor_file_shortened &lt;- cor_file_shortened[which(cor_file_shortened$Var1 != cor_file_shortened$Var2), ] # removing self comparison colnames(cor_file_shortened) &lt;- c(&quot;Var1_SparCC&quot;, &quot;Var2_SparCC&quot;, &quot;SparCC&quot;, &quot;pSparCC&quot;) #41616 write.csv(cor_file_shortened, &quot;data/SparCC_for_networks.csv&quot;) rm(cor_file_m, newdf, p_vals_m, i, cols) 4.3.1.4 Maximal information coefficient MIC MIC for pairwise interaction was calculated with the R package minearva. The MIC is part of a statistic called Maximal Information-Based Nonparametric Exploration (MINE). library(minerva) OTU &lt;- read.csv(&quot;data/gb_289.csv&quot;, header = T, sep = &quot;;&quot;) rownames(OTU) &lt;- OTU[, 1] OTU[, 1] &lt;- NULL OTU &lt;- as.data.frame(t(OTU)) # Calculate MIC of original data set. MINE &lt;- mine(OTU) MIC &lt;- MINE$MIC #dim(MIC): 289 289 Obtaining p-values for this statistic can be achieved by permutation of the original OTU table as below or empirically, by selecting the thousand strongest interactions. # 10 needs to be replaced with 1000 for final version, three times!!! reshuffling the OTU table, saving the MIC to a list, a total of 1000 times n &lt;- 0 results &lt;- list() # For reproducibility, one could e.g.: set.seed(1984) # c &lt;- Sys.time() while (n &lt; 1000) { n &lt;- n + 1 mock &lt;- apply(OTU, MARGIN = 2, sample) mock_mine &lt;- mine(mock) results[[n]] &lt;- mock_mine$MIC } # d &lt;- Sys.time() d-c # for every element of the true matrix, go through all the same elements in the 1000 generated mock matrix MIC indices and count how many of those are greater. MIC &lt;- MINE$MIC e_values &lt;- matrix(nrow = nrow(MIC), ncol = ncol(MIC), data = 0) for (i in 1:nrow(MIC)) { for (j in 1:ncol(MIC)) { n &lt;- 0 while (n &lt; 1000) { n &lt;- n + 1 if (results[[n]][i, j] &gt;= MIC[i, j]) { e_values[i, j] &lt;- e_values[i, j] + 1 } } } } e_values &lt;- e_values/1000 # write.csv(e_values, &#39;data/MIC_e_values.csv&#39;) For n=1000, the first part takes about 8 mins on one core, the second part about 2 min. The relevant output is saved in the initial calculations of the MIC and the corresponding e-values are in e_values. These are symmetric matrices that will be reduced to a long table with unique OTUs/ASVs pairs, their MIC and the e-value. For clarity, all other files are removed. rm(MINE, mock, mock_mine, results) # transforming symmetric matrix to unique-pair long format cor_file &lt;- data.frame(MIC) p_vals &lt;- data.frame(e_values) # inspect the files, adapt them and test congruency rownames(p_vals) &lt;- rownames(cor_file) colnames(p_vals) &lt;- colnames(cor_file) all(colnames(cor_file) == rownames(cor_file)) # melt into long format, all vs all comparison: 289*289=83521 rows cor_file_m &lt;- melt(as.matrix(cor_file)) all(colnames(p_vals) == rownames(p_vals)) p_vals_m &lt;- melt(as.matrix(p_vals)) # complete data set with p_vals cor_file_m[&quot;p_vals&quot;] &lt;- p_vals_m$value # This removes AB - BA duplicates but still contains self comprisons, AA, BB, etc. cols &lt;- c(&quot;Var1&quot;, &quot;Var2&quot;) newdf &lt;- cor_file_m[, cols] #generate new data set with just those two for (i in 1:nrow(cor_file_m)) { newdf[i, ] &lt;- sort(cor_file_m[i, cols]) } cor_file_shortened &lt;- cor_file_m[!duplicated(newdf), ] #and can be removed with duplicate cor_file_shortened &lt;- cor_file_shortened[which(cor_file_shortened$Var1 != cor_file_shortened$Var2), ] # removing self comparison rm(cor_file_m, newdf, p_vals_m, i, cols) # write.csv(cor_file_shortened, &#39;data/MIC_for_networks.csv&#39;) 4.3.2 Consolidation of the different networks 4.3.2.1 MIC MIC allows to detect a variety of interactions. According to the manual of the R wrapper minerva, the resulting MIC score “is related to the relationship strenght and it can be interpreted as a correlation measure. It is symmetric and it ranges in [0,1], where it tends to 0 for statistically independent data and it approaches 1 in probability for noiseless functional relationships”. Thus, it also contains strong negative relationship up to mutual exclusivity, which we want to filter out. mic &lt;- read.csv(&quot;data/MIC_for_networks.csv&quot;, header = T, sep = &quot;,&quot;) mic$X &lt;- NULL colnames(mic) &lt;- c(&quot;node1_mic&quot;, &quot;node2_mic&quot;, &quot;MIC&quot;, &quot;pMIC&quot;) Initially, the MIC network generated by the R wrapper minerva contained MIC values for all possible edges (i.e. 41616). Of those, 4370 edges/interactions had a p-value \\(\\leq\\) 0.05. As we will only include those edges in the final network, we select those and calculate the linear regression coefficient and p-value for the regression, to test whether we are able to distinguish negative from positive interaction. # Original OTU table for regressions OTU &lt;- read.csv(&quot;data/gb_289.csv&quot;, header = T, sep = &quot;;&quot;) rownames(OTU) &lt;- OTU[, 1] OTU[, 1] &lt;- NULL OTU[&quot;ID&quot;] &lt;- row.names(OTU) # goal: in mic data frame, set to &#39;NA&#39; MIC and pMIC of edges with a significant p-value for MIC that have a significant negative regression mic[&quot;regression&quot;] &lt;- NA mic[&quot;p_regression&quot;] &lt;- NA for (i in 1:nrow(mic)) { bac1 &lt;- factor(mic[i, 1]) bac2 &lt;- factor(mic[i, 2]) temp_ds &lt;- data.frame(t(rbind(OTU[OTU$ID == bac1, ], OTU[OTU$ID == bac2, ]))) temp_ds &lt;- temp_ds[-c(15), ] temp_ds[] &lt;- lapply(temp_ds, function(x) if (is.factor(x)) factor(x) else x) # removes factors, not sure if necessary mic$regression[i] &lt;- summary(lm(c(temp_ds[, 1]) ~ c(temp_ds[, 2])))$coefficients[2, 1] #slope mic$p_regression[i] &lt;- summary(lm(c(temp_ds[, 1]) ~ c(temp_ds[, 2])))$coefficients[2, 4] #p-val } before &lt;- sum(mic$pMIC &lt;= 0.05) #4370 mic$MIC &lt;- ifelse((mic$pMIC &lt;= 0.05 &amp; mic$regression &lt; 0 &amp; mic$p_regression &lt;= 0.05), NA, mic$MIC) mic$pMIC &lt;- ifelse((mic$pMIC &lt;= 0.05 &amp; mic$regression &lt; 0 &amp; mic$p_regression &lt;= 0.05), NA, mic$pMIC) after &lt;- sum(mic$pMIC &lt;= 0.05, na.rm = T) #3522 write.csv(mic, &quot;data/MIC.csv&quot;) rm(temp_ds, bac1, bac2, i, mic) The MIC data set initially contained 41616 edges, 4370 of which were significant prior to the removal of negative correlations and leaving 3522 edges with a p-value \\(\\leq\\) 0.05. 4.3.2.2 SparCC The next network data set is based on the SparCC algorithm for computing correlations in compositional data. sparcc &lt;- read.csv(&quot;data/SparCC_for_networks.csv&quot;, header = T, sep = &quot;,&quot;) sparcc$X &lt;- NULL colnames(sparcc) &lt;- c(&quot;node1_sparcc&quot;, &quot;node2_sparcc&quot;, &quot;SparCC&quot;, &quot;pSparCC&quot;) # hist(sparcc$SparCC) hist(sparcc$pSparCC) sparcc$pSparCC &lt;- ifelse((sparcc$SparCC &lt; 0), NA, sparcc$pSparCC) #setting the p-values of negative interactions to NA sparcc$SparCC &lt;- ifelse((sparcc$SparCC &lt; 0), NA, sparcc$SparCC) #setting negative interactions to NA The network based on the SparCC algorithm contained 41616 edges of which 20420 negative interactions that were removed. 6622 significant positive edges remain. 4.3.2.3 MENA The next two network data sets are generated by MENA based on random matrix theory. mena_pcc &lt;- read.csv(&quot;data/MENA_0.800_PCC_edge_attribute.txt&quot;, header = F, sep = &quot; &quot;) # &#39;np&#39; in V2 and -1 in V5 mean negative interaction, these should be removed. dim(mena_pcc)[1] - dim(mena_pcc[mena_pcc$V5 == -1, ])[1] # Number of pos interactions ## [1] 171 mena_pcc[&quot;pMENA_PCC&quot;] &lt;- ifelse((mena_pcc$V5 &lt; 0), NA, 0.001) mena_pcc$V2 &lt;- NULL mena_pcc$V4 &lt;- NULL mena_pcc$V5 &lt;- NULL colnames(mena_pcc) &lt;- c(&quot;node1_mena&quot;, &quot;node2_mena&quot;, &quot;pMENA_PCC&quot;) mena_scc &lt;- read.csv(&quot;data/MENA_0.820_SCC_edge_attribute.txt&quot;, header = F, sep = &quot; &quot;) dim(mena_scc)[1] - dim(mena_scc[mena_scc$V5 == -1, ])[1] # Number of pos interactions ## [1] 317 mena_scc[&quot;pMENA_SCC&quot;] &lt;- ifelse((mena_scc$V5 &lt; 0), NA, 0.001) mena_scc$V2 &lt;- NULL mena_scc$V4 &lt;- NULL mena_scc$V5 &lt;- NULL colnames(mena_scc) &lt;- c(&quot;node1_mena&quot;, &quot;node2_mena&quot;, &quot;pMENA_SCC&quot;) MENA network with Pearson correlation contained 3582 edges of which 3411 negative interactions were removed. For Spearman correlations, the network contained 2216 edges of which 1899 negative interactions were removed. 4.3.2.4 LSA The next network data set is based on local similarity. It does not contain any negative values for LSA, so we do not exclude any edges. lsa &lt;- read.csv(&quot;data/fastLSA_for_networks.csv&quot;, header = T, sep = &quot;,&quot;) lsa$X &lt;- NULL colnames(lsa) &lt;- c(&quot;node1_lsa&quot;, &quot;node2_lsa&quot;, &quot;LSA&quot;, &quot;pLSA&quot;) dim(lsa)[1] # Number of edges ## [1] 208 4.3.2.5 Integration of the networks Now we combine all five networks into one data set. mic &lt;- read.csv(&quot;data/MIC.csv&quot;, header = T) mic$X &lt;- NULL mic$regression &lt;- NULL mic$p_regression &lt;- NULL master_summary &lt;- mic library(dplyr) master_summary &lt;- full_join(master_summary, lsa, by = c(node1_mic = &quot;node2_lsa&quot;, node2_mic = &quot;node1_lsa&quot;)) # sum(!is.na(master_summary$pLSA))==nrow(lsa) #TRUE master_summary &lt;- full_join(master_summary, sparcc, by = c(node1_mic = &quot;node1_sparcc&quot;, node2_mic = &quot;node2_sparcc&quot;)) # sum(!is.na(master_summary$pSparCC))==sum(!is.na(sparcc$pSparCC)) #TRUE master_summary &lt;- full_join(master_summary, mena_pcc, by = c(node1_mic = &quot;node2_mena&quot;, node2_mic = &quot;node1_mena&quot;)) # sum(!is.na(master_summary$pMENA_PCC))==sum(!is.na(mena_pcc$pMENA_PCC)) #TRUE master_summary &lt;- full_join(master_summary, mena_scc, by = c(node1_mic = &quot;node2_mena&quot;, node2_mic = &quot;node1_mena&quot;)) # sum(!is.na(master_summary$pMENA_SCC))==sum(!is.na(mena_scc$pMENA_SCC)) #TRUE master_summary &lt;- master_summary[, c(1, 2, 4, 6, 8, 9, 10, 3, 5, 7)] #reorder columns head(master_summary) # write.csv(master_summary, &#39;data/master_summary_networks_1.csv&#39;, row.names = FALSE) # For p-value merging: metap::sumlog, or EmpiricalBrownsMethod::EBM library(metap) ms &lt;- read.csv(&quot;data/master_summary_networks_1.csv&quot;, header = T) par(mfrow = c(2, 3)) hist(ms$pMIC) hist(ms$pLSA) #xlim = range(0,1) hist(ms$pSparCC) plot(ms$pMENA_PCC) plot(ms$pMENA_SCC) par(mfrow = c(1, 1)) # metap::sumlog doesn&#39;t think 0 is a valid p-value, replace all zeros with small non-zero values, e.g. half-minimum ms$pSparCC[ms$pSparCC == 0] &lt;- 0.005 ms$pMIC[ms$pMIC == 0] &lt;- 5e-04 ms[&quot;NA_count&quot;] &lt;- NA ms[&quot;signif_0.05&quot;] &lt;- NA ms[&quot;signif_0.001&quot;] &lt;- NA ms[&quot;sumlog&quot;] &lt;- NA n &lt;- 0 k &lt;- dim(ms)[1] while (n &lt; k) { n &lt;- n + 1 ms$NA_count[n] &lt;- sum(is.na(ms[n, 3:7])) ms$signif_0.05[n] &lt;- sum(ms[n, 3:7] &lt;= 0.05, na.rm = T) ms$signif_0.001[n] &lt;- sum(ms[n, 3:7] &lt;= 0.001, na.rm = T) ifelse((ms$NA_count[n] &lt;= 3), (ms$sumlog[n] &lt;- sumlog(ms[n, 3:7][!is.na(ms[n, 3:7])])$p), NA) } rm(k, n) ms[&quot;p.adjust_Bonferroni&quot;] &lt;- p.adjust(ms$sumlog, method = &quot;bonferroni&quot;) ms[&quot;p.adjust_FDR&quot;] &lt;- p.adjust(ms$sumlog, method = &quot;fdr&quot;) #aka Benjamini &amp; Hochberg par(mfrow = c(1, 3)) hist(ms$signif_0.05, main = &quot;p-values &lt;= 0.05&quot;, xlab = &quot;Counts per edge &quot;, breaks = c(0, 1, 2, 3, 4, 5), labels = TRUE) hist(ms$signif_0.001, main = &quot;p-values &lt;= 0.001&quot;, xlab = &quot;Counts per edge &quot;, breaks = c(0, 1, 2, 3, 4, 5), labels = TRUE) hist(ms$NA_count, main = &quot;NAs&quot;, xlab = &quot;Counts per edge &quot;, breaks = c(0, 1, 2, 3, 4, 5), labels = TRUE) par(mfrow = c(1, 1)) ds &lt;- split(ms, ms$signif_0.05) raw_nodes &lt;- rbind(ds$`4`, ds$`5`) # Selection/inclusion criterion rm(ds) edges &lt;- raw_nodes[, 1:2] nodes &lt;- union(raw_nodes$node1_mic, raw_nodes$node2_mic) write.csv(edges, &quot;data/master_summary_networks_2.csv&quot;, row.names = F) We selected all edges with at least 4 p-values \\(\\leq\\) 0.05 to visualise in the network. This consensus network has 136 edges and 113 nodes. Below are two versions of the network. In the first version, all OTUs correlating with barettin are colour-coded by class. In the second version, all OTUs increasing/decreasing woth depth are highlighted by colour. # THANK YOU: https://kateto.net/networks-r-igraph edges &lt;- read.csv(&quot;data/master_summary_networks_2.csv&quot;, header = T, sep = &quot;,&quot;) nodes &lt;- data.frame(union(raw_nodes$node1_mic, raw_nodes$node2_mic)) # annotation data required: inc-dec/depth response, taxonomy, barettin corelation hex color code depth &lt;- read.csv(&quot;data/gb_OTUs_overall_rabdc_annotated.csv&quot;, header = T, sep = &quot;,&quot;) taxonomy &lt;- read.csv(&quot;data/microbiome_taxonomy.csv&quot;, header = T, sep = &quot;;&quot;) barettin &lt;- read.csv(&quot;data/GB_OTU_barettin_correlation.csv&quot;, header = T, sep = &quot;,&quot;) # matching IDs colnames(nodes) &lt;- c(&quot;OTU_long&quot;) nodes[&quot;OTU&quot;] &lt;- str_replace(nodes$OTU_long, &quot;OTU196900&quot;, &quot;&quot;) depth[&quot;OTU&quot;] &lt;- str_replace(depth$XOTU, &quot;X196900&quot;, &quot;&quot;) taxonomy[&quot;OTU&quot;] &lt;- str_replace(taxonomy$OTU_ID, &quot;196900&quot;, &quot;&quot;) barettin[&quot;OTU&quot;] &lt;- str_replace(barettin$XOTU, &quot;X196900&quot;, &quot;&quot;) # downsizing to relevant columns nodes$OTU_long &lt;- NULL depth &lt;- depth[, c(&quot;ttest_pval&quot;, &quot;ttest_fdr&quot;, &quot;inc_dec_estimate&quot;, &quot;inc_dec_p_val&quot;, &quot;fdr&quot;, &quot;classification&quot;, &quot;OTU&quot;)] colnames(depth) &lt;- c(&quot;ttest_pval&quot;, &quot;ttest_fdr&quot;, &quot;inc_dec_estimate&quot;, &quot;inc_dec_p_val&quot;, &quot;inc_dec_fdr&quot;, &quot;inc_dec_classification&quot;, &quot;OTU&quot;) barettin$XOTU &lt;- NULL taxonomy &lt;- taxonomy[, c(&quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;OTU&quot;)] taxonomy[] &lt;- lapply(taxonomy, str_trim) nodes &lt;- left_join(nodes, depth) nodes &lt;- left_join(nodes, barettin) nodes &lt;- left_join(nodes, taxonomy) # Adding categorical information nodes[&quot;barettin_c&quot;] &lt;- ifelse(nodes$barettin_estimate_P &gt; 0 &amp; nodes$barettin_p_val_P &lt; 0.05, 1, 0) # for scaling node size nodes[&quot;inc_dec_c&quot;] &lt;- 0 nodes$inc_dec_c[nodes$inc_dec_p_val &lt; 0.05] &lt;- 1 # for scaling node size nodes[&quot;inc_dec_c_group&quot;] &lt;- NA nodes$inc_dec_c_group[nodes$inc_dec_c == 1 &amp; nodes$inc_dec_estimate &gt; 0] &lt;- c(&quot;deep&quot;) # for colouring shallow vs. deep nodes$inc_dec_c_group[nodes$inc_dec_c == 1 &amp; nodes$inc_dec_estimate &lt; 0] &lt;- c(&quot;shallow&quot;) # for colouring shallow vs. deep # Pearson pmcc &gt; 0 &amp; p &lt; 0.05 56 fdr &lt; 0.05 22 Spearman rho &gt; 0 &amp; p &lt; 0.05 42 fdr &lt; 0.05 1 nodes[] &lt;- lapply(nodes, function(x) if (is.factor(x)) factor(x) else x) library(igraph) net &lt;- graph_from_data_frame(d = edges, vertices = nodes, directed = F) l &lt;- layout_with_kk(net) # 11 colourblind-friendly colours from https://medialab.github.io/iwanthue/ # Taxonomy coloring &amp; barettin: Class ecol &lt;- rep(&quot;gray80&quot;, ecount(net)) vcol &lt;- rep(&quot;grey40&quot;, vcount(net)) vcol[V(net)$Class == &quot;Subgroup_26&quot;] &lt;- &quot;#628ed6&quot; vcol[V(net)$Class == &quot;Subgroup_15&quot;] &lt;- &quot;#957d34&quot; vcol[V(net)$Class == &quot;Subgroup_6&quot;] &lt;- &quot;#45c097&quot; vcol[V(net)$Class == &quot;Anaerolineae&quot;] &lt;- &quot;#ba4758&quot; vcol[V(net)$Class == &quot;JG30-KF-CM66&quot;] &lt;- &quot;#b2467e&quot; vcol[V(net)$Class == &quot;SAR202_clade&quot;] &lt;- &quot;#5b3687&quot; vcol[V(net)$Class == &quot;TK10&quot;] &lt;- &quot;#ba5437&quot; vcol[V(net)$Class == &quot;BD2-11_terrestrial_group&quot;] &lt;- &quot;#69ab54&quot; vcol[V(net)$Class == &quot;Alphaproteobacteria&quot;] &lt;- &quot;#c777cb&quot; vcol[V(net)$Class == &quot;JTB23&quot;] &lt;- &quot;#c3a63e&quot; vcol[V(net)$Class == &quot;Acidimicrobiia&quot;] &lt;- &quot;#6a70d7&quot; colrs &lt;- c(&quot;#628ed6&quot;, &quot;#957d34&quot;, &quot;#45c097&quot;, &quot;#ba4758&quot;, &quot;#b2467e&quot;, &quot;#5b3687&quot;, &quot;#ba5437&quot;, &quot;#69ab54&quot;, &quot;#c777cb&quot;, &quot;#c3a63e&quot;, &quot;#6a70d7&quot;) V(net)$color &lt;- colrs[V(net)$class] # set vertex.label=V(net)$OTU for OTU numbers plot(net, vertex.color = vcol, edge.color = ecol, vertex.size = V(net)$barettin_c * 10, vertex.label = NA, layout = l) legend(x = 1, y = 0.5, c(&quot;Subgroup_26 (Acidobacteria)&quot;, &quot;Subgroup_15 (Acidobacteria)&quot;, &quot;Subgroup_6 (Acidobacteria)&quot;, &quot;Anaerolineae (Chloroflexi)&quot;, &quot;JG30-KF-CM66 (Chloroflexi)&quot;, &quot;SAR202_clade (Chloroflexi)&quot;, &quot;TK10 (Chloroflexi)&quot;, &quot;BD2-11_terrestrial_group (Gemmatimonadetes)&quot;, &quot;Alphaproteobacteria (Proteobacteria)&quot;, &quot;JTB23 (Proteobacteria)&quot;, &quot;Acidimicrobiia (Actinobacteria)&quot;), pt.bg = colrs, pch = 21, col = &quot;#777777&quot;, pt.cex = 1.5, cex = 0.7, bty = &quot;n&quot;, ncol = 1) Figure 4.1: Microbial interaction networks highlighting OTUs correlating with barettin and OTUs correlating with depth. # Depth by correlation ecol &lt;- rep(&quot;gray80&quot;, ecount(net)) vcol &lt;- rep(&quot;grey40&quot;, vcount(net)) vcol[V(net)$inc_dec_c_group == &quot;shallow&quot;] &lt;- &quot;gold&quot; vcol[V(net)$inc_dec_c_group == &quot;deep&quot;] &lt;- &quot;blue&quot; V(net)$color &lt;- colrs[V(net)$inc_dec_c_group] colrs &lt;- c(&quot;gold&quot;, &quot;blue&quot;) plot(net, vertex.color = vcol, edge.color = ecol, vertex.size = V(net)$inc_dec_c * 10, vertex.label = NA, layout = l) legend(x = 1, y = 0.5, c(&quot;decreasing&quot;, &quot;incerasing&quot;), pt.bg = colrs, pch = 21, col = &quot;#777777&quot;, pt.cex = 1.5, cex = 0.7, bty = &quot;n&quot;, ncol = 1) Figure 4.2: Microbial interaction networks highlighting OTUs correlating with barettin and OTUs correlating with depth. 4.4 Shortlist We believe the producer of barettin (and related compounds) to have the following properties: common (average relative abundance &gt; 0.25%) specific to G. barretti positively correlated with barettin the OTUs below fulfill these criteria: # most annotation from NW micro &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) OTU_prep_sqrt &lt;- function(micro) { rownames(micro) &lt;- micro$Sample_ID micro$Sample_ID &lt;- NULL # micro &lt;- sqrt(micro) micro_gb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Gb&quot;), ] micro_sf &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Sf&quot;), ] micro_wb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Wb&quot;), ] micro_gb &lt;- micro_gb[, colSums(micro_gb != 0) &gt; 0] #removes columns that only contain 0 micro_sf &lt;- micro_sf[, colSums(micro_sf != 0) &gt; 0] micro_wb &lt;- micro_wb[, colSums(micro_wb != 0) &gt; 0] micros &lt;- list(gb = micro_gb, sf = micro_sf, wb = micro_wb) return(micros) } micro_ds &lt;- OTU_prep_sqrt(micro) overall_rabdc &lt;- function(micro) { mic &lt;- micro n &lt;- 0 k &lt;- dim(mic)[1] mic[&quot;rowsum&quot;] &lt;- apply(mic, 1, sum) while (n &lt; k) { n &lt;- n + 1 mic[n, ] &lt;- mic[n, ]/(mic$rowsum[n]) } mic$rowsum &lt;- NULL mic &lt;- data.frame(t(mic)) mic[&quot;avg_rel_abdc&quot;] &lt;- apply(mic, 1, mean) mic[&quot;occurrence&quot;] &lt;- ifelse(mic$avg &gt; 0.0025, &quot;common&quot;, &quot;rare&quot;) return(mic) } occurrence &lt;- lapply(micro_ds, overall_rabdc) depth &lt;- read.csv(&quot;data/gb_OTUs_overall_rabdc_annotated.csv&quot;, header = T, sep = &quot;,&quot;) taxonomy &lt;- read.csv(&quot;data/microbiome_taxonomy.csv&quot;, header = T, sep = &quot;;&quot;) barettin &lt;- read.csv(&quot;data/GB_OTU_barettin_correlation.csv&quot;, header = T, sep = &quot;,&quot;) # matching IDs nodes[&quot;OTU&quot;] &lt;- str_replace(nodes$OTU_long, &quot;OTU196900&quot;, &quot;&quot;) depth[&quot;OTU&quot;] &lt;- str_replace(depth$XOTU, &quot;X196900&quot;, &quot;&quot;) taxonomy[&quot;OTU&quot;] &lt;- str_replace(taxonomy$OTU_ID, &quot;196900&quot;, &quot;&quot;) barettin[&quot;OTU&quot;] &lt;- str_replace(barettin$XOTU, &quot;X196900&quot;, &quot;&quot;) occurrence$gb[&quot;OTU&quot;] &lt;- str_replace(rownames(occurrence$gb), &quot;X196900&quot;, &quot;&quot;) # downsizing to relevant columns nodes$OTU_long &lt;- NULL depth &lt;- depth[, c(&quot;ttest_pval&quot;, &quot;ttest_fdr&quot;, &quot;inc_dec_estimate&quot;, &quot;inc_dec_p_val&quot;, &quot;fdr&quot;, &quot;classification&quot;, &quot;OTU&quot;)] colnames(depth) &lt;- c(&quot;ttest_pval&quot;, &quot;ttest_fdr&quot;, &quot;inc_dec_estimate&quot;, &quot;inc_dec_p_val&quot;, &quot;inc_dec_fdr&quot;, &quot;inc_dec_classification&quot;, &quot;OTU&quot;) barettin$XOTU &lt;- NULL taxonomy[] &lt;- lapply(taxonomy, str_trim) # combining dfs shortlist_gb &lt;- left_join(occurrence$gb, depth) shortlist_gb &lt;- left_join(shortlist_gb, barettin) shortlist_gb &lt;- left_join(shortlist_gb, taxonomy) # Exclude Sf OTUs intersect(colnames(micro_ds$gb), colnames(micro_ds$sf)) # shared OTUs length(intersect(colnames(micro_ds$gb), colnames(micro_ds$sf))) # 316 # setdiff(colnames(micro_ds$gb), colnames(micro_ds$sf)) # Setdiff finds rows that appear in first table but not in second length(setdiff(colnames(micro_ds$gb), # colnames(micro_ds$sf))) # 104:ok! gb_unique &lt;- data.frame(setdiff(colnames(micro_ds$gb), colnames(micro_ds$sf))) colnames(gb_unique) &lt;- c(&quot;XOTU&quot;) gb_unique[&quot;OTU&quot;] &lt;- str_replace(gb_unique$XOTU, &quot;X196900&quot;, &quot;&quot;) gb_unique &lt;- left_join(gb_unique, shortlist_gb) gb_unique &lt;- gb_unique %&gt;% filter(occurrence == &quot;common&quot;) %&gt;% filter(barettin_estimate_P &gt; 0 &amp; barettin_p_val_P &lt; 0.05) # Recovery of those OTUs that also correlate with barettin in S. fortis sf_shared &lt;- data.frame(intersect(colnames(micro_ds$gb), colnames(micro_ds$sf))) colnames(sf_shared) &lt;- c(&quot;XOTU&quot;) sf_barretin &lt;- read.csv(&quot;data/SF_OTU_barettin_correlation.csv&quot;, header = T, sep = &quot;,&quot;) sf_shared &lt;- left_join(sf_shared, sf_barretin) # Sf Pearson p&lt;0.05 24 pFDR&gt;0.05 8 Spearman p&lt;0.05 14 pFDR&gt;0.05 0 sf_shared &lt;- sf_shared %&gt;% filter(barettin_estimate_P &gt; 0) %&gt;% filter(barettin_p_val_P &lt; 0.05) sf_shared[&quot;OTU&quot;] &lt;- str_replace(sf_shared$XOTU, &quot;X196900&quot;, &quot;&quot;) shared_recovered &lt;- shortlist_gb[shortlist_gb$OTU %in% sf_shared$OTU, ] # check again for the criteria in the new df shared_recovered &lt;- shared_recovered %&gt;% filter(occurrence == &quot;common&quot;) %&gt;% filter(barettin_estimate_P &gt; 0 &amp; barettin_p_val_P &lt; 0.05) # combine the two data sets: gb_unique and shared_recovered gb_unique[&quot;group&quot;] &lt;- c(&quot;gb_unique&quot;) shared_recovered[&quot;group&quot;] &lt;- c(&quot;sf_shared&quot;) gb_unique &lt;- gb_unique[, c(&quot;OTU&quot;, &quot;group&quot;, &quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;)] shared_recovered &lt;- shared_recovered[, c(&quot;OTU&quot;, &quot;group&quot;, &quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;)] shortlist &lt;- rbind(gb_unique, shared_recovered) options(kableExtra.html.bsTable = T) kable(shortlist, col.names = c(&quot;OTU&quot;, &quot;group&quot;, &quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;), longtable = T, booktabs = T, caption = &quot;Shortlist of OTUs that were deemed candidate producers of barettin.&quot;, row.names = FALSE) %&gt;% add_header_above(c(` ` = 2, Taxonomy = 3)) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), full_width = F, latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;)) Table 4.3: Shortlist of OTUs that were deemed candidate producers of barettin. Taxonomy OTU group Kingdom Phylum Class 589 gb_unique Bacteria Chloroflexi SAR202_clade 588 gb_unique Bacteria Acidobacteria Subgroup_9 180 sf_shared Bacteria Proteobacteria JTB23 144 sf_shared Bacteria Acidobacteria Subgroup_9 213 sf_shared Bacteria Gemmatimonadetes BD2-11_terrestrial_group 310 sf_shared Bacteria Proteobacteria JTB23 micro &lt;- read.csv(&quot;data/OTU_all_R.csv&quot;, header = T, sep = &quot;;&quot;) meta_data &lt;- read.csv(&quot;data/Steffen_et_al_metadata_PANGAEA.csv&quot;, header = T, sep = &quot;;&quot;) cmp &lt;- read.csv(&quot;data/metabolite_master_20190605.csv&quot;, header = T, sep = &quot;,&quot;) meta_data &lt;- meta_data_prep(meta_data) OTU_prep &lt;- function(micro) { rownames(micro) &lt;- micro$Sample_ID micro$Sample_ID &lt;- NULL # micro &lt;- sqrt(micro) micro_gb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Gb&quot;), ] micro_sf &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Sf&quot;), ] micro_wb &lt;- micro[(str_sub(rownames(micro), 1, 2) == &quot;Wb&quot;), ] micro_gb &lt;- micro_gb[, colSums(micro_gb != 0) &gt; 0] micro_sf &lt;- micro_sf[, colSums(micro_sf != 0) &gt; 0] micro_wb &lt;- micro_wb[, colSums(micro_wb != 0) &gt; 0] micros &lt;- list(gb = micro_gb, sf = micro_sf, wb = micro_wb) return(micros) } micro_ds &lt;- OTU_prep(micro) overall_rabdc &lt;- function(micros) { mic &lt;- micros n &lt;- 0 k &lt;- dim(mic)[1] mic[&quot;rowsum&quot;] &lt;- apply(mic, 1, sum) while (n &lt; k) { n &lt;- n + 1 mic[n, ] &lt;- mic[n, ]/(mic$rowsum[n]) } mic$rowsum &lt;- NULL mic &lt;- data.frame(t(mic)) # mic[&#39;avg_rel_abdc&#39;] &lt;- apply(mic, 1, mean) mic[&#39;occurrence&#39;] &lt;- ifelse(mic$avg&gt;0.0025, &#39;common&#39;, &#39;rare&#39;) return(mic) } rabdc &lt;- lapply(micro_ds, overall_rabdc) rabdc$gb[, c(&quot;avg_rel_abdc&quot;, &quot;occurrence&quot;)] &lt;- list(NULL) # Shortlist OTUs sl &lt;- rabdc$gb[c(&quot;X196900144&quot;, &quot;X196900180&quot;, &quot;X196900213&quot;, &quot;X196900310&quot;, &quot;X196900588&quot;, &quot;X196900589&quot;), ] sl &lt;- data.frame(t(sl)) sl[&quot;unified_ID&quot;] &lt;- rownames(sl) sl &lt;- left_join(sl, meta_data[, c(&quot;unified_ID&quot;, &quot;Depth&quot;)]) sl &lt;- left_join(sl, cmp[, c(&quot;unified_ID&quot;, &quot;bar&quot;)]) summary(lm(X196900144 ~ bar, sl)) ## ## Call: ## lm(formula = X196900144 ~ bar, data = sl) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.009272 -0.002497 -0.001386 0.002741 0.013684 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.026e-03 4.204e-03 -0.720 0.49218 ## bar 1.720e-08 2.992e-09 5.748 0.00043 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.006446 on 8 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.8051, Adjusted R-squared: 0.7807 ## F-statistic: 33.04 on 1 and 8 DF, p-value: 0.0004298 summary(lm(X196900180 ~ bar, sl)) ## ## Call: ## lm(formula = X196900180 ~ bar, data = sl) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0039199 -0.0014359 -0.0002769 0.0008649 0.0053601 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.756e-04 1.734e-03 0.562 0.589192 ## bar 6.287e-09 1.234e-09 5.093 0.000938 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.002659 on 8 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.7643, Adjusted R-squared: 0.7348 ## F-statistic: 25.94 on 1 and 8 DF, p-value: 0.0009383 summary(lm(X196900213 ~ bar, sl)) ## ## Call: ## lm(formula = X196900213 ~ bar, data = sl) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0031031 -0.0012324 -0.0004205 0.0011007 0.0048727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.866e-04 1.642e-03 -0.357 0.73020 ## bar 3.927e-09 1.169e-09 3.360 0.00994 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.002518 on 8 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.5852, Adjusted R-squared: 0.5333 ## F-statistic: 11.29 on 1 and 8 DF, p-value: 0.009939 summary(lm(X196900310 ~ bar, sl)) ## ## Call: ## lm(formula = X196900310 ~ bar, data = sl) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0023418 -0.0007641 -0.0005424 -0.0002813 0.0047438 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.181e-05 1.476e-03 -0.062 0.9519 ## bar 2.655e-09 1.051e-09 2.527 0.0354 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.002263 on 8 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.4439, Adjusted R-squared: 0.3744 ## F-statistic: 6.385 on 1 and 8 DF, p-value: 0.03543 summary(lm(X196900588 ~ bar, sl)) ## ## Call: ## lm(formula = X196900588 ~ bar, data = sl) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0036391 -0.0008598 -0.0003235 0.0000593 0.0060071 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.362e-04 1.815e-03 -0.295 0.775 ## bar 2.997e-09 1.292e-09 2.320 0.049 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.002783 on 8 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.4021, Adjusted R-squared: 0.3274 ## F-statistic: 5.38 on 1 and 8 DF, p-value: 0.04895 summary(lm(X196900589 ~ bar, sl)) ## ## Call: ## lm(formula = X196900589 ~ bar, data = sl) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0020026 -0.0004915 -0.0001909 0.0003738 0.0020246 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.725e-04 8.105e-04 -0.46 0.65800 ## bar 3.063e-09 5.769e-10 5.31 0.00072 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.001243 on 8 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.779, Adjusted R-squared: 0.7513 ## F-statistic: 28.19 on 1 and 8 DF, p-value: 0.0007201 library(reshape2) sl &lt;- melt(sl, id.vars = c(&quot;unified_ID&quot;, &quot;Depth&quot;, &quot;bar&quot;)) ggplot(sl, aes(x = Depth, y = value)) + geom_point() + facet_wrap(. ~ variable, scales = &quot;free&quot;) + theme_bw() + scale_y_continuous(labels = scales::scientific) + scale_x_continuous(labels = scales::scientific) + ggtitle(&quot;Shortlisted OTUs and depth&quot;) ggplot(sl, aes(x = bar, y = value)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x) + facet_wrap(. ~ variable, scales = &quot;free&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + xlab(&quot;Barettin signal&quot;) + ylab(&quot;OTU relative abundance&quot;) + scale_y_continuous(labels = scales::scientific) + scale_x_continuous(labels = scales::scientific) + ggtitle(&quot;Shortlisted OTUs and barettin&quot;) sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS 10.15.5 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] minerva_1.5.8 viridis_0.5.1 viridisLite_0.3.0 pheatmap_1.0.12 rgl_0.100.50 plot3D_1.3 ggExtra_0.9 ## [8] plotly_4.9.2 usdm_1.1-18 raster_3.0-12 sp_1.4-1 rnaturalearthdata_0.1.0 rnaturalearth_0.1.0 sf_0.8-1 ## [15] marmap_1.0.3 mapdata_2.3.0 maps_3.3.0 ggmap_3.0.0.901 igraph_1.2.5 ropls_1.14.1 reshape_0.8.8 ## [22] ggrepel_0.8.2 metap_1.3 gtools_3.8.2 seqinr_3.6-1 picante_1.8.1 nlme_3.1-145 ape_5.3 ## [29] phyloseq_1.28.0 vegan_2.5-6 lattice_0.20-41 permute_0.9-5 DT_0.13 gridExtra_2.3 kableExtra_1.1.0.9000 ## [36] RColorBrewer_1.1-2 reshape2_1.4.3 forcats_0.5.0 dplyr_0.8.5 purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 ## [43] tibble_3.0.0 ggplot2_3.3.0 tidyverse_1.3.0 knitr_1.28 stringr_1.4.0 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.0.0 RSQLite_2.2.0 htmlwidgets_1.5.1 grid_3.5.1 munsell_0.5.0 codetools_0.2-16 mutoss_0.1-12 ## [8] units_0.6-6 miniUI_0.1.1.1 misc3d_0.8-4 withr_2.1.2 colorspace_1.4-1 Biobase_2.42.0 highr_0.8 ## [15] rstudioapi_0.11 stats4_3.5.1 gbRd_0.4-11 Rdpack_0.11-1 labeling_0.3 RgoogleMaps_1.4.5.3 mnormt_1.5-6 ## [22] bit64_0.9-7 farver_2.0.3 rhdf5_2.26.2 vctrs_0.2.4 generics_0.0.2 TH.data_1.0-10 xfun_0.13.1 ## [29] R6_2.4.1 isoband_0.2.0 manipulateWidget_0.10.1 bitops_1.0-6 assertthat_0.2.1 promises_1.1.0 scales_1.1.0 ## [36] multcomp_1.4-12 rgeos_0.5-2 gtable_0.3.0 sandwich_2.5-1 rlang_0.4.5 splines_3.5.1 lazyeval_0.2.2 ## [43] broom_0.5.5 yaml_2.2.1 modelr_0.1.6 crosstalk_1.1.0.1 backports_1.1.5 httpuv_1.5.2 tools_3.5.1 ## [50] bookdown_0.18 ellipsis_0.3.0 biomformat_1.10.1 BiocGenerics_0.28.0 TFisher_0.2.0 Rcpp_1.0.4 plyr_1.8.6 ## [57] zlibbioc_1.28.0 classInt_0.4-2 S4Vectors_0.20.1 zoo_1.8-7 haven_2.2.0 cluster_2.1.0 fs_1.4.0 ## [64] magrittr_1.5 data.table_1.12.8 reprex_0.3.0 mvtnorm_1.1-0 hms_0.5.3 mime_0.9 evaluate_0.14 ## [71] xtable_1.8-4 jpeg_0.1-8.1 readxl_1.3.1 IRanges_2.16.0 shape_1.4.4 compiler_3.5.1 KernSmooth_2.23-16 ## [78] ncdf4_1.17 crayon_1.3.4 htmltools_0.4.0 mgcv_1.8-31 later_1.0.0 lubridate_1.7.4 DBI_1.1.0 ## [85] formatR_1.7 dbplyr_1.4.2 MASS_7.3-51.5 Matrix_1.2-18 ade4_1.7-15 cli_2.0.2 parallel_3.5.1 ## [92] pkgconfig_2.0.3 sn_1.6-1 numDeriv_2016.8-1.1 xml2_1.3.0 foreach_1.5.0 multtest_2.38.0 webshot_0.5.2 ## [99] XVector_0.22.0 bibtex_0.4.2.2 rvest_0.3.5 digest_0.6.25 Biostrings_2.50.2 rmarkdown_2.1 cellranger_1.1.0 ## [106] shiny_1.4.0.2 rjson_0.2.20 lifecycle_0.2.0 jsonlite_1.6.1 Rhdf5lib_1.4.3 fansi_0.4.1 pillar_1.4.3 ## [113] fastmap_1.0.1 httr_1.4.1 plotrix_3.7-7 survival_3.1-11 glue_1.3.2 png_0.1-7 iterators_1.0.12 ## [120] bit_1.1-15.2 class_7.3-16 adehabitatMA_0.3.14 stringi_1.4.6 blob_1.2.1 memoise_1.1.0 e1071_1.7-3 "],
["funding-disclaimer.html", "5 Funding disclaimer", " 5 Funding disclaimer This study was financially supported by the SponGES project from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 679849. This document reflects only the authors’ view and the Executive Agency for Small and Medium-sized Enterprises (EASME) is not responsible for any use that may be made of the information it contains. For more information, please visit the SponGES website. # automatically create a bib database for R packages knitr::write_bib(c(.packages(), &quot;bookdown&quot;, &quot;knitr&quot;, &quot;rmarkdown&quot;, &quot;kableExtra&quot;, &quot;ggplot2&quot;, &quot;ggmap&quot;, &quot;maps&quot;, &quot;mapdata&quot;, &quot;marmap&quot;, &quot;ggrepel&quot;, &quot;lattice&quot;, &quot;sf&quot;, &quot;rnaturalearth&quot;, &quot;rnaturalearthdata&quot;, &quot;vegan&quot;, &quot;xcms&quot;, &quot;camera&quot;, &quot;ropls&quot;, &quot;tidyverse&quot;, &quot;stringr&quot;, &quot;usdm&quot;, &quot;reshape2&quot;, &quot;gridExtra&quot;, &quot;dplyr&quot;, &quot;plotly&quot;, &quot;phyloseq&quot;, &quot;seqinr&quot;, &quot;igraph&quot;), &quot;packages.bib&quot;) Maps were made with the pacakges marmap (Pante and Simon-Bouhet 2013), ggmap (Kahle and Wickham 2013), maps (Brownrigg 2018), mapdata (Richard A. Becker and Ray Brownrigg. 2018), sf (Pebesma 2018), rnaturalearth (South 2017b), rnaturalearthdata (South 2017a), lattice (Sarkar 2008). Analyses used the functionalities from R (R Core Team 2018) as well as the packages usdm (Naimi et al. 2014), tidyverse (Wickham et al. 2019), plotly (Sievert et al. 2020), dplyr (Wickham et al. 2020), stringr (Wickham 2019), kableExtra (Zhu 2019), GridExtra (Auguie 2017), ggrepel (Slowikowski 2020), reshape2 (Wickham 2017), knitr (Xie 2015), Rmarkadown (Xie, Allaire, and Grolemund 2018) and bookdown (Xie 2020). Specifically, metabolomics relied on XCMS (Smith et al. 2006), (Tautenhahn, Boettcher, and Neumann 2008), (Benton, Want, and Ebbels 2010) with CAMERA (Kuhl et al. 2012) ROPLS (Thevenot et al. 2015), and the microbiome analyses on vegan (Oksanen et al. 2019), phyloseq (McMurdie and Holmes 2013), seqinr (Charif and Lobry 2019), and igraph (Csardi and Nepusz 2006). References "],
["references.html", "References", " References "]
]
